Directory structure:
└── lzell-aiproxybootstrap/
    ├── README.md
    ├── AIProxyAnthropic/
    │   └── AIProxyAnthropic/
    │       ├── AIProxyAnthropicApp.swift
    │       ├── AppConstants.swift
    │       ├── ContentView.swift
    │       ├── MessageRequestView.swift
    │       ├── ToolsView.swift
    │       ├── VisionView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── anthropic.imageset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   └── climber.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    ├── AIProxyDeepL/
    │   └── AIProxyDeepL/
    │       ├── AIProxyDeepLApp.swift
    │       ├── AppConstants.swift
    │       ├── ContentView.swift
    │       ├── TranslationView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   └── deepl.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    ├── AIProxyFal/
    │   └── AIProxyFal/
    │       ├── AIProxyFalApp.swift
    │       ├── AppConstants.swift
    │       ├── ContentView.swift
    │       ├── TextToImageView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   └── fal.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    ├── AIProxyGemini/
    │   └── AIProxyGemini/
    │       ├── AIProxyGeminiApp.swift
    │       ├── AppConstants.swift
    │       ├── ContentView.swift
    │       ├── TextGenerationView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   └── icon.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    ├── AIProxyGroq/
    │   └── AIProxyGroq/
    │       ├── AIProxyGroqApp.swift
    │       ├── AppConstants.swift
    │       ├── ChatView.swift
    │       ├── ContentView.swift
    │       ├── StreamingChatView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   └── groq.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    ├── AIProxyOpenAI/
    │   └── AIProxyOpenAI/
    │       ├── AIProxyOpenAIApp.swift
    │       ├── AppConstants.swift
    │       ├── ChatView.swift
    │       ├── ContentView.swift
    │       ├── DalleView.swift
    │       ├── MultiModalChatView.swift
    │       ├── StreamingChatView.swift
    │       ├── TextToSpeechView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   ├── openai.imageset/
    │       │   │   └── Contents.json
    │       │   └── surfer.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    ├── AIProxyReplicate/
    │   └── AIProxyReplicate/
    │       ├── AIProxyReplicateApp.swift
    │       ├── AppConstants.swift
    │       ├── ContentView.swift
    │       ├── ImageGenView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   └── replicate.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    ├── AIProxyStabilityAI/
    │   └── AIProxyStabilityAI/
    │       ├── AIProxyStabilityAIApp.swift
    │       ├── AppConstants.swift
    │       ├── ContentView.swift
    │       ├── ImageGenView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   └── stability.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    ├── AIProxyTogetherAI/
    │   └── AIProxyTogetherAI/
    │       ├── AIProxyTogetherAIApp.swift
    │       ├── AppConstants.swift
    │       ├── ChatView.swift
    │       ├── ContentView.swift
    │       ├── JSONResponseView.swift
    │       ├── StreamingChatView.swift
    │       ├── Assets.xcassets/
    │       │   ├── Contents.json
    │       │   ├── AccentColor.colorset/
    │       │   │   └── Contents.json
    │       │   ├── AppIcon.appiconset/
    │       │   │   └── Contents.json
    │       │   └── togetherai.imageset/
    │       │       └── Contents.json
    │       └── Preview Content/
    │           └── Preview Assets.xcassets/
    │               └── Contents.json
    └── Demos/
        ├── AIColorPalette/
        │   ├── README.md
        │   └── AIColorPalette/
        │       ├── AIColorPaletteApp.swift
        │       ├── AIProxyIntegration.swift
        │       ├── ColorData.swift
        │       ├── ColorDetailView.swift
        │       ├── ContentView.swift
        │       ├── Ripple.metal
        │       ├── Ripple.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   ├── AppIcon.appiconset/
        │       │   │   └── Contents.json
        │       │   └── palm.imageset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        ├── Chat/
        │   └── Chat/
        │       ├── AppConstants.swift
        │       ├── AppLogger.swift
        │       ├── ChatApp.swift
        │       ├── ChatBubble.swift
        │       ├── ChatDataLoader.swift
        │       ├── ChatInputView.swift
        │       ├── ChatManager.swift
        │       ├── ChatMessage.swift
        │       ├── ChatView.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   └── AppIcon.appiconset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        ├── Classifier/
        │   └── Classifier/
        │       ├── AppConstants.swift
        │       ├── AppLogger.swift
        │       ├── CameraControlsView.swift
        │       ├── CameraDataLoader.swift
        │       ├── CameraFrameManager.swift
        │       ├── CameraView.swift
        │       ├── ClassifierApp.swift
        │       ├── ClassifierDataLoader.swift
        │       ├── ClassifierManager.swift
        │       ├── ClassifierView.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   └── AppIcon.appiconset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        ├── EmojiPuzzleMaker/
        │   └── EmojiPuzzleMaker/
        │       ├── ContentView.swift
        │       ├── EmojiPuzzleMakerApp.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   └── AppIcon.appiconset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        ├── FilmFinder/
        │   └── FilmFinder/
        │       ├── AppConstants.swift
        │       ├── ContentView.swift
        │       ├── FilmFinderApp.swift
        │       ├── GenreSelectorView.swift
        │       ├── GetStartedTip.swift
        │       ├── Info.plist
        │       ├── Movie.swift
        │       ├── MovieDetailsView.swift
        │       ├── Ripple.metal
        │       ├── Ripple.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   └── AppIcon.appiconset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        ├── PuLIDDemo/
        │   └── PuLIDDemo/
        │       ├── AppConstants.swift
        │       ├── ContentView.swift
        │       ├── Info.plist
        │       ├── PuLIDDemoApp.swift
        │       ├── Ripple.metal
        │       ├── Ripple.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   ├── AppIcon.appiconset/
        │       │   │   └── Contents.json
        │       │   └── pulid.imageset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        ├── Stickers/
        │   └── Stickers/
        │       ├── AppConstants.swift
        │       ├── AppLogger.swift
        │       ├── ButtonStyles.swift
        │       ├── StickerDataLoader.swift
        │       ├── StickerImageView.swift
        │       ├── StickerInputView.swift
        │       ├── StickerLoadingView.swift
        │       ├── StickerManager.swift
        │       ├── StickersApp.swift
        │       ├── StickerView.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   └── AppIcon.appiconset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        ├── Transcriber/
        │   └── Transcriber/
        │       ├── AppConstants.swift
        │       ├── AppLogger.swift
        │       ├── AudioFileWriter.swift
        │       ├── AudioRecorder.swift
        │       ├── AudioRecording.swift
        │       ├── ButtonStyles.swift
        │       ├── FileUtils.swift
        │       ├── MicrophoneSampleVendor.swift
        │       ├── ModelContext+Extensions.swift
        │       ├── NoRecordingsView.swift
        │       ├── RecordingRowView.swift
        │       ├── TranscribedAudioRecording.swift
        │       ├── TranscriberApp.swift
        │       ├── TranscriberDataLoader.swift
        │       ├── TranscriberManager.swift
        │       ├── TranscriberView.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   └── AppIcon.appiconset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        ├── Translator/
        │   └── Translator/
        │       ├── AppConstants.swift
        │       ├── AppLogger.swift
        │       ├── BottomTranslateView.swift
        │       ├── ButtonStyles.swift
        │       ├── TopTranslateView.swift
        │       ├── TranslateView.swift
        │       ├── TranslationDataLoader.swift
        │       ├── TranslatorApp.swift
        │       ├── Assets.xcassets/
        │       │   ├── Contents.json
        │       │   ├── AccentColor.colorset/
        │       │   │   └── Contents.json
        │       │   └── AppIcon.appiconset/
        │       │       └── Contents.json
        │       └── Preview Content/
        │           └── Preview Assets.xcassets/
        │               └── Contents.json
        └── Trivia/
            └── Trivia/
                ├── AppConstants.swift
                ├── AppLogger.swift
                ├── TriviaAnswerPicker.swift
                ├── TriviaApp.swift
                ├── TriviaCardData.swift
                ├── TriviaCardView.swift
                ├── TriviaDataLoader.swift
                ├── TriviaFormView.swift
                ├── TriviaManager.swift
                ├── TriviaView.swift
                ├── Assets.xcassets/
                │   ├── Contents.json
                │   ├── AccentColor.colorset/
                │   │   └── Contents.json
                │   └── AppIcon.appiconset/
                │       └── Contents.json
                └── Preview Content/
                    └── Preview Assets.xcassets/
                        └── Contents.json

================================================
FILE: README.md
================================================
# Starter apps for AIProxy

Use these apps as a jumping off point to build your own experiences using AIProxy. Sample apps are organized by services (for ex. OpenAI, Anthropic etc.). Each sample app has a placeholder to add your AIProxy constants (see AppConstants.swift). The apps all use [AIProxySwift](https://github.com/lzell/AIProxySwift) to implement API calls.

### Instructions to build and run

1. Watch [the AIProxy bootstrap walkthrough video](https://www.youtube.com/watch?v=ohsN9awCzw4)
2. Replace the constants in `AppConstants.swift` files with the snippet you receive from the AIProxy dashboard in step 1
3. Change the bundler identifier of the sample app to match the App ID you created in step 1
4. Add an AIPROXY_DEVICE_CHECK_BYPASS env variable to Xcode. This is necessary for the iOS simulator to communicate with the AIProxy backend. Type **cmd-shift-comma** to open up the "Edit Schemes" menu. Select Run in the sidebar, then select Arguments from the top nav. Add to the "Environment Variables" section an env variable with name AIPROXY_DEVICE_CHECK_BYPASS and value displayed on the key details screen.

### Quickstart apps

- **AIProxyAnthropic** - An Anthropic app that generates a message.
- **AIProxyDeepL** - A DeepL app that translates input text to Spanish.
- **AIProxyOpenAI** - An OpenAI app with chat, DALLE, and vision.
- **AIProxyReplicate** - A Replicate app with Stable Diffusion XL.
- **AIProxyGroq** - A Groq app with chat completion and streaming chat completion examples.
- **AIProxyStability** - A Stability AI app that generates an image.
- **AIProxyTogetherAI** - A Together AI app with examples for chat, streaming chat, and JSON response.

### Playground apps

- **FilmFinder** - A movie recommendation app that uses Groq and TMDB (requires Xcode 16).
- **PuLIDDemo** - An image generator app that uses PuLID on Replicate (requires Xcode 16).
- **AIColorPalette** - An OpenAI color palette generator that uses an image as input (requires Xcode 16).
- **Chat** - A basic chat application and interface with OpenAI. Includes streaming responses and ability to stop stream.
- **Image Classifier** - An OpenAI image classification app that identifies plants and provides a link to Wikipedia.
- **Transriber** - An OpenAI app that transcribes audio recorded using the device microphone.
- **Translator** - A simple English to Spanish translation app with text to speech using OpenAI.
- **Trivia Game** - A trivia game that uses GPT to generate multiple choice questions from a JSON response.
- **Stickers** - An OpenAI app that turns a prompt into a kawaii style sticker and extracts the foreground/background using Vision.
- **EmojiPuzzleMaker** - Generate emoji puzzles using Anthropic's Claude 3.5 Sonnet API.



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/AIProxyAnthropicApp.swift
================================================
//
//  AIProxyAnthropicApp.swift
//  AIProxyAnthropic
//
//  Created by Todd Hamilton on 6/17/24.
//

import SwiftUI

@main
struct AIProxyAnthropicApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyAnthropic
//
//  Created by Todd Hamilton on 8/14/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let anthropicService = AIProxy.anthropicDirectService(
    unprotectedAPIKey: "your-anthropic-key"
)

/* Uncomment for all other production use cases */
//let anthropicService = AIProxy.anthropicService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyAnthropic
//
//  Created by Todd Hamilton on 6/17/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            
            VStack(spacing:24){
                VStack{
                    Image("anthropic")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("Anthropic")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Sample")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Message Request Example",destination: MessageRequestView())
                        .bold()
                        .controlSize(.large)
                        .tint(.brown)
                        .buttonStyle(.bordered)
                    NavigationLink("Vision Example",destination: VisionView())
                        .bold()
                        .controlSize(.large)
                        .tint(.brown)
                        .buttonStyle(.bordered)
                    NavigationLink("Tools Example",destination: ToolsView())
                        .bold()
                        .controlSize(.large)
                        .tint(.brown)
                        .buttonStyle(.bordered)
                }
            }
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/MessageRequestView.swift
================================================
//
//  MessageRequestView.swift
//  AIProxyAnthropic
//
//  Created by Todd Hamilton on 8/14/24.
//

import SwiftUI
import AIProxy

struct MessageRequestView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let response = try await anthropicService.messageRequest(body: AnthropicMessageRequestBody(
                maxTokens: 1024,
                messages: [
                    AnthropicInputMessage(content: [.text(prompt)], role: .user)
                ],
                model: "claude-3-5-sonnet-20240620"
            ))
            for content in response.content {
                switch content {
                case .text(let message):
                    print("Claude sent a message: \(message)")
                    result = message
                    showingAlert = true
                case .toolUse(id: _, name: let toolName, input: let toolInput):
                    print("Claude used a tool \(toolName) with input: \(toolInput)")
                }
            }
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate Message",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text(result),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Message")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Message Example")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    MessageRequestView()
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/ToolsView.swift
================================================
//
//  ToolsView.swift
//  AIProxyAnthropic
//
//  Created by Todd Hamilton on 8/14/24.
//

import SwiftUI
import AIProxy

struct ToolsView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Stock Symbol Lookup",
                    systemImage: "chart.line.uptrend.xyaxis",
                    description: Text("Type the name of the company you want the symbol for.")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text(result),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a company", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Look Up")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Tools Example")
        .navigationBarTitleDisplayMode(.inline)
    }
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let requestBody = AnthropicMessageRequestBody(
                maxTokens: 1024,
                messages: [
                    .init(
                        content: [.text(prompt)],
                        role: .user
                    )
                ],
                model: "claude-3-5-sonnet-20240620",
                tools: [
                    .init(
                        description: "Call this function when the user wants a stock symbol",
                        inputSchema: [
                            "type": "object",
                            "properties": [
                                "ticker": [
                                    "type": "string",
                                    "description": "The stock ticker symbol, e.g. AAPL for Apple Inc."
                                ]
                            ],
                            "required": ["ticker"]
                        ],
                        name: "get_stock_symbol"
                    )
                ]
            )
            let response = try await anthropicService.messageRequest(body: requestBody)
            
            for content in response.content {
                switch content {
                case .text(let message):
                    print("Claude sent a message: \(message)")
                case .toolUse(id: _, name: let toolName, input: let toolInput):
                    print("Claude used a tool \(toolName) with input: \(toolInput)")
                    result = toolInput.description
                    showingAlert = true
                }
            }
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
}

#Preview {
    ToolsView()
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/VisionView.swift
================================================
//
//  VisionView.swift
//  AIProxyAnthropic
//
//  Created by Todd Hamilton on 8/14/24.
//

import SwiftUI
import AIProxy

struct VisionView: View {
    
    @State private var prompt:String = ""
    @State private var result:String = ""
    @State private var showingAlert = false
    @State private var isLoading = false
    
    func generate() async throws {
        
        guard let image = UIImage(named: "climber") else {
            print("Could not find an image named 'climber' in your app assets")
            return
        }

        guard let jpegData = AIProxy.encodeImageAsJpeg(image: image, compressionQuality: 0.8) else {
            print("Could not convert image to jpeg")
            return
        }
        
        isLoading = true
        defer { isLoading = false }
        do {
            let response = try await anthropicService.messageRequest(body: AnthropicMessageRequestBody(
                maxTokens: 1024,
                messages: [
                    AnthropicInputMessage(content: [
                        .text("Provide a very short description of this image"),
                        .image(mediaType: .jpeg, data: jpegData.base64EncodedString())
                    ], role: .user)
                ],
                model: "claude-3-5-sonnet-20240620"
            ))
            for content in response.content {
                switch content {
                case .text(let message):
                    print("Claude sent a message: \(message)")
                    result = message
                    showingAlert = true
                case .toolUse(id: _, name: let toolName, input: let toolInput):
                    print("Claude used a tool \(toolName) with input: \(toolInput)")
                }
            }
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    var body: some View {
        VStack{
            VStack{
                Image("climber")
                    .resizable()
                    .scaledToFit()
                    .frame(maxWidth: .infinity)
            }
            .frame(maxHeight: .infinity)
            .alert(isPresented: $showingAlert){
                Alert(
                    title: Text("Result"),
                    message: Text("\(result)"),
                    dismissButton: .default(Text("Close"))
                )
            }
            
            Spacer()
            
            VStack(spacing:12){
                Button{
                    Task {
                        try await generate()
                    }
                }label: {
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Describe Image")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Vision Example")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    VisionView()
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/Assets.xcassets/anthropic.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "anthropic.jpeg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/Assets.xcassets/climber.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "climber.jpg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyAnthropic/AIProxyAnthropic/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyDeepL/AIProxyDeepL/AIProxyDeepLApp.swift
================================================
//
//  AIProxyDeepLApp.swift
//  AIProxyDeepL
//
//  Created by Todd Hamilton on 8/14/24.
//

import SwiftUI

@main
struct AIProxyDeepLApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyDeepL/AIProxyDeepL/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyDeepL
//
//  Created by Todd Hamilton on 8/14/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let deepLService = AIProxy.deepLDirectService(
    unprotectedAPIKey: "your-deepL-key",
    accountType: .free
)

/* Uncomment for all other production use cases */
//let deepLService = AIProxy.deepLService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyDeepL/AIProxyDeepL/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyDeepL
//
//  Created by Todd Hamilton on 8/14/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            
            VStack(spacing:48){
                VStack{
                    Image("deepl")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("DeepL")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Sample")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Translation Example",destination: TranslationView())
                        .bold()
                        .controlSize(.large)
                        .tint(.blue)
                        .buttonStyle(.bordered)
                    
                }
            }
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: AIProxyDeepL/AIProxyDeepL/TranslationView.swift
================================================
//
//  TranslationView.swift
//  AIProxyDeepL
//
//  Created by Todd Hamilton on 8/14/24.
//

import SwiftUI
import AIProxy

struct TranslationView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let body = DeepLTranslateRequestBody(targetLang: "ES", text: [prompt])
            let response = try await deepLService.translateRequest(body: body)
            // Do something with `response.translations`
            result = response.translations.first?.text ?? ""
            showingAlert = true
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print("Could not create translation: \(error.localizedDescription)")
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Translate to Spanish",
                    systemImage: "captions.bubble.fill",
                    description: Text("Write text you want to translate below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text(result),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type your text here", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Translate")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Translate Example")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    TranslationView()
}



================================================
FILE: AIProxyDeepL/AIProxyDeepL/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyDeepL/AIProxyDeepL/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyDeepL/AIProxyDeepL/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyDeepL/AIProxyDeepL/Assets.xcassets/deepl.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "deepl.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyDeepL/AIProxyDeepL/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyFal/AIProxyFal/AIProxyFalApp.swift
================================================
//
//  AIProxyFalApp.swift
//  AIProxyFal
//
//  Created by Todd Hamilton on 6/13/24.
//

import SwiftUI

@main
struct AIProxyFalApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyFal/AIProxyFal/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyFal
//
//  Created by Todd Hamilton on 9/17/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let falService = AIProxy.falDirectService(
    unprotectedAPIKey: "your-fal-key"
)

/* Uncomment for all other production use cases */
//let falService = AIProxy.falService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyFal/AIProxyFal/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyFal
//
//  Created by Todd Hamilton on 6/13/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            
            VStack(spacing:24){
                VStack{
                    Image("fal")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("Fal")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Sample")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Text to Image with FastSDXL",destination: TextToImageView())
                        .bold()
                        .controlSize(.large)
                        .tint(.indigo)
                        .buttonStyle(.bordered)
                }
            }
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: AIProxyFal/AIProxyFal/TextToImageView.swift
================================================
//
//  ImageGenView.swift
//  AIProxyFal
//
//  Created by Todd Hamilton on 6/13/24.
//

import SwiftUI
import AIProxy

struct TextToImageView: View {
    
    @State private var prompt: String = ""
    @State private var imageUrl: String?
    @State private var isLoading: Bool = false
    
    private func generate() async throws {
        
        let input = FalFastSDXLInputSchema(
            prompt: prompt,
            enableSafetyChecker: false
        )
        isLoading = true  // Start loading
        defer { isLoading = false }
        do {
            let output = try await falService.createFastSDXLImage(input: input)
            print("""
                  The first output image is at \(output.images?.first?.url?.absoluteString ?? "")
                  It took \(output.timings?.inference ?? Double.nan) seconds to generate.
                  """)
            imageUrl = output.images?.first?.url?.absoluteString
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print("Could not create Fal SDXL image: \(error.localizedDescription)")
        }
    }
    
    var body: some View {
        VStack{
                
            VStack{
                if (imageUrl != nil) {
                    AsyncImage(url: URL(string: imageUrl!)) { phase in
                        if let image = phase.image {
                            image
                                .resizable()
                                .aspectRatio(contentMode: .fit)
                        } else if phase.error != nil {
                            Text("Failed to load image")
                                .foregroundColor(.red)
                        } else {
                            ProgressView()
                        }
                    }
                } else{
                    ContentUnavailableView(
                        "Generate an image",
                        systemImage: "photo.fill",
                        description: Text("Write a prompt below")
                    )
                }
            }
            .frame(maxHeight: .infinity)
            
            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Image")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Generate Image")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    TextToImageView()
}



================================================
FILE: AIProxyFal/AIProxyFal/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyFal/AIProxyFal/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyFal/AIProxyFal/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyFal/AIProxyFal/Assets.xcassets/fal.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "fal.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyFal/AIProxyFal/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGemini/AIProxyGemini/AIProxyGeminiApp.swift
================================================
//
//  AIProxyGeminiApp.swift
//  AIProxyGemini
//
//  Created by Todd Hamilton on 10/18/24.
//

import SwiftUI

@main
struct AIProxyGeminiApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyGemini/AIProxyGemini/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyGemini
//
//  Created by Todd Hamilton on 10/18/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let geminiService = AIProxy.geminiDirectService(
    unprotectedAPIKey: "your-gemini-key"
)

/* Uncomment for all other production use cases */
//let geminiService = AIProxy.geminiService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyGemini/AIProxyGemini/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyGemini
//
//  Created by Todd Hamilton on 10/18/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            VStack(spacing:24){
                VStack{
                    Image("icon")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("Gemini")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Sample")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Text Generation",destination: TextGenerationView())
                }
                .bold()
                .controlSize(.large)
                .tint(.teal)
                .buttonStyle(.bordered)
            }
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: AIProxyGemini/AIProxyGemini/TextGenerationView.swift
================================================
//
//  TextGenerationView.swift
//  AIProxyGemini
//
//  Created by Todd Hamilton on 10/18/24.
//

import SwiftUI
import AIProxy

struct TextGenerationView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let requestBody = GeminiGenerateContentRequestBody(
                model: "gemini-1.5-flash",
                contents: [
                    .init(
                        parts: [.text("Tell me a joke")]
                    )
                ]
            )
            let response = try await geminiService.generateContentRequest(body: requestBody)
            for part in response.candidates?.first?.content?.parts ?? [] {
                switch part {
                case .text(let text):
                    print("Gemini sent: \(text)")
                    result = text
                }
            }
            if let usage = response.usageMetadata {
                print(
                    """
                    Used:
                     \(usage.promptTokenCount ?? 0) prompt tokens
                     \(usage.cachedContentTokenCount ?? 0) cached tokens
                     \(usage.candidatesTokenCount ?? 0) candidate tokens
                     \(usage.totalTokenCount ?? 0) total tokens
                    """
                )
            }
            
            showingAlert = true
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received \(statusCode) status code with response body: \(responseBody)")
        } catch {
            print("Could not create Gemini generate content request: \(error.localizedDescription)")
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate Text",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text(result),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Chat Completion")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    TextGenerationView()
}



================================================
FILE: AIProxyGemini/AIProxyGemini/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGemini/AIProxyGemini/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGemini/AIProxyGemini/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGemini/AIProxyGemini/Assets.xcassets/icon.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "gemini.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGemini/AIProxyGemini/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGroq/AIProxyGroq/AIProxyGroqApp.swift
================================================
//
//  AIProxyGroqApp.swift
//  AIProxyGroq
//
//  Created by Todd Hamilton on 10/1/24.
//

import SwiftUI

@main
struct AIProxyGroqApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyGroq/AIProxyGroq/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyGroq
//
//  Created by Todd Hamilton on 10/1/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let groqService = AIProxy.groqDirectService(
    unprotectedAPIKey: "your-groq-key"
)

/* Uncomment for all other production use cases */
//let groqService = AIProxy.groqService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyGroq/AIProxyGroq/ChatView.swift
================================================
//
//  ChatView.swift
//  AIProxyGroq
//
//  Created by Todd Hamilton on 10/1/24.
//

import SwiftUI
import AIProxy

struct ChatView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let response = try await groqService.chatCompletionRequest(body: .init(
                messages: [.assistant(content: prompt)],
                model: "mixtral-8x7b-32768"
            ))
            print(response.choices.first?.message.content ?? "")
            result = response.choices.first?.message.content ?? ""
            showingAlert = true
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate Text",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text(result),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Chat Completion")
        .navigationBarTitleDisplayMode(.inline)
    }
}


#Preview {
    ChatView()
}



================================================
FILE: AIProxyGroq/AIProxyGroq/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyGroq
//
//  Created by Todd Hamilton on 10/1/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            VStack(spacing:24){
                VStack{
                    Image("groq")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("Groq")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Sample")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Chat Completion",destination: ChatView())
                    NavigationLink("Streaming Chat Completion",destination: StreamingChatView())
                }
                .bold()
                .controlSize(.large)
                .tint(.red)
                .buttonStyle(.bordered)
            }
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: AIProxyGroq/AIProxyGroq/StreamingChatView.swift
================================================
//
//  StreamingChatView.swift
//  AIProxyGroq
//
//  Created by Todd Hamilton on 10/1/24.
//


import SwiftUI
import AIProxy

struct StreamingChatView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let stream = try await groqService.streamingChatCompletionRequest(body: .init(
                    messages: [.assistant(content: prompt)],
                    model: "mixtral-8x7b-32768"
                )
            )
            for try await chunk in stream {
                print(chunk.choices.first?.delta.content ?? "")
            }
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received \(statusCode) status code with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate Text",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text("View the streaming response in the Xcode console."),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        showingAlert = true
                        Task{ try await generate() }
                    }
                Button{
                    showingAlert = true
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Streaming Chat Completion")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    ChatView()
}



================================================
FILE: AIProxyGroq/AIProxyGroq/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGroq/AIProxyGroq/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGroq/AIProxyGroq/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGroq/AIProxyGroq/Assets.xcassets/groq.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "groq.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyGroq/AIProxyGroq/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/AIProxyOpenAIApp.swift
================================================
//
//  AIProxyOpenAIApp.swift
//  AIProxyOpenAI
//
//  Created by Todd Hamilton on 6/14/24.
//

import SwiftUI

@main
struct AIProxyOpenAIApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyOpenAI
//
//  Created by Todd Hamilton on 6/14/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let openAIService = AIProxy.openAIDirectService(
    unprotectedAPIKey: "your-openai-key"
)

/* Uncomment for all other production use cases */
//let openAIService = AIProxy.openAIService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/ChatView.swift
================================================
//
//  ChatView.swift
//  AIProxyOpenAI
//
//  Created by Todd Hamilton on 6/14/24.
//

import SwiftUI
import AIProxy

struct ChatView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let response = try await openAIService.chatCompletionRequest(body: .init(
                model: "gpt-4o",
                messages: [.system(content: .text(prompt))]
            ))
            result = (response.choices.first?.message.content)!
            showingAlert = true
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(String(describing: responseBody))")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate Text",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text(result),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Chat Completion")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    ChatView()
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyOpenAI
//
//  Created by Todd Hamilton on 6/14/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            
            VStack(spacing:48){
                VStack{
                    Image("openai")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("OpenAI")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Sample")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Chat Example",destination: ChatView())
                    NavigationLink("Streaming Chat Example",destination: ChatView())
                    NavigationLink("Multi-Modal Chat Example",destination: MultiModalChatView())
                    NavigationLink("DALLE Example",destination: DalleView())
                    NavigationLink("Text-to-Speech Example",destination: TextToSpeechView())
                }
                .bold()
                .controlSize(.large)
                .buttonStyle(.bordered)
                .tint(.purple)
            }
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/DalleView.swift
================================================
//
//  DalleView.swift
//  AIProxyOpenAI
//
//  Created by Todd Hamilton on 8/13/24.
//

import SwiftUI
import AIProxy

struct DalleView: View {
    
    @State private var prompt = ""
    @State private var imageUrl: String?
    @State private var isLoading = false
    
    func generate() async throws {
        isLoading = true  // Start loading
        defer { isLoading = false }
        do {
            let requestBody = OpenAICreateImageRequestBody(
                prompt: prompt,
                model: "dall-e-3"
            )
            let response = try await openAIService.createImageRequest(body: requestBody)
            imageUrl = response.data.first?.url?.absoluteString ?? ""
//            print(response.data.first?.url ?? "")
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    var body: some View {
        VStack{
                
            VStack{
                if (imageUrl != nil) {
                    AsyncImage(url: URL(string: imageUrl!)) { phase in
                        if let image = phase.image {
                            image
                                .resizable()
                                .aspectRatio(contentMode: .fit)
                        } else if phase.error != nil {
                            Text("Failed to load image")
                                .foregroundColor(.red)
                        } else {
                            ProgressView()
                        }
                    }
                } else{
                    ContentUnavailableView(
                        "Generate an image",
                        systemImage: "photo.fill",
                        description: Text("Write a prompt below")
                    )
                }
            }
            .frame(maxHeight: .infinity)
            
            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Generate Image")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    DalleView()
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/MultiModalChatView.swift
================================================
//
//  MultiModalChatView.swift
//  AIProxyOpenAI
//
//  Created by Todd Hamilton on 6/14/24.
//

import SwiftUI
import AIProxy
import UIKit

struct MultiModalChatView: View {
    
    @State private var prompt:String = ""
    @State private var result:String = ""
    @State private var showingAlert = false
    @State private var isLoading = false
    
    var body: some View {
        VStack{
            VStack{
                Image("surfer")
                    .resizable()
                    .scaledToFit()
                    .frame(maxWidth: .infinity)
            }
            .frame(maxHeight: .infinity)
            .alert(isPresented: $showingAlert){
                Alert(
                    title: Text("Result"),
                    message: Text("\(result)"),
                    dismissButton: .default(Text("Close"))
                )
            }
            
            Spacer()
            
            VStack(spacing:12){
                Button{
                    Task {
                        try await generate()
                    }
                }label: {
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Describe Image")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Describe Image")
        .navigationBarTitleDisplayMode(.inline)
    }
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        
        let image = UIImage(named: "surfer")
        let localURL = createOpenAILocalURL(forImage: image!)
        
        do {
            let response = try await openAIService.chatCompletionRequest(body: .init(
                model: "gpt-4o",
                messages: [
                    .system(
                        content: .text("Tell me what you see")
                    ),
                    .user(
                        content: .parts(
                            [
                                .text("What do you see?"),
                                .imageURL(localURL!)
                            ]
                        )
                    )
                ]
            ))
            result = (response.choices.first?.message.content)!
            showingAlert = true
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(String(describing: responseBody))")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    func createOpenAILocalURL(forImage image: UIImage) -> URL? {
        // Attempt to get JPEG data from the UIImage
        guard let jpegData = image.jpegData(compressionQuality: 1.0) else {
            return nil
        }
        
        // Encode the JPEG data to a base64 string
        let base64String = jpegData.base64EncodedString()
        
        // Create the data URL string
        let urlString = "data:image/jpeg;base64,\(base64String)"
        
        // Return the URL constructed from the data URL string
        return URL(string: urlString)
    }
 
}

#Preview {
    MultiModalChatView()
}

private extension CGImage {
    var jpegData: Data? {
        let uiImage = UIImage(cgImage: self)
        return uiImage.jpegData(compressionQuality: 1.0)
    }
}







================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/StreamingChatView.swift
================================================
//
//  StreamingChatView.swift
//  AIProxyOpenAI
//
//  Created by Todd Hamilton on 8/13/24.
//

import SwiftUI
import AIProxy

struct StreamingChatView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        
        let requestBody = OpenAIChatCompletionRequestBody(
            model: "gpt-4o",
            messages: [.user(content: .text(prompt))]
        )
        isLoading = true
        defer { isLoading = false }
        do {
            let stream = try await openAIService.streamingChatCompletionRequest(body: requestBody)
            for try await chunk in stream {
                print(chunk.choices.first?.delta.content ?? "")
            }
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate Text",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text("View the streaming response in the Xcode console."),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    showingAlert = true
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Streaming Chat Completion")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    StreamingChatView()
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/TextToSpeechView.swift
================================================
//
//  TextToSpeechView.swift
//  AIProxyOpenAI
//
//  Created by Todd Hamilton on 10/9/24.
//

import SwiftUI
import AIProxy
import AVKit

struct TextToSpeechView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State var audioPlayer: AVAudioPlayer!
    
    // List of available voices
    let voices = ["nova", "aria", "bella", "emma"] // Replace with actual voice names from OpenAI
    @State private var selectedVoice = "nova" // Default selected voice
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let requestBody = OpenAITextToSpeechRequestBody(
                input: prompt,
                voice: OpenAITextToSpeechRequestBody.Voice(rawValue: selectedVoice) ?? .nova
            )

            let mpegData = try await openAIService.createTextToSpeechRequest(body: requestBody)

            // Do not use a local `let` or `var` for AVAudioPlayer.
            // You need the lifecycle of the player to live beyond the scope of this function.
            // Instead, use file scope or set the player as a member of a reference type with long life.
            // For example, at the top of this file you may define:
            //
            //   fileprivate var audioPlayer: AVAudioPlayer? = nil
            //
            // And then use the code below to play the TTS result:
            audioPlayer = try AVAudioPlayer(data: mpegData)
            audioPlayer?.prepareToPlay()
            audioPlayer?.play()
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received \(statusCode) status code with response body: \(responseBody)")
        } catch {
            print("Could not create ElevenLabs TTS audio: \(error.localizedDescription)")
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Text-to-Speech",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt and select a voice")
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                
                // Voice Picker
                Picker("Select Voice", selection: $selectedVoice) {
                    ForEach(voices, id: \.self) {
                        Text($0.capitalized)
                    }
                }
                .pickerStyle(.segmented)
                .padding()
                
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Speech")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Text-to-Speech")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    TextToSpeechView()
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/Assets.xcassets/openai.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "openai.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/Assets.xcassets/surfer.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "surfer.jpeg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyOpenAI/AIProxyOpenAI/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyReplicate/AIProxyReplicate/AIProxyReplicateApp.swift
================================================
//
//  AIProxyReplicateApp.swift
//  AIProxyReplicate
//
//  Created by Todd Hamilton on 6/13/24.
//

import SwiftUI

@main
struct AIProxyReplicateApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyReplicate/AIProxyReplicate/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyReplicate
//
//  Created by Todd Hamilton on 6/13/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let replicateService = AIProxy.replicateDirectService(
    unprotectedAPIKey: "your-replicate-key"
)

/* Uncomment for all other production use cases */
//let replicateService = AIProxy.replicateService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyReplicate/AIProxyReplicate/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyReplicate
//
//  Created by Todd Hamilton on 6/13/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            VStack(spacing:24){
                VStack{
                    Image("replicate")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("Replicate")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Samples")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Generate Image Example",destination: ImageGenView())
                        .bold()
                        .controlSize(.large)
                        .tint(.pink)
                        .buttonStyle(.bordered)
                }
            }
        }
    }
}

#Preview {
    ContentView()
}




================================================
FILE: AIProxyReplicate/AIProxyReplicate/ImageGenView.swift
================================================
//
//  ImageGenView.swift
//  AIProxyReplicate
//
//  Created by Todd Hamilton on 6/13/24.
//

import SwiftUI
import AIProxy

struct ImageGenView: View {
    
    @State private var prompt = ""
    @State private var imageUrl: URL?
    @State private var isLoading = false

    func generate() async throws {
        isLoading = true  // Start loading
        defer { isLoading = false }
        do {
            let input = ReplicateSDXLInputSchema(
                prompt: prompt
            )
            let output = try await replicateService.createSDXLImage(
                input: input
            )
            print("Done creating SDXL image: ", output.first ?? "")
            imageUrl = output.first
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print("Could not create SDXL image: \(error.localizedDescription)")
        }
    }
    
    var body: some View {
        VStack{
                
            VStack{
                if (imageUrl != nil) {
                    AsyncImage(url: imageUrl) { phase in
                        if let image = phase.image {
                            image
                                .resizable()
                                .aspectRatio(contentMode: .fit)
                        } else if phase.error != nil {
                            Text("Failed to load image")
                                .foregroundColor(.red)
                        } else {
                            ProgressView()
                        }
                    }
                } else{
                    ContentUnavailableView(
                        "Generate an image",
                        systemImage: "photo.fill",
                        description: Text("Write a prompt below")
                    )
                }
            }
            .frame(maxHeight: .infinity)
            
            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(.white)
                    .cornerRadius(8)
                    .shadow(radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Generate Image")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    ImageGenView()
}



================================================
FILE: AIProxyReplicate/AIProxyReplicate/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyReplicate/AIProxyReplicate/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyReplicate/AIProxyReplicate/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyReplicate/AIProxyReplicate/Assets.xcassets/replicate.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "replicate.jpeg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyReplicate/AIProxyReplicate/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/AIProxyStabilityAIApp.swift
================================================
//
//  AIProxyStabilityAIApp.swift
//  AIProxyStabilityAI
//
//  Created by Todd Hamilton on 8/13/24.
//

import SwiftUI

@main
struct AIProxyStabilityAIApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyStabilityAI
//
//  Created by Todd Hamilton on 8/13/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let stabilityService = AIProxy.stabilityAIDirectService(
    unprotectedAPIKey: "your-stability-key"
)

/* Uncomment for all other production use cases */
//let stabilityService = AIProxy.stabilityAIService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyStabilityAI
//
//  Created by Todd Hamilton on 8/13/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            
            VStack(spacing:48){
                VStack{
                    Image("stability")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("Stability.ai")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Sample")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Generate Image Example",destination: ImageGenView())
                        .bold()
                        .controlSize(.large)
                        .tint(.indigo)
                        .buttonStyle(.bordered)
                }
            }
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/ImageGenView.swift
================================================
//
//  ImageGenView.swift
//  AIProxyStabilityAI
//
//  Created by Todd Hamilton on 8/13/24.
//

import SwiftUI
import AIProxy

struct ImageGenView: View {
    
    @State private var prompt = ""
    @State private var image: UIImage? = nil
    @State private var isLoading = false
    
    func generate() async throws {
        isLoading = true  // Start loading
        defer { isLoading = false }
        do {
            let body = StabilityAIUltraRequestBody(prompt: prompt)
            
            // This demo is of text-to-image, which only requires a prompt
            // To use image-to-image the following parameters are required:
            // prompt - text to generate the image from
            // image - the image to use as the starting point for the generation
            // strength - controls how much influence the image parameter has on the output image
            // mode - must be set to image-to-image
            // Learn more: https://platform.stability.ai/docs/api-reference#tag/Generate
            
            let response = try await stabilityService.ultraRequest(body: body)
            image = UIImage(data: response.imageData)
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    var body: some View {
        VStack{
                
            VStack{
                if (image != nil) {
                    Image(uiImage: image!)
                        .resizable()
                        .aspectRatio(contentMode: .fit)
                        .frame(maxHeight: UIScreen.main.bounds.width)
                } else{
                    ContentUnavailableView(
                        "Generate an image",
                        systemImage: "photo.fill",
                        description: Text("Write a prompt below")
                    )
                }
            }
            .frame(maxHeight: .infinity)
            
            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(Color(.systemBackground))
                    .cornerRadius(8)
                    .shadow(color:.primary, radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Image")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Generate Image")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    ImageGenView()
}



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/Assets.xcassets/stability.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "stability.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyStabilityAI/AIProxyStabilityAI/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/AIProxyTogetherAIApp.swift
================================================
//
//  AIProxyTogetherAIApp.swift
//  AIProxyTogetherAI
//
//  Created by Todd Hamilton on 8/18/24.
//

import SwiftUI

@main
struct AIProxyTogetherAIApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyTogetherAI
//
//  Created by Todd Hamilton on 8/18/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let togetherAIService = AIProxy.togetherAIDirectService(
    unprotectedAPIKey: "your-togetherAI-key"
)

/* Uncomment for all other production use cases */
//let togetherAIService = AIProxy.togetherAIService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/ChatView.swift
================================================
//
//  ChatView.swift
//  AIProxyTogetherAI
//
//  Created by Todd Hamilton on 8/18/24.
//

import SwiftUI
import AIProxy

struct ChatView: View {
    
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let requestBody = TogetherAIChatCompletionRequestBody(
                messages: [TogetherAIMessage(content: prompt, role: .user)],
                model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
            )
            let response = try await togetherAIService.chatCompletionRequest(body: requestBody)
            print(response.choices.first?.message.content ?? "")
            result = response.choices.first?.message.content ?? ""
            showingAlert = true
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print("Could not create TogetherAI chat completion: \(error.localizedDescription)")
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate Text",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text(result),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(.white)
                    .cornerRadius(8)
                    .shadow(radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Chat Example")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    ChatView()
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/ContentView.swift
================================================
//
//  ContentView.swift
//  AIProxyTogetherAI
//
//  Created by Todd Hamilton on 8/18/24.
//

import SwiftUI

struct ContentView: View {
    var body: some View {
        NavigationStack{
            
            VStack(spacing:48){
                VStack{
                    Image("togetherai")
                        .resizable()
                        .scaledToFit()
                        .frame(width: /*@START_MENU_TOKEN@*/100/*@END_MENU_TOKEN@*/)
                        .cornerRadius(14)
                        .foregroundColor(.primary)
                    Text("Together AI")
                        .bold()
                        .font(.largeTitle)
                    Text("AIProxy Sample")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .frame(maxWidth:.infinity,alignment:.center)
                
                VStack{
                    NavigationLink("Chat Example",destination: ChatView())
                        .bold()
                        .controlSize(.large)
                        .tint(.blue)
                        .buttonStyle(.bordered)
                    NavigationLink("Streaming Chat Example",destination: StreamingChatView())
                        .bold()
                        .controlSize(.large)
                        .tint(.blue)
                        .buttonStyle(.bordered)
                    NavigationLink("JSON Response",destination: JSONResponseView())
                        .bold()
                        .controlSize(.large)
                        .tint(.blue)
                        .buttonStyle(.bordered)
                    
                }
            }
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/JSONResponseView.swift
================================================
//
//  JSONResponseView.swift
//  AIProxyTogetherAI
//
//  Created by Todd Hamilton on 8/18/24.
//

import SwiftUI
import AIProxy

struct JSONResponseView: View {
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let schema: [String: AIProxyJSONValue] = [
                "type": "object",
                "properties": [
                    "people": [
                        "type": "array",
                        "items": [
                            "type": "object",
                            "properties": [
                                "name": [
                                    "type": "string",
                                    "description": "The name of the person"
                                ],
                                "address": [
                                    "type": "string",
                                    "description": "The address of the person"
                                ]
                            ],
                            "required": ["name", "address"]
                        ]
                    ]
                ]
            ]
            let requestBody = TogetherAIChatCompletionRequestBody(
                messages: [
                    TogetherAIMessage(
                        content: "You are a helpful assistant that answers in JSON",
                        role: .system
                    ),
                    TogetherAIMessage(
                        content: prompt,
                        role: .user
                    )
                ],
                model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
                responseFormat: .json(schema: schema)
            )
            let response = try await togetherAIService.chatCompletionRequest(body: requestBody)
            print(response.choices.first?.message.content ?? "")
            result = response.choices.first?.message.content ?? ""
            showingAlert = true
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print("Could not create TogetherAI JSON chat completion: \(error.localizedDescription)")
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate JSON",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text(result),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(.white)
                    .cornerRadius(8)
                    .shadow(radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate JSON")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("JSON Example")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    JSONResponseView()
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/StreamingChatView.swift
================================================
//
//  StreamingChatView.swift
//  AIProxyTogetherAI
//
//  Created by Todd Hamilton on 8/18/24.
//

import SwiftUI
import AIProxy

struct StreamingChatView: View {
    @State private var prompt = ""
    @State private var result = ""
    @State private var isLoading = false
    @State private var showingAlert = false
    
    func generate() async throws {
        isLoading = true
        defer { isLoading = false }
        do {
            let requestBody = TogetherAIChatCompletionRequestBody(
                messages: [TogetherAIMessage(content: prompt, role: .user)],
                model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
            )
            let stream = try await togetherAIService.streamingChatCompletionRequest(body: requestBody)
            for try await chunk in stream {
                print(chunk.choices.first?.delta.content ?? "")
            }
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print("Could not create TogetherAI streaming chat completion: \(error.localizedDescription)")
        }
    }
    
    var body: some View {
        VStack {
            VStack{
                ContentUnavailableView(
                    "Generate Text",
                    systemImage: "doc.plaintext.fill",
                    description: Text("Write a prompt below")
                )
            }
            .alert(isPresented: $showingAlert) {
                Alert(
                    title: Text("Result"),
                    message: Text("View streaming response in the Xcode console."),
                    dismissButton: .default(Text("Close"))
                )
            }

            Spacer()
            
            VStack(spacing:12){
                TextField("Type a prompt", text:$prompt)
                    .submitLabel(.go)
                    .padding(12)
                    .background(.white)
                    .cornerRadius(8)
                    .shadow(radius: 1)
                    .onSubmit {
                        Task{ try await generate() }
                    }
                Button{
                    showingAlert = true
                    Task{ try await generate() }
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.regular)
                            .frame(maxWidth:.infinity)
                    } else {
                        Text("Generate Text")
                            .bold()
                            .frame(maxWidth:.infinity)
                    }
                }
                .controlSize(.large)
                .buttonStyle(.borderedProminent)
                .disabled(isLoading ? true : false)
            }
        }
        .padding()
        .navigationTitle("Streaming Chat Example")
        .navigationBarTitleDisplayMode(.inline)
    }
}

#Preview {
    StreamingChatView()
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/Assets.xcassets/togetherai.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "togetherai.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: AIProxyTogetherAI/AIProxyTogetherAI/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/AIColorPalette/README.md
================================================
## Example projects

### AIColorPalette

#### About

AIColorPalette generates a color palette from a photo in your camera roll. This project uses [AIProxy](https://www.aiproxy.pro) to secure your OpenAI key.

#### Features demonstrated

- Shows off new iOS 18 SwiftUI effects
- Calls the OpenAI chat completion endpoint
- Submits a photo as the chat completion request body
- Compels OpenAI to return valid JSON in the chat completion response

#### Minimum requirements

The minimum deployment target for this sample app is 18.0, as it uses beta UI effects.

You'll need:

- macOS Sonoma 14.5 or higher
- Xcode Beta 16.0 or higher

#### How to run with your own AIProxy settings

- Set the `AIPROXY_DEVICE_CHECK_BYPASS` environment variable in your Xcode build settings.
  Refer to the [README](https://github.com/lzell/AIProxySwift?tab=readme-ov-file#adding-this-package-as-a-dependency-to-your-xcode-project)
  for instructions on adding an env variable to your Xcode project.

- Replace the `partialKey` placeholder value in `AIColorPalette/AIProxyIntegration.swift` with the
  value provided to you in the AIProxy dashboard when you submit your OpenAI key



================================================
FILE: Demos/AIColorPalette/AIColorPalette/AIColorPaletteApp.swift
================================================
//
//  AIColorPaletteApp.swift
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/20/24.
//

import SwiftUI

@main
struct AIColorPaletteApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/AIProxyIntegration.swift
================================================
//
//  AIProxyIntegration.swift
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/20/24.
//

import AIProxy // The AIProxy SPM package is found at https://github.com/lzell/AIProxySwift
import Foundation
import UIKit

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let openAIService = AIProxy.openAIDirectService(
    unprotectedAPIKey: "your-openai-key"
)

/* Uncomment for all other production use cases */
//let openAIService = AIProxy.openAIService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)

struct AIProxyIntegration {

    static func getColorPalette(forImage image: UIImage) async -> String? {
        let message = "generate a color palette based on the provided image, return 4 colors in valid JSON, nothing else. Here's an example of the JSON format: 'colors': [{red: 0.85, green: 0.85, blue: 0.85}, {red: 0.85, green: 0.85, blue: 0.85}, {red: 0.85, green: 0.85, blue: 0.85}, {red: 0.85, green: 0.85, blue: 0.85}, {red: 0.85, green: 0.85, blue: 0.85}]."

        let localURL = createOpenAILocalURL(forImage: image)!
        do {
            let response = try await openAIService.chatCompletionRequest(body: .init(
                model: "gpt-4o",
                messages: [
                    .system(
                        content: .text(message)
                    ),
                    .user(
                        content: .parts(
                            [
                                .imageURL(localURL, detail: .auto)
                            ]
                        )
                    )
                ],
                responseFormat: .jsonObject
            ))
            return response.choices.first?.message.content
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
        return nil
    }

    private init() {
        fatalError("This type is not intended to be instantiated")
    }
}

func createOpenAILocalURL(forImage image: UIImage) -> URL? {
    // Attempt to get JPEG data from the UIImage
    guard let jpegData = image.jpegData(compressionQuality: 0.4) else {
        return nil
    }

    // Encode the JPEG data to a base64 string
    let base64String = jpegData.base64EncodedString()

    // Create the data URL string
    let urlString = "data:image/jpeg;base64,\(base64String)"

    // Return the URL constructed from the data URL string
    return URL(string: urlString)
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/ColorData.swift
================================================
//
//  ColorData.swift
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/21/24.
//

import SwiftUI

// Define the structure for the JSON data
struct ColorData: Codable {
    let red: Double
    let green: Double
    let blue: Double

    enum CodingKeys: String, CodingKey {
        case red
        case green
        case blue
    }
}

struct Colors: Codable {
    let colors: [ColorData]
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/ColorDetailView.swift
================================================
//
//  ColorDetailView.swift
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/22/24.
//

import SwiftUI

struct ColorDetailView: View {

    @Binding var currentColor:ColorData
    @State var copiedToClipboard:Bool = false
    @State private var hexValue:String = ""
    @State private var rgbValue:String = ""

    var body: some View {
        VStack(spacing:16){
            ZStack{
                Circle()
                    .fill(.white)
                    .frame(width:68)
                    .shadow(radius: 2, x: 0, y: 2)
                Circle()
                    .fill(Color(red:currentColor.red, green: currentColor.green, blue: currentColor.blue))
                    .frame(width:60)
            }
            .padding(.vertical, 16)

            HStack{
                Text(hexValue)
                    .fontWeight(.medium)
                Spacer()
                Button{
                    copyToClipboard(copyItem: hexValue)
                }label:{
                    Image(systemName: "square.on.square")
                }
                .buttonStyle(.bordered)
                .tint(Color(red:currentColor.red, green: currentColor.green, blue: currentColor.blue))
                .sensoryFeedback(.success, trigger: copiedToClipboard)
            }
            Divider()
            HStack{
                Text(rgbValue)
                    .fontWeight(.medium)
                Spacer()
                Button{
                    copyToClipboard(copyItem: rgbValue)
                }label:{
                    Image(systemName: "square.on.square")
                }
                .buttonStyle(.bordered)
                .tint(Color(red:currentColor.red, green: currentColor.green, blue: currentColor.blue))
                .sensoryFeedback(.success, trigger: copiedToClipboard)
            }
            Spacer()

        }
        .padding()
        .overlay(
            ZStack{
                if copiedToClipboard {
                    Text("Copied to clipboard")
                        .fontWeight(.medium)
                        .foregroundColor(.white)
                        .padding()
                        .background(Color.black.opacity(0.75).cornerRadius(20))
                        .padding(.bottom)
                        .shadow(radius: 5)
                        .transition(.move(edge: .bottom))
                        .frame(maxHeight:.infinity, alignment:.bottom)
                }
            }
        )
        .onAppear{
            hexValue = rgbToHex(red: currentColor.red, green: currentColor.green, blue: currentColor.blue)
            rgbValue = "rgb(\(String(format: "%.2f", currentColor.red)), \(String(format: "%.2f", currentColor.green)), \(String(format: "%.2f", currentColor.blue)))"
        }
    }


    func rgbToHex(red: CGFloat, green: CGFloat, blue: CGFloat) -> String {
        let r = Int(red * 255.0)
        let g = Int(green * 255.0)
        let b = Int(blue * 255.0)
        return String(format: "#%02X%02X%02X", r, g, b)
    }

    func copyToClipboard(copyItem: String) {
        UIPasteboard.general.string = copyItem
        withAnimation(.snappy){
            copiedToClipboard = true
        }
        DispatchQueue.main.asyncAfter(deadline: .now() + 2){
            withAnimation(.snappy){
                copiedToClipboard = false
            }
        }
    }

}

#Preview {
    ZStack{
        Image("palm")
            .resizable()
            .scaledToFill()
            .ignoresSafeArea()
            .frame(width:UIScreen.main.bounds.width)
        ColorDetailView(currentColor: .constant(ColorData(red: 0.5, green:0.2, blue:0.3)))
            .background(.thickMaterial)
    }
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/ContentView.swift
================================================
//
//  ContentView.swift
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/20/24.
//

import SwiftUI
import PhotosUI

struct ContentView: View {

    @State private var presentSheet = false
    @State private var sheetColor:ColorData = ColorData(red: 1.0, green:1.0, blue:1.0)
    @State private var copiedToClipboard = false

    @State var counter: Int = 0
    @State var origin: CGPoint = .zero

    @State private var sampleImage: UIImage? = UIImage(named: "palm")
    @State private var photosPickerItem: PhotosPickerItem?
    @State private var loading = false
    @State private var loadingIndicator = false
    @State private var sampleColors:Colors = Colors(
        colors:
            [
                ColorData(red: 0.28, green: 0.28, blue: 0.20),
                ColorData(red: 0.72, green: 0.75, blue: 0.73),
                ColorData(red: 0.08, green: 0.54, blue: 0.63),
                ColorData(red: 0.59, green: 0.49, blue: 0.35)
            ]
    )

    var body: some View {

        ZStack{
            MeshGradient(
                width: 2,
                height: 2,
                points: [
                    [0, 0], [1, 0],
                    [0, 1], [1, 1],
                ], colors: createMeshColors())
            .ignoresSafeArea()

            VStack(spacing:48){

                // Sample image
                Image(uiImage: sampleImage!)
                    .resizable()
                    .scaledToFill()
                    .modifier(RippleEffect(at: origin, trigger: counter))
                    .frame(maxWidth:320, maxHeight:360)
                    .cornerRadius(24)
                    .clipped()
                    .shadow(radius: 10)
                    .overlay(
                        ZStack{
                            Color
                                .black
                                .opacity(0.75)
                            Text("Generating palette...")
                                .bold()
                                .foregroundStyle(.white)
                        }
                            .cornerRadius(24)
                            .clipped()
                            .opacity(loadingIndicator ? 1 : 0)
                    )

                VStack(spacing: 48){
                    ZStack{
                        // Color swatches
                        HStack(spacing:loading ? -60 : 20){
                            ForEach(Array(sampleColors.colors.enumerated()), id: \.offset) { index, color in
                                Circle()
                                    .stroke(.white, lineWidth: 8)
                                    .fill(Color(red: color.red,green: color.green,blue: color.blue))
                                    .frame(width:60)
                                    .onTapGesture {
                                        sheetColor = color
                                        presentSheet.toggle()
                                    }
                            }
                        }

                        if loadingIndicator{
                            Circle()
                                .fill(.white)
                                .stroke(.white, lineWidth:8)
                                .frame(width:60)
                                .overlay(
                                    ProgressView()
                                        .tint(.black)
                                )
                        }
                    }

                    PhotosPicker(selection: $photosPickerItem, matching: .images){
                        Text("Choose Photo")
                            .bold()
                    }
                    .buttonStyle(.borderedProminent)
                    .tint(.black)
                    .controlSize(.extraLarge)
                }

            }

        }
        .sheet(isPresented: $presentSheet) {
            ColorDetailView(currentColor: $sheetColor)
                .presentationDetents([.medium])
                .presentationBackground(.thickMaterial)

        }
        .onChange(of: photosPickerItem){ _, _ in
            Task{
                loading = true
                loadingIndicator = true
                if let photosPickerItem,
                   let data = try? await photosPickerItem.loadTransferable(type: Data.self){
                    if let image = UIImage(data: data){
                        withAnimation(){
                            sampleImage = image
                        }
                        try await generateColorPalette(image: sampleImage!)
                    }
                }
                withAnimation(){
                    loading = false
                }
                withAnimation(.bouncy){
                    loadingIndicator = false
                }
                counter += 1
            }
        }

    }

    func createMeshColors() -> [Color] {
        var meshColors: [Color] = []
        for color in sampleColors.colors {
            meshColors.append(Color(red: color.red, green: color.green, blue: color.blue))
        }
        return meshColors
    }

    func generateColorPalette(image: UIImage) async throws  {
        let jsonString = await AIProxyIntegration.getColorPalette(forImage: image)
        let jsonData = jsonString!.data(using: .utf8)!

        do {
            sampleColors = try JSONDecoder().decode(Colors.self, from: jsonData)
        } catch {
            fatalError("Failed to decode JSON: \(error)")
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/Ripple.metal
================================================
//
//  Ripple.metal
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/21/24.
//

// Insert #include <metal_stdlib>
#include <SwiftUI/SwiftUI.h>
using namespace metal;

[[ stitchable ]]
half4 Ripple(
    float2 position,
    SwiftUI::Layer layer,
    float2 origin,
    float time,
    float amplitude,
    float frequency,
    float decay,
    float speed
) {
    // The distance of the current pixel position from `origin`.
    float distance = length(position - origin);
    // The amount of time it takes for the ripple to arrive at the current pixel position.
    float delay = distance / speed;

    // Adjust for delay, clamp to 0.
    time -= delay;
    time = max(0.0, time);

    // The ripple is a sine wave that Metal scales by an exponential decay
    // function.
    float rippleAmount = amplitude * sin(frequency * time) * exp(-decay * time);

    // A vector of length `amplitude` that points away from position.
    float2 n = normalize(position - origin);

    // Scale `n` by the ripple amount at the current pixel position and add it
    // to the current pixel position.
    //
    // This new position moves toward or away from `origin` based on the
    // sign and magnitude of `rippleAmount`.
    float2 newPosition = position + rippleAmount * n;

    // Sample the layer at the new position.
    half4 color = layer.sample(newPosition);

    // Lighten or darken the color based on the ripple amount and its alpha
    // component.
    color.rgb += 0.3 * (rippleAmount / amplitude) * color.a;

    return color;
}





================================================
FILE: Demos/AIColorPalette/AIColorPalette/Ripple.swift
================================================
//
//  Ripple.swift
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/21/24.
//

import SwiftUI

struct PushEffect<T: Equatable>: ViewModifier {
    var trigger: T

    func body(content: Content) -> some View {
        content.keyframeAnimator(
            initialValue: 1.0,
            trigger: trigger
        ) { view, value in
            view.visualEffect { view, _ in
                view.scaleEffect(value)
            }
        } keyframes: { _ in
            SpringKeyframe(0.95, duration: 0.2, spring: .snappy)
            SpringKeyframe(1.0, duration: 0.2, spring: .bouncy)
        }
    }
}

/// A modifer that performs a ripple effect to its content whenever its
/// trigger value changes.
struct RippleEffect<T: Equatable>: ViewModifier {
    var origin: CGPoint

    var trigger: T

    init(at origin: CGPoint, trigger: T) {
        self.origin = origin
        self.trigger = trigger
    }

    func body(content: Content) -> some View {
        let origin = origin
        let duration = duration

        content.keyframeAnimator(
            initialValue: 0,
            trigger: trigger
        ) { view, elapsedTime in
            view.modifier(RippleModifier(
                origin: origin,
                elapsedTime: elapsedTime,
                duration: duration
            ))
        } keyframes: { _ in
            MoveKeyframe(0)
            LinearKeyframe(duration, duration: duration)
        }
    }

    var duration: TimeInterval { 3 }
}

/// A modifier that applies a ripple effect to its content.
struct RippleModifier: ViewModifier {
    var origin: CGPoint

    var elapsedTime: TimeInterval

    var duration: TimeInterval

    var amplitude: Double = 12
    var frequency: Double = 15
    var decay: Double = 8
    var speed: Double = 1200

    func body(content: Content) -> some View {
        let shader = ShaderLibrary.Ripple(
            .float2(origin),
            .float(elapsedTime),

            // Parameters
            .float(amplitude),
            .float(frequency),
            .float(decay),
            .float(speed)
        )

        let maxSampleOffset = maxSampleOffset
        let elapsedTime = elapsedTime
        let duration = duration

        content.visualEffect { view, _ in
            view.layerEffect(
                shader,
                maxSampleOffset: maxSampleOffset,
                isEnabled: 0 < elapsedTime && elapsedTime < duration
            )
        }
    }

    var maxSampleOffset: CGSize {
        CGSize(width: amplitude, height: amplitude)
    }
}

extension View {
    func onPressingChanged(_ action: @escaping (CGPoint?) -> Void) -> some View {
        modifier(SpatialPressingGestureModifier(action: action))
    }
}

struct SpatialPressingGestureModifier: ViewModifier {
    var onPressingChanged: (CGPoint?) -> Void

    @State var currentLocation: CGPoint?

    init(action: @escaping (CGPoint?) -> Void) {
        self.onPressingChanged = action
    }

    func body(content: Content) -> some View {
        let gesture = SpatialPressingGesture(location: $currentLocation)

        content
            .gesture(gesture)
            .onChange(of: currentLocation, initial: false) { _, location in
                onPressingChanged(location)
            }
    }
}

struct SpatialPressingGesture: UIGestureRecognizerRepresentable {
    final class Coordinator: NSObject, UIGestureRecognizerDelegate {
        @objc
        func gestureRecognizer(
            _ gestureRecognizer: UIGestureRecognizer,
            shouldRecognizeSimultaneouslyWith other: UIGestureRecognizer
        ) -> Bool {
            true
        }
    }

    @Binding var location: CGPoint?

    func makeCoordinator(converter: CoordinateSpaceConverter) -> Coordinator {
        Coordinator()
    }

    func makeUIGestureRecognizer(context: Context) -> UILongPressGestureRecognizer {
        let recognizer = UILongPressGestureRecognizer()
        recognizer.minimumPressDuration = 0
        recognizer.delegate = context.coordinator

        return recognizer
    }

    func handleUIGestureRecognizerAction(
        _ recognizer: UIGestureRecognizerType, context: Context) {
            switch recognizer.state {
                case .began:
                    location = context.converter.localLocation
                case .ended, .cancelled, .failed:
                    location = nil
                default:
                    break
            }
        }
    }



================================================
FILE: Demos/AIColorPalette/AIColorPalette/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/Assets.xcassets/palm.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "palm.jpg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/AIColorPalette/AIColorPalette/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Chat/Chat/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import AIProxy

enum AppConstants {
    #error(
        """
        Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
        Please see https://www.aiproxy.pro/docs/integration-guide.html")
        """
    )
    
    /* Uncomment for BYOK use cases */
    static let openAIService = AIProxy.openAIDirectService(
        unprotectedAPIKey: "your-openai-key"
    )

    /* Uncomment for all other production use cases */
//    static let openAIService = AIProxy.openAIService(
//        partialKey: "partial-key-from-your-developer-dashboard",
//        serviceURL: "service-url-from-your-developer-dashboard"
//    )
}



================================================
FILE: Demos/Chat/Chat/AppLogger.swift
================================================
//
//  AppLogger.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import OSLog

/// Log levels available:
///
///     AppLogger.debug
///     AppLogger.info
///     AppLogger.warning
///     AppLogger.error
///     AppLogger.critical
///
/// Flip on metadata logging in Xcode's console to show which source line the log occurred from.
///
/// See my reddit post for a video instructions:
/// https://www.reddit.com/r/SwiftUI/comments/15lsdtk/how_to_use_the_oslog_logger/
let AppLogger = Logger(subsystem: Bundle.main.bundleIdentifier ?? "UnknownApp",
                       category: "AIProxyBootstrapChat")



================================================
FILE: Demos/Chat/Chat/ChatApp.swift
================================================
//
//  ChatApp.swift
//  Chat
//
//  Created by Lou Zell
//

import SwiftUI

@main
@MainActor
struct ChatApp: App {

    @State private var chatManager = ChatManager()

    var body: some Scene {
        WindowGroup {
            ChatView(chatManager: chatManager)
        }
    }
}



================================================
FILE: Demos/Chat/Chat/ChatBubble.swift
================================================
//
//  ChatBubbleView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

/// A view to contain a single message from either the user or OpenAI.
struct ChatBubble: View {

    /// The message to display
    let message: ChatMessage

    /// Whether to animate in the chat bubble
    let animateIn: Bool

    /// State used to animate in the chat bubble if `animateIn` is true
    @State private var animationTrigger = false

    var body: some View {
        HStack(alignment: .top, spacing: 12) {
            chatIcon
            VStack(alignment: .leading) {
                chatName
                chatBody
            }
        }
        .opacity(bubbleOpacity)
        .animation(.easeIn(duration: 0.75), value: animationTrigger)
        .onAppear {
            adjustAnimationTriggerIfNecessary()
        }
    }

    private var bubbleOpacity: Double {
        guard animateIn else {
            return 1
        }
        return animationTrigger ? 1 : 0
    }

    private func adjustAnimationTriggerIfNecessary() {
        guard animateIn else {
            return
        }
        animationTrigger = true
    }

    private var chatIcon: some View {
        Image(systemName: message.isUser ? "person.circle.fill" : "command.circle.fill")
            .font(.title2)
            .frame(width:24, height:24)
            .foregroundColor(message.isUser ? .primary : .teal)
    }

    private var chatName: some View {
        Text(message.isUser ? "You" : "ChatGPT")
            .fontWeight(.bold)
            .frame(maxWidth: .infinity, maxHeight:24, alignment: .leading)
    }

    @ViewBuilder
    private var chatBody: some View {
        if message.isUser {
            Text(LocalizedStringKey(message.text))
                .fixedSize(horizontal: false, vertical: true)
                .foregroundColor(.primary)
        } else {
            if message.isWaitingForFirstText {
                ProgressView()
            } else {
                Text(LocalizedStringKey(message.text))
                    .fixedSize(horizontal: false, vertical: true)
                    .foregroundColor(.primary)
            }
        }
    }
}

#Preview {
    ChatBubble(message: ChatMessage(text: "hello", isUser: false), animateIn: false)
        .frame(maxWidth:.infinity)
        .padding()
}



================================================
FILE: Demos/Chat/Chat/ChatDataLoader.swift
================================================
//
//  ChatDataLoader.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import AIProxy

enum ChatDataLoaderError: Error {
    case busy
}

/// Asynchronously sends prompts to OpenAI and streams back the response
final actor ChatDataLoader {
    private var streamingResponseAccumulator: String?

    /// All chat messages, including user queries and openai responses.
    /// The full history of the chat is sent with each request to openai to provide an ongoing conversation with memory.
    private var messages = [OpenAIChatCompletionMessage]()

    /// Add a user message to the conversation and stream back the openai response
    func addToConversation(_ prompt: String) async throws -> AsyncThrowingStream<String, Error> {
        guard streamingResponseAccumulator == nil else {
            throw ChatDataLoaderError.busy
        }
        self.streamingResponseAccumulator = ""
        
        self.messages.append((.user(content: .text(prompt))))
        let requestBody = OpenAIChatCompletionRequestBody(
            model: "gpt-4o-mini",
            messages: [.user(content: .text(prompt))]
        )
        let stream = try await AppConstants.openAIService.streamingChatCompletionRequest(body: requestBody)

        return AsyncThrowingStream { continuation in
            let task = Task {
                for try await result in stream {
                    guard let choice = result.choices.first,
                          let content = choice.delta.content else
                    {
                        self.addAccumulatedResponseToMessageHistory()
                        continuation.finish()
                        return
                    }

                    self.addToResponseAccumulator(text: content)
                    continuation.yield(content)
                }
            }

            continuation.onTermination = { @Sendable termination in
                task.cancel()
                if case .cancelled = termination {
                    Task {
                        await self.addAccumulatedResponseToMessageHistory()
                    }
                }
            }
        }
    }


    private func addAccumulatedResponseToMessageHistory() {
        if let accumulator = self.streamingResponseAccumulator {
            self.messages.append(.assistant(content: .text(accumulator)))
            self.streamingResponseAccumulator = nil
        }
    }

    private func addToResponseAccumulator(text: String) {
        if let accumulator = self.streamingResponseAccumulator {
            self.streamingResponseAccumulator = accumulator + text
        } else {
            self.streamingResponseAccumulator = text
        }
    }
}



================================================
FILE: Demos/Chat/Chat/ChatInputView.swift
================================================
//
//  ChatInputView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

/// A view for the user to enter chat messages
struct ChatInputView: View {

    private enum FocusedField {
        case newMessageText
    }

    /// Is a streaming chat response in progress
    let isStreamingResponse: Bool

    /// Callback invoked when the user taps the submit button or presses return
    var didSubmit: (String) -> Void

    /// Callback invoked when the user taps on the stop button
    var didTapStop: () -> Void

    /// State to collect new text messages
    @State private var newMessageText: String = ""
    @FocusState private var focusedField: FocusedField?

    var body: some View {
        HStack(spacing:0){
            chatInputTextField
            actionButton
        }
        .padding(8)
    }

    private var chatInputTextField: some View {
        TextField("Type a message", text: $newMessageText, axis: .vertical)
            .focused($focusedField, equals: .newMessageText)
            .lineLimit(5)
            .padding(.horizontal, 16)
            .padding(.vertical, 10)
            .background(
                RoundedRectangle(cornerRadius:30)
                    .fill(Color(.tertiarySystemGroupedBackground))
                    .stroke(.separator)
            )
            .onAppear {
                focusedField = .newMessageText
            }
            .onSubmit {
                didSubmit(newMessageText)
                newMessageText = ""
            }
    }

    private var actionButton: some View {
        Button {
            if isStreamingResponse {
                didTapStop()
            } else {
                didSubmit(newMessageText)
                newMessageText = ""
            }
        } label:{
            Image(systemName: isStreamingResponse ? "stop.circle.fill" : "arrow.up.circle.fill")
                .font(.title)
                .foregroundColor((isStreamingResponse || !newMessageText.isEmpty) ? .primary : .secondary)
                .frame(width:40, height:40)
        }
        .contentTransition(.symbolEffect(.replace))
        .padding(.horizontal, 8)
    }
}

#Preview {
    ChatInputView(isStreamingResponse: false, didSubmit: { _ in }, didTapStop: { })
}



================================================
FILE: Demos/Chat/Chat/ChatManager.swift
================================================
//
//  ChatManager.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI

@MainActor
@Observable
final class ChatManager {

    /// Messages sent from the user or received from OpenAI
    var messages = [ChatMessage]()

    /// Returns true if OpenAI is still streaming a response back to us
    var isProcessing: Bool {
        return self.streamTask != nil
    }

    /// Task that encapsulates OpenAI's streaming response.
    /// Cancel this to interrupt OpenAI's response.
    private var streamTask: Task<Void, Never>? = nil
    private let chatDataLoader = ChatDataLoader()

    /// Send a new message to OpenAI and start streaming OpenAI's response
    func send(message: ChatMessage) {
        self.messages.append(message)
        self.setupStreamingTask(withPrompt: message.text)
    }

    /// Stop the streaming response from OpenAI
    func stop() {
        self.streamTask?.cancel()
        self.streamTask = nil
    }

    private func setupStreamingTask(withPrompt prompt: String) {
        self.messages.append(ChatMessage(text: "", isUser: false, isWaitingForFirstText: true))
        self.streamTask = Task { [weak self] in
            guard let this = self else { return }
            do {
                let responseStream = try await this.chatDataLoader.addToConversation(prompt)
                for try await responseText in responseStream {
                    if var last = this.messages.popLast() {
                        last.isWaitingForFirstText = false
                        last.text += responseText
                        this.messages.append(last)
                    }
                }
                this.streamTask = nil
            } catch {
                AppLogger.error("Received an unexpected error from OpenAI streaming: \(error)")
            }
        }
    }
}



================================================
FILE: Demos/Chat/Chat/ChatMessage.swift
================================================
//
//  ChatMessage.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation

/// Data model to represent a chat message
struct ChatMessage: Identifiable, Equatable {
    /// Unique identifier
    let id = UUID()

    /// The body of the chat message
    var text: String

    /// True if the message originates from the user, false if it originates from OpenAI
    let isUser: Bool

    /// Indicates that we are waiting for the first bit of message content from OpenAI
    var isWaitingForFirstText = false
}



================================================
FILE: Demos/Chat/Chat/ChatView.swift
================================================
//
//  ChatView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

@MainActor
struct ChatView: View {

    private enum ScrollChange {
        case userInteraction
        case programmatic
    }

    /// The chat manager that controls communication with OpenAI
    let chatManager: ChatManager

    /// Should we auto-scroll the scrollview as message content arrives
    @State private var isTrackingScrollView = false

    /// As the scroll position changes, this state indicates whether the change originated from a user interation or programmatically
    @State private var scrollChange: ScrollChange = .userInteraction

    /// The user's scroll position of the content view (e.g. where in the chat message contents they are positioned)
    @State private var scrollPosition: CGPoint = .zero {
        didSet {
            switch scrollChange {
            case .userInteraction:
                isTrackingScrollView = false
            case .programmatic:
                scrollChange = .userInteraction
            }
        }
    }

    /// The height of the scroll view (e.g. the frame's height)
    @State private var scrollViewHeight: CGFloat = .infinity

    /// The height of the contents contained within the scroll view
    @State private var scrollViewContentHeight: CGFloat = .zero

    @State private var scrollTrigger = false

    var body: some View {
        VStack {
            autoScrollingChatView
                .padding([.top, .leading, .trailing])

            ZStack(alignment: .top) {

                AutoScrollButton(isVisible: !isTrackingScrollView && scrollViewContentHeight > scrollViewHeight) {
                    isTrackingScrollView = true
                    scrollTrigger.toggle()
                }
                .offset(y: -50)

                ChatInputView(isStreamingResponse: chatManager.isProcessing,
                              didSubmit: { sendMessage($0) },
                              didTapStop: { chatManager.stop() })
            }
        }
    }

    private var autoScrollingChatView: some View {
        ScrollViewReader { scrollView in
            ScrollView(.vertical) {
                ChatMessagesView(chatMessages: chatManager.messages)
                    .id("MessagesView")
                    .background(GeometryReader { geometry in
                        Color.clear
                            .preference(key: ScrollOffsetPreferenceKey.self,
                                        value: geometry.frame(in: .named("chatScrollView")).origin)
                            .preference(key: ScrollHeightPreferenceKey.self,
                                        value: geometry.size.height)
                    })
                    .onPreferenceChange(ScrollOffsetPreferenceKey.self) { value in
                        scrollPosition = value
                    }
                    .onPreferenceChange(ScrollHeightPreferenceKey.self) { value in
                        scrollViewContentHeight = value
                    }
            }
            .coordinateSpace(name: "chatScrollView")
            .background(
                GeometryReader { outerScrollViewGeometry in
                    Color.clear
                        .preference(key: ScrollHeightPreferenceKey.self,
                                    value: outerScrollViewGeometry.size.height)
                }
            )
            .onPreferenceChange(ScrollHeightPreferenceKey.self) { value in
                scrollViewHeight = value
            }
            .onChange(of: scrollTrigger) {
                scrollChange = .programmatic
                scrollView.scrollTo("MessagesView", anchor: .bottom)
            }
            .onChange(of: chatManager.messages) { _, _ in
                if isTrackingScrollView {
                    scrollTrigger.toggle()
                }
            }
        }
    }

    private func sendMessage(_ message: String) {
        guard !message.isEmpty else { return }
        chatManager.send(message: ChatMessage(text: message, isUser: true))
    }
}

private struct ChatMessagesView: View {
    /// Flags to prevent messages from animating in multiple times as dependencies that drive `body` change
    @State private var shouldAnimateMessageIn = [UUID: Bool]()
    let chatMessages: [ChatMessage]

    var body: some View {
        VStack(alignment: .leading) {
            ChatBubble(message: ChatMessage(text: "How can I help you?", isUser: false), animateIn: true)
                .listRowSeparator(.hidden)

            ForEach(chatMessages) { message in
                ChatBubble(message: message, animateIn: shouldAnimateMessageIn[message.id] ?? true)
                    .listRowSeparator(.hidden)
                    .transition(.opacity)
                    .onAppear {
                        shouldAnimateMessageIn[message.id] = false
                    }
            }
        }
    }
}


// Credit: https://saeedrz.medium.com/detect-scroll-position-in-swiftui-3d6e0d81fc6b
private struct ScrollOffsetPreferenceKey: PreferenceKey {
    static var defaultValue: CGPoint = .zero

    static func reduce(value: inout CGPoint, nextValue: () -> CGPoint) {}
}

private struct ScrollHeightPreferenceKey: PreferenceKey {
    static var defaultValue: CGFloat = .zero

    static func reduce(value: inout CGFloat, nextValue: () -> CGFloat) {}
}

private struct AutoScrollButton: View {
    let isVisible: Bool
    let action: () -> Void

    var body: some View {
        Button {
            action()
        } label: {
            Image(systemName: "arrowshape.down.fill")
                .font(.body)
                .foregroundColor(.primary)
                .padding(8)
        }
        .background(
            Circle()
                .fill(Color(.secondarySystemBackground))
                .stroke(.primary.opacity(0.1), lineWidth:1)
        ).shadow(color:.primary.opacity(0.14), radius: 3, x:0, y:2)
        .opacity(isVisible ? 1 : 0)
    }

}

#Preview {
    ChatView(chatManager: ChatManager())
}



================================================
FILE: Demos/Chat/Chat/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Chat/Chat/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Chat/Chat/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "chat.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Chat/Chat/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Classifier/Classifier/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftData
import AIProxy

enum AppConstants {
    
    static let videoSampleQueue = DispatchQueue(label: "com.AIProxyBootstrap.videoSampleQueue")
    
    #error(
        """
        Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
        Please see https://www.aiproxy.pro/docs/integration-guide.html")
        """
    )
    
    /* Uncomment for BYOK use cases */
    static let openAIService = AIProxy.openAIDirectService(
        unprotectedAPIKey: "your-openai-key"
    )

    /* Uncomment for all other production use cases */
//    static let openAIService = AIProxy.openAIService(
//        partialKey: "partial-key-from-your-developer-dashboard",
//        serviceURL: "service-url-from-your-developer-dashboard"
//    )
}



================================================
FILE: Demos/Classifier/Classifier/AppLogger.swift
================================================
//
//  AppLogger.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import OSLog

/// Log levels available:
///
///     AppLogger.debug
///     AppLogger.info
///     AppLogger.warning
///     AppLogger.error
///     AppLogger.critical
///
/// Flip on metadata logging in Xcode's console to show which source line the log occurred from.
///
/// See my reddit post for a video instructions:
/// https://www.reddit.com/r/SwiftUI/comments/15lsdtk/how_to_use_the_oslog_logger/
let AppLogger = Logger(subsystem: Bundle.main.bundleIdentifier ?? "UnknownApp",
                       category: "AIProxyBootstrapClassifier")



================================================
FILE: Demos/Classifier/Classifier/CameraControlsView.swift
================================================
//
//  CameraControlsView.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI

struct CameraControlsView: View {

    let shutterButtonAction: () -> Void

    var body: some View {

        Button(action: shutterButtonAction) {
            ZStack{
                Circle()
                    .fill(.clear)
                    .stroke(.mint, lineWidth: 4)
                    .frame(width:72, height: 72)
                Circle()
                    .fill(.mint.gradient)
                    .frame(width:60, height: 60)
                Image(systemName: "camera")
                    .font(.title2)
                    .fontWeight(.semibold)
                    .foregroundColor(.black.opacity(0.4))
            }
        }
        .buttonStyle(.plain)
    }
}

#Preview {
    CameraControlsView(shutterButtonAction: {})
}



================================================
FILE: Demos/Classifier/Classifier/CameraDataLoader.swift
================================================
//
//  CameraFrameHandler.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import AVFoundation
import CoreImage

/// Vends camera frames from the built-in back camera.
final actor CameraDataLoader {
    private let sampleBufferDelegate = CameraFrameSampleBufferDelegate()
    private let captureSession = AVCaptureSession()

    /// Streams images of the camera frame.
    /// Use the returned stream in a `for await` loop.
    func imageStream() -> AsyncStream<CGImage> {
        self.setupCaptureSession()
        self.captureSession.startRunning()
        return AsyncStream { [weak self] continuation in
            self?.sampleBufferDelegate.didReceiveImage = { image in
                continuation.yield(image)
            }
        }
    }

    private func setupCaptureSession() {
        let videoOutput = AVCaptureVideoDataOutput()
        guard let videoDevice = AVCaptureDevice.default(.builtInDualWideCamera,for: .video, position: .back) else { return }
        guard let videoDeviceInput = try? AVCaptureDeviceInput(device: videoDevice) else { return }
        guard captureSession.canAddInput(videoDeviceInput) else { return }
        captureSession.addInput(videoDeviceInput)

        videoOutput.setSampleBufferDelegate(
            sampleBufferDelegate,
            queue: AppConstants.videoSampleQueue
        )
        captureSession.addOutput(videoOutput)

        videoOutput.connection(with: .video)?.videoRotationAngle = 90
    }
}


private final class CameraFrameSampleBufferDelegate: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    private let coreImageContext = CIContext()
    var didReceiveImage: ((CGImage) -> Void)?

    /// Delegate implementation for AVCaptureVideoDataOutputSampleBufferDelegate conformance
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        dispatchPrecondition(condition: .onQueue(AppConstants.videoSampleQueue))
        guard let cgImage = self.imageFromSampleBuffer(sampleBuffer: sampleBuffer) else {
            AppLogger.info("Could not convert a sample buffer from the camera into a CGImage")
            return
        }

        self.didReceiveImage?(cgImage)
    }

    private func imageFromSampleBuffer(sampleBuffer: CMSampleBuffer) -> CGImage? {
        guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            AppLogger.info("Could not get an image buffer from CMSampleBuffer")
            return nil
        }

        let ciImage = CIImage(cvPixelBuffer: imageBuffer)
        guard let cgImage = self.coreImageContext.createCGImage(ciImage, from: ciImage.extent) else {
            AppLogger.info("Could not create a CGImage using a core image context")
            return nil
        }

        return cgImage
    }
}



================================================
FILE: Demos/Classifier/Classifier/CameraFrameManager.swift
================================================
//
//  CameraFrameManager.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import AVFoundation
import Foundation
import SwiftUI

@MainActor
@Observable
final class CameraFrameManager {

    /// The most recent camera frame of the back-facing built-in camera
    private(set) var cameraFrameImage: CGImage?
    private let cameraDataLoader = CameraDataLoader()

    init() {
        self.checkPermission() { [weak self] granted in
            if granted {
                self?.startCapturingCameraFrames()
            }
        }
    }

    private func startCapturingCameraFrames() {
        Task {
            let stream = await self.cameraDataLoader.imageStream()
            for await image in stream {
                self.cameraFrameImage = image
            }
        }
    }

    private func checkPermission(checkComplete: @escaping (Bool) -> Void) {
        switch AVCaptureDevice.authorizationStatus(for: .video) {
        case .authorized:
            checkComplete(true)
        case .notDetermined:
            AVCaptureDevice.requestAccess(for: .video) { granted in
                checkComplete(granted)
            }
        default:
            checkComplete(false)
        }
    }
}



================================================
FILE: Demos/Classifier/Classifier/CameraView.swift
================================================
//
//  CameraView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct CameraView: View {

    /// The camera frame image to display
    var image: CGImage?

    private let label = Text("frame")
    
    var body: some View {
        GeometryReader { geo in
           VStack {
               if let image = image {
                   Image(image, scale: 0.5, orientation: .up, label: label)
                       .resizable()
                       .scaledToFill()
                       .frame(maxWidth:geo.size.width, maxHeight: geo.size.width)
                       .clipShape(RoundedRectangle(cornerRadius: 14))
                       .padding()

               } else {
                   Color.black
                       .frame(maxWidth:geo.size.width, maxHeight: geo.size.width)
                       .clipShape(RoundedRectangle(cornerRadius: 14))
                       .padding()
               }
           }
        }
    }
}

#Preview {
    CameraView()
}



================================================
FILE: Demos/Classifier/Classifier/ClassifierApp.swift
================================================
//
//  ClassifierApp.swift
//  Classifier
//
//  Created by Lou Zell
//

import SwiftUI

@main
@MainActor
struct ClassifierApp: App {

    @State private var cameraFrameManager = CameraFrameManager()
    @State private var classifierManager = ClassifierManager()

    var body: some Scene {
        WindowGroup {
            ClassifierView(cameraFrameManager: cameraFrameManager,
                           classifierManager: classifierManager)
        }
    }
}



================================================
FILE: Demos/Classifier/Classifier/ClassifierDataLoader.swift
================================================
//
//  ClassifierDataLoader.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import UIKit
import RegexBuilder
import AIProxy

enum ClassifierDataLoaderError: Error {
    case couldNotCreateImageURL
    case couldNotIdentifyPlant
}

/// Sends requests to OpenAI to classify plant images, and returns the result asynchronously
final actor ClassifierDataLoader {

    /// Uses OpenAI to fetch a description of the image passed as argument
    /// - Parameter image: The image to describe
    /// - Returns: An OpenAI description of the image
    func identify(fromImage image: CGImage) async throws -> (String, URL?) {

        guard let localURL = image.openAILocalURLEncoding() else {
            throw ClassifierDataLoaderError.couldNotCreateImageURL
        }
  
        let prompt = "What kind of plant is this and provide the wikipedia link for it, not in markdown"
        let response = try await AppConstants.openAIService.chatCompletionRequest(body: .init(
            model: "gpt-4o",
            messages: [
                .user(
                    content: .parts(
                        [
                            .text(prompt),
                            .imageURL(localURL, detail: .auto)
                        ]
                    )
                )
            ]
        ))
        let choices = response.choices
        guard let text = choices.first?.message.content else {
            throw ClassifierDataLoaderError.couldNotIdentifyPlant
        }

        return extractDescriptionAndWikipediaURL(text)
    }
}


// Assumes that the wikipedia link as at the end of input `text`
private func extractDescriptionAndWikipediaURL(_ text: String) -> (String, URL?) {
    var mutableText = text
    let re = Regex {
        TryCapture {
           /https?:\/\/[^.]*\.wikipedia\.org[^\b]+$/
        } transform: {
            URL(string: String($0))
        }
    }

    var matchingURL: URL? = nil
    mutableText.replace(re, maxReplacements: 1) { matchingURL = $0.1; return "" }
    return (mutableText, matchingURL)
}

private extension CGImage {
    func openAILocalURLEncoding() -> URL? {
        if let data = UIImage(cgImage: self).jpegData(compressionQuality: 0.4) {
            let base64String = data.base64EncodedString()
            if let url = URL(string: "data:image/jpeg;base64,\(base64String)") {
                return url
            }
        }
        return nil
    }
}



================================================
FILE: Demos/Classifier/Classifier/ClassifierManager.swift
================================================
//
//  ClassifierManager.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import SwiftUI

@MainActor
@Observable
final class ClassifierManager {
    /// The description of a plant. Descriptions are generated by OpenAI
    private(set) var plantDescription: String?

    /// The image of a plant. This camera image is supplied by the user
    private(set) var image: CGImage?

    /// A wikipedia URL for the user to learn more about the identified plant. This URL is generated by OpenAI
    private(set) var wikipediaURL: URL?

    /// Loads data from OpenAI
    private let classifierDataLoader = ClassifierDataLoader()

    /// Identify a plant based on a passed in image
    /// - Parameter image: a camera frame that the user took of a plant in their surroundings
    func identify(_ image: CGImage) {
        self.image = image
        Task {
            let (description, wikipediaURL) = try await classifierDataLoader.identify(fromImage: image)
            self.plantDescription = description
            self.wikipediaURL = wikipediaURL
        }
    }
    
    /// Reset all previously classified state
    func reset() {
        self.plantDescription = nil
        self.image = nil
        self.wikipediaURL = nil
    }
}



================================================
FILE: Demos/Classifier/Classifier/ClassifierView.swift
================================================
//
//  ClassifierView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

@MainActor
struct ClassifierView: View {

    let cameraFrameManager: CameraFrameManager
    let classifierManager: ClassifierManager
    @Environment(\.dismiss) private var dismiss
    @State private var showingSheet = false

    var body: some View {
        ZStack {
            Rectangle()
                .fill(Color(.secondarySystemBackground))
                .ignoresSafeArea()

            VStack {
                CameraView(image: cameraFrameManager.cameraFrameImage)

                Text("Take a photo to identify a plant")
                    .fontWeight(.semibold)
                    .font(.subheadline)
                    .padding(16)
                    .background(Color(.tertiarySystemBackground))
                    .cornerRadius(8)

                CameraControlsView(shutterButtonAction: {
                    if let image = cameraFrameManager.cameraFrameImage {
                        classifierManager.identify(image)
                        showingSheet = true
                    }
                })
                .padding()
                .sheet(isPresented: $showingSheet){
                    SheetView(classifierManager: classifierManager)
                        .presentationDetents([.medium, .large])
                        .presentationBackground(Color(.systemBackground))
                }
            }
        }
    }
}


/// A sheet that slides up over the camera controls to display the results of plant classification
private struct SheetView: View {

    let classifierManager: ClassifierManager
    @Environment(\.dismiss) var dismiss

    var body: some View {
        
        if let plantDescription = classifierManager.plantDescription,
           let plantImage = classifierManager.image {
            ZStack(alignment:.topTrailing){
                Button(){
                    classifierManager.reset()
                    dismiss()
                }label:{
                    Image(systemName: "xmark.circle.fill")
                        .font(.system(size: 32))
                        .foregroundColor(.secondary)
                }
                VStack(spacing:16){
                    Image(uiImage: UIImage(cgImage: plantImage))
                        .resizable()
                        .scaledToFill()
                        .frame(width:80, height:80)
                        .cornerRadius(8)
                        .clipShape(Circle())
                        .overlay(
                            Circle()
                                .fill(.clear)
                                .stroke(.mint, lineWidth:2)
                        )
                        .shadow(radius: 8, x: 0, y: 4)
                    
                    VStack(spacing:8){
                        Text("Description")
                            .font(.title)
                        Text(plantDescription)
                            .font(.subheadline)
                            .foregroundColor(.secondary)
                    }
                    if let wikipediaURL = classifierManager.wikipediaURL {
                        Button(){
                            UIApplication.shared.open(wikipediaURL)
                        } label:{
                            Text("View on Wikipedia")
                                .frame(maxWidth:.infinity)
                                .fontWeight(.semibold)
                                .foregroundColor(.white)
                        }
                        .buttonStyle(.borderedProminent)
                        .controlSize(.large)
                    }
                }
                .padding(.top, 16)
            }
            .padding(.horizontal, 16)
            .padding(.vertical, 24)
            .frame(maxWidth: .infinity, maxHeight: .infinity, alignment:.topLeading)
            .transition(.opacity)
        } else {
            ProgressView()
        }
    }
}


#Preview {
    ClassifierView(cameraFrameManager: CameraFrameManager(),
                   classifierManager: ClassifierManager())
}



================================================
FILE: Demos/Classifier/Classifier/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Classifier/Classifier/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Classifier/Classifier/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "classify.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Classifier/Classifier/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/EmojiPuzzleMaker/EmojiPuzzleMaker/ContentView.swift
================================================
//
//  ContentView.swift
//  EmojiPuzzleMaker
//
//  Created by Todd Hamilton on 8/1/24.
//

import AIProxy
import SwiftUI
import WebKit
import Foundation

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let anthropicService = AIProxy.anthropicDirectService(
    unprotectedAPIKey: "your-anthropic-key"
)

/* Uncomment for all other production use cases */
//let anthropicService = AIProxy.anthropicService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)

struct ContentView: View {

    @State var emojis = "🏔️💦"
    @State var hint = "A fizzy green beverage"
    @State var prompt = ""
    @State var color = "blue"
    @State var json = ""
    let instructions = """
    You are an AI tasked with generating a fun and interesting emoji puzzle based on a given word or phrase. Your goal is to create a puzzle using 2-3 emojis that represent the word or phrase, along with a hint to help solve the puzzle. Follow these guidelines:
    
    1. Analyze the given word or phrase and think of relevant emojis that can represent it.
    2. Choose 2-3 emojis that, when combined, cleverly represent the word or phrase.
    3. Ensure the emojis are not too obvious, but also not impossibly difficult to decipher.
    4. Create a short, helpful hint that gives a clue about the word or phrase without directly stating it.
    5. Choose a SwiftUI color to match the theme of the puzzle.
    6. Your returned message content should only be valid JSON. Do not introduce the JSON with "Here is my emoji puzzle response" or any other introduction.
    7. In the JSON for `emojis` field, always put a space between unicode characters
    
    Generate a hint text that provides a subtle clue about the word or phrase without giving it away completely. The hint should be concise and engaging, encouraging the puzzle solver to think creatively.
    
    Your response should be in valid JSON format with the following structure:
    {
      "emojis": "unicode_emoji1 unicode_emoji2 ...",
      "hint": "Your hint text here",
      "color": "The SwiftUI color here",
      "solution": "The original word or phrase"
    }
    
    Example output for the prompt 'Starbucks':
    {
      "emojis": "⭐️ 💵",
      "hint": "Your daily fix",
      "color": "brown",
      "solution": "Starbucks"
    }
    
    Remember to use unicode representations for the emojis and ensure your output is in valid JSON format.
    
    Here is the word or phrase to base your emoji puzzle on:
    """

    var body: some View {

        VStack{
            Spacer()
            Text("Make an emoji puzzle!")
                .font(.title)
                .fontWeight(.bold)
                .fontDesign(.rounded)
            
            if emojis == "" {
                ProgressView()
                    .controlSize(.large)
                    .frame(maxHeight: 280)
            }else {
                VStack(spacing:12){
                    Text(emojis)
                        .font(.system(size: 72, weight: .bold, design: .rounded))
                        .minimumScaleFactor(0.1)
                        .frame(maxWidth:.infinity)
                        .padding(.vertical, 72)
                    Text("Hint: \(hint)")
                        .font(.subheadline)
                        .fontWeight(.medium)
                        .fontDesign(.rounded)
                        .fixedSize(horizontal: false, vertical: true)
                        .multilineTextAlignment(.center)
                        .padding(16)
                        .foregroundStyle(.white)
                        .background(.black.opacity(0.75))
                        .cornerRadius(30)
                }
                .frame(maxWidth:.infinity, maxHeight: 280)
                .transition(.scale)
            }
            

            Spacer()

            VStack(spacing:8){
                TextField("ex. Mountain Dew", text:$prompt)
                    .keyboardType(.alphabet)
                    .disableAutocorrection(true)
                    .padding()
                    .overlay(
                        RoundedRectangle(cornerRadius: 10)
                            .stroke(.black.opacity(0.24), lineWidth: 2)
                    )
                    .background(Color(.white))
                    .cornerRadius(10)

                Button{

                    emojis = ""
                    hint = ""

                    Task{
                        do{
                            let prompt = "\n\nHuman: \(instructions + prompt)\n\nAssistant:"
                            let requestBody = AnthropicMessageRequestBody(
                                maxTokens: 1024,
                                messages: [
                                    .init(content: [.text(prompt)], role: .user)
                                ],
                                model: "claude-3-5-sonnet-20240620"
                            )
                            let response = try await anthropicService.messageRequest(body: requestBody)
                            if let puzzle = getPuzzle(fromClaudeResponse: response) {
                                updateUI(withPuzzle: puzzle)
                            }
                        } catch {
                            print(error.localizedDescription)
                        }
                    }
                }label:{
                    Text("Generate Puzzle")
                        .frame(maxWidth: .infinity)
                }
                .buttonStyle(.borderedProminent)
                .controlSize(.large)
                .tint(Color(string: color))
                .fontWeight(.bold)

            }
            .padding(.horizontal,24)
            Spacer()
        }
    }

    func getPuzzle(fromClaudeResponse response: AnthropicMessageResponseBody) -> Puzzle? {
        // We expected Claude to return a single text content in the response:
        guard case .text(let messageContent) = response.content.first else {
            print("Claude didn't return a text response")
            return nil
        }

        print("Puzzle content:\n\n\(messageContent)")
        guard let jsonData = messageContent.data(using: .utf8) else {
            print("Could not convert Claude's message to jsonData")
            return nil
        }

        do {
            let decoder = JSONDecoder()
            return try decoder.decode(Puzzle.self, from: jsonData)
        } catch {
            print("Error decoding puzzle JSON: \(error)")
            return nil
        }
    }

    func updateUI(withPuzzle puzzle: Puzzle) {
        withAnimation(.snappy){
            emojis = puzzle.emojis
            hint = puzzle.hint
            color = puzzle.color
        }
    }
}

// Define the struct that matches the JSON structure
struct Puzzle: Codable {
    let emojis: String
    let hint: String
    let color: String
    let solution: String
}

extension Color {
    init?(string: String) {
        switch string.lowercased() {
        case "red":
            self = .red
        case "orange":
            self = .orange
        case "yellow":
            self = .yellow
        case "green":
            self = .green
        case "blue":
            self = .blue
        case "purple":
            self = .purple
        case "pink":
            self = .pink
        case "black":
            self = .black
        case "white":
            self = .white
        case "gray":
            self = .gray
        case "teal":
            self = .teal
        default:
            return nil
        }
    }
}

#Preview {
    ContentView()
}



================================================
FILE: Demos/EmojiPuzzleMaker/EmojiPuzzleMaker/EmojiPuzzleMakerApp.swift
================================================
//
//  EmojiPuzzleMakerApp.swift
//  EmojiPuzzleMaker
//
//  Created by Todd Hamilton on 8/1/24.
//

import SwiftUI

@main
struct EmojiPuzzleMakerApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: Demos/EmojiPuzzleMaker/EmojiPuzzleMaker/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/EmojiPuzzleMaker/EmojiPuzzleMaker/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/EmojiPuzzleMaker/EmojiPuzzleMaker/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/EmojiPuzzleMaker/EmojiPuzzleMaker/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/FilmFinder/FilmFinder/AppConstants.swift
================================================
//
//  AppConstants.swift
//  FilmFinder
//
//  Created by Todd Hamilton on 11/4/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    
    You will also need a read access token from TMDB:
    https://developer.themoviedb.org/docs/getting-started
    """
)

/* Uncomment for BYOK use cases */
let groqService = AIProxy.groqDirectService(
    unprotectedAPIKey: "your-groq-key"
)

/* Uncomment for all other production use cases */
//static let groqService = AIProxy.groqService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)

let tmdb = "api-read-access-token-from-tmdb"



================================================
FILE: Demos/FilmFinder/FilmFinder/ContentView.swift
================================================
//
//  ContentView.swift
//  FilmFinder
//
//  Created by Todd Hamilton on 10/28/24.
//

import Foundation
import SwiftUI
import AIProxy
import UIKit


struct ContentView: View {
    
    @State private var isLoading = false
    @State private var showingDetails = false
    
    @State var counter: Int = 0
    @State var origin: CGPoint = CGPoint(x: 0.5, y: 0.5)

    @State var movieID: String = "335984"
    @State var movieTitle: String = "Inside Out 2"
    @State var movieReleaseDate: String = "10/06/2017 (US)"
    @State var moviePoster: String = "vpnVM9B6NMmQpWeZvzLvDESb2QY.jpg"
    @State var movieBackdrop: String = "p5ozvmdgsmbWe0H8Xk7Rc8SCwAB.jpg"
    @State var movieOverview: String = "Teenager Riley's mind headquarters is undergoing a sudden demolition to make room for something entirely unexpected: new Emotions! Joy, Sadness, Anger, Fear and Disgust, who’ve long been running a successful operation by all accounts, aren’t sure how to feel when Anxiety shows up. And it looks like she’s not alone."
    
    @State var previousSuggestions:[String] = []
    
    @State var genreLabel: String = ""
    let jsonExample:String = """
{
  "title": "the title of the movie"
}
"""

    var body: some View {
        ZStack {
            
            // Background
            Color.clear
                .edgesIgnoringSafeArea(.all)
                .background(
                    AsyncImage(
                        url: URL(string: "https://image.tmdb.org/t/p/w500/\(moviePoster)"),
                        content: { image in
                            image.resizable()
                                .ignoresSafeArea()
                                .blur(radius: 50)
                                .modifier(RippleEffect(at: origin, trigger: counter))
                                .overlay(Color.black.opacity(0.5))
                        },
                        placeholder: {
                            Color.clear
                        }
                    )
                )
            
            VStack(spacing:0){
                AsyncImage(
                    url: URL(string: "https://image.tmdb.org/t/p/w500/\(moviePoster)"),
                    content: { image in
                        image.resizable()
                             .aspectRatio(contentMode: .fill)
                             .frame(width: 192, height: 296)
                             .cornerRadius(8)
                             .shadow(radius: 8, x: 0, y: 4)
                             .modifier(RippleEffect(at: origin, trigger: counter))
                             .onTapGesture {
                                 showingDetails.toggle()
                             }
                    },
                    placeholder: {
                        ProgressView()
                            .frame(width: 192, height: 296)
                    }
                )
                
                Button{
                    showingDetails.toggle()
                }label:{
                    if isLoading {
                        ProgressView()
                            .controlSize(.small)
                            .padding(.horizontal, 12)
                    } else {
                        Text("\(movieTitle)")
                            .font(.caption)
                            .fontDesign(.monospaced)
                            .lineLimit(1)
                            .truncationMode(.tail)
                            .padding(.horizontal, 12)
                        
                    }
                }
                .frame(maxHeight:34)
                .background(
                    Capsule()
                        .fill(.thickMaterial)
                        .stroke(.white.opacity(0.14))
                )
                .foregroundStyle(.white)
                .padding()
                .sheet(isPresented: $showingDetails) {
                    MovieDetailsView(
                        movieID: $movieID,
                        movieBackdrop: $movieBackdrop,
                        movieTitle: $movieTitle,
                        movieReleaseDate: $movieReleaseDate,
                        movieOverview: $movieOverview,
                        genreLabel: $genreLabel
                    )
                    .presentationDetents([.large, .large])
                    .presentationDragIndicator(.visible)
                }
                
                GenreSelectorView(getMovieRecFromGroq: {
                    Task{
                        await getMovieRecFromGroq()
                    }
                    
                }, genreLabel: $genreLabel)

            }
            
        }
        .preferredColorScheme(.dark)
    }

    
    // Get the movie recommendation from Groq based on the selected genre
    func getMovieRecFromGroq() async {
    
        isLoading = true

        defer {
            isLoading = false
        }
        do {
            let response = try await groqService.chatCompletionRequest(body: .init(
                messages: [
                    .system(content: "Recommend a \(genreLabel) movie to watch. Respond with a single movie title and a description for why it was chosen only in json. Don't recommend any of the previous movies: \(previousSuggestions). Do not include any other content in the repsonse. Use this example json as a template: \(jsonExample). Make sure to recommend a wide variety of movies.")
                ],
                model: "llama3-8b-8192",
                responseFormat: .jsonObject
                
            ))
            let movie = response.choices.first?.message.content ?? ""
            await parseRecommendationFromGroq(movieData: movie)
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print(error.localizedDescription)
        }
    }
    
    // Parse the response from Groq, set variables, the fetch data from TMDB
    func parseRecommendationFromGroq(movieData: String) async {
        
        let jsonData = movieData.data(using: .utf8)!
        
        // Decode JSON data
        if let movie = try? JSONDecoder().decode(Recommendation.self, from: jsonData) {
            movieTitle = movie.title
            previousSuggestions.append(movieTitle)
            await fetchMovieDataFromTMDB()
        }
    }
    
    // Fetch data from TMDB
    func fetchMovieDataFromTMDB() async {
        let url = URL(string: "https://api.themoviedb.org/3/search/movie")!
        var components = URLComponents(url: url, resolvingAgainstBaseURL: true)!
        let queryItems: [URLQueryItem] = [
          URLQueryItem(name: "query", value: movieTitle),
          URLQueryItem(name: "include_adult", value: "false"),
          URLQueryItem(name: "language", value: "en-US"),
          URLQueryItem(name: "page", value: "1"),
        ]
        components.queryItems = components.queryItems.map { $0 + queryItems } ?? queryItems

        var request = URLRequest(url: components.url!)
        request.httpMethod = "GET"
        request.timeoutInterval = 10
        request.allHTTPHeaderFields = [
          "accept": "application/json",
          "Authorization": "Bearer \(tmdb)"
        ]

        do {
            let (data, _) = try await URLSession.shared.data(for: request)
            extractMovieDataFromTMDBResponse(tmdbResponse: String(decoding: data, as: UTF8.self))
        } catch {
            print(error.localizedDescription)
        }
    }
    
    // Parse the response from TMDB and store in state variables
    func extractMovieDataFromTMDBResponse(tmdbResponse: String) {
        // Convert JSON string to Data
        if let jsonData = tmdbResponse.data(using: .utf8) {
            do {
                // Decode JSON to MovieResponse object
                let movieResponse = try JSONDecoder().decode(MovieResponse.self, from: jsonData)
                
                // Access parsed data
                if let firstMovie = movieResponse.results.first {
                    movieID = String(firstMovie.id)
                    movieReleaseDate = firstMovie.releaseDate
                    movieOverview = firstMovie.overview
                    moviePoster = firstMovie.posterPath ?? ""
                    movieBackdrop = firstMovie.backdropPath ?? ""
                    
                    counter += 1
                }
            } catch {
                print("Failed to decode JSON:", error)
            }
        }
    }
    
}

#Preview {
    ContentView()
}



================================================
FILE: Demos/FilmFinder/FilmFinder/FilmFinderApp.swift
================================================
//
//  FilmFinderApp.swift
//  FilmFinder
//
//  Created by Todd Hamilton on 10/30/24.
//

import SwiftUI
import TipKit

@main
struct FilmFinderApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
    
    init() {
        /// Load and configure the state of all the tips of the app
        try? Tips.configure()
    }
}



================================================
FILE: Demos/FilmFinder/FilmFinder/GenreSelectorView.swift
================================================
//
//  GenreSelectorView.swift
//  FilmFinder
//
//  Created by Todd Hamilton on 11/3/24.
//

import SwiftUI
import TipKit

struct GenreSelectorView: View {
    
    private let getStartedTip = GetStartedTip()
    
    private let modelSize: CGFloat = UIScreen.main.bounds.width - 40
    private let gridSize = 40  // Adjust for more dots if desired
    private let dotSize: CGFloat = 2
    private let spacing: CGFloat = 20  // Increase spacing between dots
    private let thresholdDistance: CGFloat = 75  // Max distance for attraction
    
    let getMovieRecFromGroq: () -> Void
    
    @State private var isDragging = false
    @Binding var genreLabel: String
    @State var genreScores: [String: Double] = [
        "Action": 0.0,
        "Comedy": 0.0,
        "Drama": 0.0,
        "Horror": 0.0,
        "Sci-Fi": 0.0,
        "Fantasy": 0.0,
        "Romance": 0.0,
        "Documentary": 0.0
    ]
    // Define genre centers (normalized)
    @State var genreCenters: [String: CGPoint] = [
        "Drama": CGPoint(x: 1.000, y: 0.500),
        "Horror": CGPoint(x: 0.853, y: 0.853),
        "Sci-Fi": CGPoint(x: 0.500, y: 1.000),
        "Fantasy": CGPoint(x: 0.147, y: 0.853),
        "Romance": CGPoint(x: 0.000, y: 0.500),
        "Documentary": CGPoint(x: 0.147, y: 0.147),
        "Action": CGPoint(x: 0.500, y: 0.000),
        "Comedy": CGPoint(x: 0.853, y: 0.147)
    ]
    
    @State var knobPosition: CGPoint = CGPoint(x: (UIScreen.main.bounds.width - 40) / 2, y: (UIScreen.main.bounds.width - 40) / 2)
    
    var body: some View {
        ZStack {
            
            // Drag area background
            RoundedRectangle(cornerRadius: 25)
                .fill(.thinMaterial)
                .frame(width: modelSize, height: modelSize)
                .overlay(
                    ZStack{
                        RoundedRectangle(cornerRadius: 25)
                            .stroke(.white.opacity(0.24))
                    }
                )
                .shadow(radius: 8, x: 0, y: 4)
            
            // Dots
            GeometryReader { geo in
                let rows = Int(modelSize / spacing)
                let cols = Int(modelSize / spacing)
                
                ZStack {
                    ForEach(0..<rows, id: \.self) { row in
                        ForEach(0..<cols, id: \.self) { col in
                            Circle()
                                .fill(Color.white.opacity(dotOpacity(row: row, col: col)))
                                .frame(width: dotSize, height: dotSize)
                                .position(self.dotPosition(row: row, col: col))
                        }
                    }
                }
                .background(Color.clear)
                .padding(11)
            }
            
            // Crosshare
            Rectangle()
                .fill(LinearGradient(colors: [.clear,.clear, .white.opacity(0.24),.clear, .clear], startPoint: .leading, endPoint: .trailing))
                .frame(width: modelSize, height: 1)
                
            Rectangle()
                .fill(LinearGradient(colors: [.clear,.clear, .white.opacity(0.24), .clear, .clear], startPoint: .top, endPoint: .bottom))
                .frame(width: 1, height: modelSize)

            // Y axis labels
            VStack{
                Text("Action")
                    .foregroundStyle(genreLabel == "Action" ? .primary : .secondary)
                    .fontWeight(genreLabel == "Action" ? .bold : .regular)
                    .scaleEffect(genreLabel == "Action" ? 1.1 : 1.0)
                Spacer()
                Text("Sci-Fi")
                    .foregroundStyle(genreLabel == "Sci-Fi" ? .primary : .secondary)
                    .fontWeight(genreLabel == "Sci-Fi" ? .bold : .regular)
                    .scaleEffect(genreLabel == "Sci-Fi" ? 1.1 : 1.0)
            }
            .padding()
            .frame(width: modelSize, height: modelSize)
            
            // X axis labels
            HStack{
                Text("Romance")
                    .foregroundStyle(genreLabel == "Romance" ? .primary : .secondary)
                    .fontWeight(genreLabel == "Romance" ? .bold : .regular)
                    .scaleEffect(genreLabel == "Romance" ? 1.1 : 1.0)
                Spacer()
                Text("Drama")
                    .foregroundStyle(genreLabel == "Drama" ? .primary : .secondary)
                    .fontWeight(genreLabel == "Drama" ? .bold : .regular)
                    .scaleEffect(genreLabel == "Drama" ? 1.1 : 1.0)
            }
            .padding()
            .frame(width: modelSize, height: modelSize)
            
            // Quadrant labels
            Group {
                Text("Documentary").position(x: modelSize * 0.25, y: modelSize * 0.25)
                    .foregroundStyle(genreLabel == "Documentary" ? .primary : .secondary)
                    .fontWeight(genreLabel == "Documentary" ? .bold : .regular)
                    .scaleEffect(genreLabel == "Documentary" ? 1.1 : 1.0)
                Text("Comedy").position(x: modelSize * 0.75, y: modelSize * 0.25)
                    .foregroundStyle(genreLabel == "Comedy" ? .primary : .secondary)
                    .fontWeight(genreLabel == "Comedy" ? .bold : .regular)
                    .scaleEffect(genreLabel == "Comedy" ? 1.1 : 1.0)
                Text("Fantasy").position(x: modelSize * 0.25, y: modelSize * 0.75)
                    .foregroundStyle(genreLabel == "Fantasy" ? .primary : .secondary)
                    .fontWeight(genreLabel == "Fantasy" ? .bold : .regular)
                    .scaleEffect(genreLabel == "Fantasy" ? 1.1 : 1.0)
                Text("Horror").position(x: modelSize * 0.75, y: modelSize * 0.75)
                    .foregroundStyle(genreLabel == "Horror" ? .primary : .secondary)
                    .fontWeight(genreLabel == "Horror" ? .bold : .regular)
                    .scaleEffect(genreLabel == "Horror" ? 1.1 : 1.0)
            }
            
            // Knob to drag around
            Circle()
                .fill(Color.white.gradient)
                .stroke(.white.opacity(0.48), lineWidth:1)
                .frame(width: 40, height: 40)
                .popoverTip(getStartedTip)
                .shadow(radius: 15)
                .position(knobPosition)
                .gesture(
                    DragGesture()
                        .onChanged { value in
                            isDragging = true // Set isDragging to true when dragging starts
                            getStartedTip.invalidate(reason: .actionPerformed)
                            withAnimation(.easeOut(duration: 0.1)) {
                                // Limit position within bounds
                                knobPosition = CGPoint(
                                    x: max(0, min(modelSize, value.location.x)),
                                    y: max(0, min(modelSize, value.location.y))
                                )
                                calculateGenre()
                                calculateGenreProximityScores()
                            }
                            
                        }
                        .onEnded { _ in
                            withAnimation(.bouncy) {
                                knobPosition = CGPoint(
                                    x: modelSize / 2,
                                    y: modelSize / 2
                                )
                            }
                            // Delay setting isDragging to false until animation completes
                            DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
                                withAnimation(.bouncy){
                                    isDragging = false
                                }
                                
                            }
                            getMovieRecFromGroq()
                        }
                )
            
        }
        .frame(width: modelSize, height: modelSize)
        .font(.caption2)
        .fontDesign(.monospaced)
        .preferredColorScheme(.dark)
    }
    
    // Initial grid positions with increased spacing
    private func initialDotPosition(row: Int, col: Int) -> CGPoint {
        CGPoint(x: CGFloat(col) * spacing, y: CGFloat(row) * spacing)
    }
    
    private func dotPosition(row: Int, col: Int) -> CGPoint {
        
        guard isDragging else { // Check if dragging is active
            return initialDotPosition(row: row, col: col)
        }
        
        let fingerPosition = knobPosition
        let dotPos = initialDotPosition(row: row, col: col)
        let distance = hypot(dotPos.x  - fingerPosition.x + 10, dotPos.y - fingerPosition.y + 10)
        
        if distance < thresholdDistance {
            // Calculate offset based on distance to create attraction
            let offsetFactor = (thresholdDistance - distance) / thresholdDistance
            let direction = CGPoint(x: (fingerPosition.x - dotPos.x) * offsetFactor,
                                    y: (fingerPosition.y - dotPos.y) * offsetFactor)
            return CGPoint(x: dotPos.x + direction.x, y: dotPos.y + direction.y)
        } else {
            // Outside of threshold, snap back to original position
            return dotPos
        }
    }
    
    private func dotOpacity(row: Int, col: Int) -> Double {
        
        guard isDragging else { // Only change opacity if dragging
            return 0.1
        }
        
        let fingerPosition = knobPosition
        let dotPos = initialDotPosition(row: row, col: col)
        let distance = hypot(dotPos.x - fingerPosition.x + 10, dotPos.y - fingerPosition.y + 10)

        // Calculate opacity based on proximity; closer dots are more opaque
        let opacityFactor = max(0.1, min(1.0, (thresholdDistance - distance) / thresholdDistance))
        return opacityFactor
    }
    
    // Determine the closest genre based on knob position
    private func calculateGenre() {

        // Normalize knob position
        let normalizedKnobPosition = CGPoint(x: knobPosition.x / modelSize, y: knobPosition.y / modelSize)
        
        // Find the genre with the minimum distance to the knob position
        var closestGenre: String = "Neutral"
        var minDistance: CGFloat = .greatestFiniteMagnitude
        
        for (genre, center) in genreCenters {
            let distance = hypot(normalizedKnobPosition.x - center.x, normalizedKnobPosition.y - center.y)
            if distance < minDistance {
                minDistance = distance
                closestGenre = genre
            }
        }
        
        // Update the genre label with the closest genre
        withAnimation {
            genreLabel = closestGenre
        }
    }
    

    // Calculate proximity scores for each emotion based on knob position
    private func calculateGenreProximityScores() {
        
        // Calculate proximity scores
        let normalizedKnobPosition = CGPoint(x: knobPosition.x / modelSize, y: knobPosition.y / modelSize)
        var scores: [String: Double] = [:]
        
        for (genre, center) in genreCenters {
            // Calculate inverse distance as proximity score (higher = closer)
            let distance = hypot(normalizedKnobPosition.x - center.x, normalizedKnobPosition.y - center.y)
            scores[genre] = max(0, 1 - distance) // Scale to 0-1 range
        }
    
        withAnimation(.bouncy){
            genreScores = scores
        }

    }
    
}

#Preview {
    GenreSelectorView(getMovieRecFromGroq: {}, genreLabel: .constant(""))
}



================================================
FILE: Demos/FilmFinder/FilmFinder/GetStartedTip.swift
================================================
//
//  GetStartedTip.swift
//  FilmFinder
//
//  Created by Todd Hamilton on 10/31/24.
//

import SwiftUI
import TipKit

// Tooltip for first time users
struct GetStartedTip: Tip {
    var title: Text {
        Text("Get movie recommendations")
    }
    var message: Text? {
        Text("Drag the circle to choose a genre.")
    }
}



================================================
FILE: Demos/FilmFinder/FilmFinder/Info.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict/>
</plist>



================================================
FILE: Demos/FilmFinder/FilmFinder/Movie.swift
================================================
//
//  Movie.swift
//  FilmFinder
//
//  Created by Todd Hamilton on 10/29/24.
//

import Foundation

struct Recommendation: Codable {
    let title: String
}

// Define the structs to match the JSON structure
struct MovieResponse: Codable {
    let page: Int
    let results: [Movie]
    let totalPages: Int
    let totalResults: Int
    
    // Map JSON keys to Swift property names if they differ
    enum CodingKeys: String, CodingKey {
        case page, results
        case totalPages = "total_pages"
        case totalResults = "total_results"
    }
}

struct Movie: Codable {
    let adult: Bool
    let backdropPath: String?
    let genreIds: [Int]
    let id: Int
    let originalLanguage: String
    let originalTitle: String
    let overview: String
    let popularity: Double
    let posterPath: String?
    let releaseDate: String
    let title: String
    let video: Bool
    let voteAverage: Double
    let voteCount: Int
    
    enum CodingKeys: String, CodingKey {
        case adult
        case backdropPath = "backdrop_path"
        case genreIds = "genre_ids"
        case id
        case originalLanguage = "original_language"
        case originalTitle = "original_title"
        case overview, popularity
        case posterPath = "poster_path"
        case releaseDate = "release_date"
        case title, video
        case voteAverage = "vote_average"
        case voteCount = "vote_count"
    }
}



================================================
FILE: Demos/FilmFinder/FilmFinder/MovieDetailsView.swift
================================================
//
//  MovieDetailsView.swift
//  MovieMoods
//
//  Created by Todd Hamilton on 10/29/24.
//

import SwiftUI

struct MovieDetailsView: View {
    
    @Binding var movieID: String
    @Binding var movieBackdrop: String
    @Binding var movieTitle: String
    @Binding var movieReleaseDate: String
    @Binding var movieOverview: String
    @Binding var genreLabel: String
    
    @State var showPoster:Bool = false
    @State var showTitle:Bool = false
    @State var showRelease:Bool = false
    @State var showOverview:Bool = false
    @State var showCTA:Bool = false
    
    var body: some View {
        VStack(spacing:24){
            
            
            AsyncImage(
                url: URL(string: "https://image.tmdb.org/t/p/w1920_and_h800_multi_faces/\(movieBackdrop)"),
                content: { image in
                    image
                        .resizable()
                        .scaledToFit()
                        .frame(maxWidth:.infinity)
                        .clipped()
                        .cornerRadius(8)
                        
                },
                placeholder: {
                    Color.clear
                }
            )
            .opacity(showPoster ? 1 : 0)

            VStack(spacing:24){
                
                Text(movieTitle)
                    .font(.title2)
                    .fontWeight(.bold)
                    .frame(maxWidth:.infinity, alignment: .leading)
                    .opacity(showTitle ? 1 : 0)
                
                VStack(spacing:8){
                    Text("Release date")
                        .frame(maxWidth:.infinity, alignment: .leading)
                        .foregroundColor(.secondary)
                        .textCase(.uppercase)
                    Text("\(movieReleaseDate)")
                        .frame(maxWidth:.infinity, alignment: .leading)
                }
                .font(.caption)
                .opacity(showRelease ? 1 : 0)
                
                VStack(spacing:8){
                    Text("Overview")
                        .frame(maxWidth:.infinity, alignment: .leading)
                        .foregroundColor(.secondary)
                        .textCase(.uppercase)
                    Text(movieOverview)
                        .frame(maxWidth:.infinity, alignment: .leading)
                }
                .font(.caption)
                .opacity(showOverview ? 1 : 0)
                
            }
            .fontDesign(.monospaced)
            
            Link(destination: URL(string: "https://www.themoviedb.org/movie/\(movieID)")!){
                Text("View on TMDB")
                    .foregroundColor(.black)
                    .padding(6)
                    .frame(maxWidth:.infinity)
            }
            .buttonStyle(.borderedProminent)
            .tint(.white)
            .fontDesign(.monospaced)
            .fontWeight(.bold)
            .opacity(showCTA ? 1 : 0)
            
            Spacer()
        }
        .padding(16)
        .padding(.top, 12)
        .preferredColorScheme(.dark)
        .onAppear(){
            withAnimation(.easeInOut.delay(0.2)){
                showPoster.toggle()
            }
            withAnimation(.easeInOut.delay(0.25)){
                showTitle.toggle()
            }
            withAnimation(.easeInOut.delay(0.3)){
                showRelease.toggle()
            }
            withAnimation(.easeInOut.delay(0.35)){
                showOverview.toggle()
            }
            withAnimation(.easeInOut.delay(0.4)){
                showCTA.toggle()
            }
        }
    }
}

#Preview {
    MovieDetailsView(
        movieID: .constant("23232"),
        movieBackdrop: .constant("p5ozvmdgsmbWe0H8Xk7Rc8SCwAB.jpg"),
        movieTitle: .constant("Inside Out 2"),
        movieReleaseDate: .constant("06/14/2024 (US)"),
        movieOverview: .constant("Teenager Riley's mind headquarters is undergoing a sudden demolition to make room for something entirely unexpected: new Emotions! Joy, Sadness, Anger, Fear and Disgust, who’ve long been running a successful operation by all accounts, aren’t sure how to feel when Anxiety shows up. And it looks like she’s not alone."),
        genreLabel: .constant("Comedy")
    )
}



================================================
FILE: Demos/FilmFinder/FilmFinder/Ripple.metal
================================================
//
//  Ripple.metal
//  FilmFinder
//
//  Created by Todd Hamilton on 6/21/24.
//

// Insert #include <metal_stdlib>
#include <SwiftUI/SwiftUI.h>
using namespace metal;

[[ stitchable ]]
half4 Ripple(
    float2 position,
    SwiftUI::Layer layer,
    float2 origin,
    float time,
    float amplitude,
    float frequency,
    float decay,
    float speed
) {
    // The distance of the current pixel position from `origin`.
    float distance = length(position - origin);
    // The amount of time it takes for the ripple to arrive at the current pixel position.
    float delay = distance / speed;

    // Adjust for delay, clamp to 0.
    time -= delay;
    time = max(0.0, time);

    // The ripple is a sine wave that Metal scales by an exponential decay
    // function.
    float rippleAmount = amplitude * sin(frequency * time) * exp(-decay * time);

    // A vector of length `amplitude` that points away from position.
    float2 n = normalize(position - origin);

    // Scale `n` by the ripple amount at the current pixel position and add it
    // to the current pixel position.
    //
    // This new position moves toward or away from `origin` based on the
    // sign and magnitude of `rippleAmount`.
    float2 newPosition = position + rippleAmount * n;

    // Sample the layer at the new position.
    half4 color = layer.sample(newPosition);

    // Lighten or darken the color based on the ripple amount and its alpha
    // component.
    color.rgb += 0.3 * (rippleAmount / amplitude) * color.a;

    return color;
}





================================================
FILE: Demos/FilmFinder/FilmFinder/Ripple.swift
================================================
//
//  Ripple.swift
//  FilmFinder
//
//  Created by Todd Hamilton on 6/21/24.
//

import SwiftUI

struct PushEffect<T: Equatable>: ViewModifier {
    var trigger: T

    func body(content: Content) -> some View {
        content.keyframeAnimator(
            initialValue: 1.0,
            trigger: trigger
        ) { view, value in
            view.visualEffect { view, _ in
                view.scaleEffect(value)
            }
        } keyframes: { _ in
            SpringKeyframe(0.95, duration: 0.2, spring: .snappy)
            SpringKeyframe(1.0, duration: 0.2, spring: .bouncy)
        }
    }
}

/// A modifer that performs a ripple effect to its content whenever its
/// trigger value changes.
struct RippleEffect<T: Equatable>: ViewModifier {
    var origin: CGPoint

    var trigger: T

    init(at origin: CGPoint, trigger: T) {
        self.origin = origin
        self.trigger = trigger
    }

    func body(content: Content) -> some View {
        let origin = origin
        let duration = duration

        content.keyframeAnimator(
            initialValue: 0,
            trigger: trigger
        ) { view, elapsedTime in
            view.modifier(RippleModifier(
                origin: origin,
                elapsedTime: elapsedTime,
                duration: duration
            ))
        } keyframes: { _ in
            MoveKeyframe(0)
            LinearKeyframe(duration, duration: duration)
        }
    }

    var duration: TimeInterval { 3 }
}

/// A modifier that applies a ripple effect to its content.
struct RippleModifier: ViewModifier {
    var origin: CGPoint

    var elapsedTime: TimeInterval

    var duration: TimeInterval

    var amplitude: Double = 12
    var frequency: Double = 15
    var decay: Double = 8
    var speed: Double = 1200

    func body(content: Content) -> some View {
        let shader = ShaderLibrary.Ripple(
            .float2(origin),
            .float(elapsedTime),

            // Parameters
            .float(amplitude),
            .float(frequency),
            .float(decay),
            .float(speed)
        )

        let maxSampleOffset = maxSampleOffset
        let elapsedTime = elapsedTime
        let duration = duration

        content.visualEffect { view, _ in
            view.layerEffect(
                shader,
                maxSampleOffset: maxSampleOffset,
                isEnabled: 0 < elapsedTime && elapsedTime < duration
            )
        }
    }

    var maxSampleOffset: CGSize {
        CGSize(width: amplitude, height: amplitude)
    }
}

extension View {
    func onPressingChanged(_ action: @escaping (CGPoint?) -> Void) -> some View {
        modifier(SpatialPressingGestureModifier(action: action))
    }
}

struct SpatialPressingGestureModifier: ViewModifier {
    var onPressingChanged: (CGPoint?) -> Void

    @State var currentLocation: CGPoint?

    init(action: @escaping (CGPoint?) -> Void) {
        self.onPressingChanged = action
    }

    func body(content: Content) -> some View {
        let gesture = SpatialPressingGesture(location: $currentLocation)

        content
            .gesture(gesture)
            .onChange(of: currentLocation, initial: false) { _, location in
                onPressingChanged(location)
            }
    }
}

struct SpatialPressingGesture: UIGestureRecognizerRepresentable {
    final class Coordinator: NSObject, UIGestureRecognizerDelegate {
        @objc
        func gestureRecognizer(
            _ gestureRecognizer: UIGestureRecognizer,
            shouldRecognizeSimultaneouslyWith other: UIGestureRecognizer
        ) -> Bool {
            true
        }
    }

    @Binding var location: CGPoint?

    func makeCoordinator(converter: CoordinateSpaceConverter) -> Coordinator {
        Coordinator()
    }

    func makeUIGestureRecognizer(context: Context) -> UILongPressGestureRecognizer {
        let recognizer = UILongPressGestureRecognizer()
        recognizer.minimumPressDuration = 0
        recognizer.delegate = context.coordinator

        return recognizer
    }

    func handleUIGestureRecognizerAction(
        _ recognizer: UIGestureRecognizerType, context: Context) {
            switch recognizer.state {
                case .began:
                    location = context.converter.localLocation
                case .ended, .cancelled, .failed:
                    location = nil
                default:
                    break
            }
        }
    }



================================================
FILE: Demos/FilmFinder/FilmFinder/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/FilmFinder/FilmFinder/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/FilmFinder/FilmFinder/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "icon.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/FilmFinder/FilmFinder/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/AppConstants.swift
================================================
//
//  AppConstants.swift
//  PuLIDDemo
//
//  Created by Todd Hamilton on 9/28/24.
//

import AIProxy

#error(
    """
    Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
    Please see https://www.aiproxy.pro/docs/integration-guide.html")
    """
)

/* Uncomment for BYOK use cases */
let replicateService = AIProxy.replicateDirectService(
    unprotectedAPIKey: "your-replicate-key"
)

/* Uncomment for all other production use cases */
//let replicateService = AIProxy.replicateService(
//    partialKey: "partial-key-from-your-developer-dashboard",
//    serviceURL: "service-url-from-your-developer-dashboard"
//)



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/ContentView.swift
================================================
//
//  ContentView.swift
//  PuLIDDemo
//
//  Created by Todd Hamilton on 9/26/24.
//

import SwiftUI
import AIProxy
import PhotosUI

struct ContentView: View {

    @State var isAnimating = false
    @State private var selectedPhoto: PhotosPickerItem?
    @State private var image: UIImage? = UIImage(named: "pulid")
    @State var prompt = ""
    @State private var isLoading = false
    @State var counter: Int = 0
    @State var origin: CGPoint = .zero
    @FocusState var isFocused : Bool
    
    var body: some View {
        ZStack {
            
            if #available(iOS 18.0, *) {
                MeshGradient(width: 2, height: 2, points: [
                    [0, 0], [1, 0], [0, 1], [1, 1]
                ], colors: [.blue, .teal, .cyan, .blue])
                .ignoresSafeArea()
            } else {
                // Fallback on earlier versions
                Color.purple
            }
            
            LinearGradient(gradient: Gradient(colors: [.white.opacity(0), .black]), startPoint: .top, endPoint: .bottom)
                .ignoresSafeArea()
                    
            VStack{
                
                ZStack(alignment: .bottom){

                    if let image = image {
                        Image(uiImage: image)
                            .resizable()
                            .scaledToFill()
                            .frame(height:400)
                            .overlay(
                                RoundedRectangle(cornerRadius: 16)
                                    .stroke(
                                        LinearGradient(
                                            gradient: Gradient(colors: [.white.opacity(0.75), .white.opacity(0.25), .white.opacity(0.25), .white.opacity(0.15), .white.opacity(0.15)]),
                                            startPoint: .topLeading,
                                            endPoint: .bottomTrailing
                                        ),
                                        lineWidth: 2
                                    )
                                    .blendMode(.overlay)
                            )
                            .cornerRadius(16)
                            .modifier(RippleEffect(at: origin, trigger: counter))
                            .shadow(color:.black.opacity(0.14),radius: 24)
                            .shadow(radius: 25)
                        

                        PhotosPicker(selection: $selectedPhoto, matching: .images){
                            Image(systemName: "photo.circle.fill")
                        }
                        .foregroundStyle(.regularMaterial)
                        .buttonBorderShape(.circle)
                        .padding()
                        .font(.largeTitle)
                        .textCase(.uppercase)

                        if isLoading{
                            ProgressView(){
                                Text("Generating")
                                    .foregroundStyle(.white)
                                    .font(.system(size: 12, weight:.semibold, design: .monospaced))
                            }
                            .tint(.white)
                            .frame(maxWidth: .infinity, maxHeight: 400)
                            .background(.black.opacity(0.5))
                            .cornerRadius(16)
                        }
                    }
                }
                .padding()
                .task(id: selectedPhoto) {
                    if let data = try? await selectedPhoto?.loadTransferable(type: Data.self),
                       let uiImage = UIImage(data: data) {
                        image = uiImage
                    }
                }
                
                VStack(spacing:12){
                    
                    Text("Type a prompt to generate a new image.")
                        .font(.caption)
                        .fontWeight(.medium)
                        .textCase(.uppercase)
                        .foregroundStyle(.white)
                        .fontDesign(.monospaced)
                    
                    ZStack{
                        TextField("Prompt", text: $prompt)
                            .focused($isFocused)
                            .submitLabel(.done)
                            .onSubmit {
                                isFocused = false
                                Task{ try await generateImage() }
                            }
                    }
                    .padding(.vertical)
                    .padding(.horizontal, 12)
                    .fontDesign(.monospaced)
                    .background(.white)
                    .clipShape(RoundedRectangle(cornerRadius: 8))
                    .font(.system(size: 14))
                    .shadow(radius: 24)
                    
                    HStack{
                        Button("Generate"){
                            isFocused = false
                            Task{ try await generateImage() }
                        }
                        .font(.system(size: 16, weight: .semibold, design: .monospaced))
                        .textCase(.uppercase)
                        .controlSize(.large)
                        .padding(8)
                        .buttonStyle(.borderedProminent)
                        .tint(.teal)
                        .disabled(isLoading ? true : false)
                    }
                    
                }
                .padding(16)
                
                Spacer()
            }
        }
        .preferredColorScheme(.light)
    }
    
    private func generateImage() async throws {
        
        withAnimation(.default.delay(0.5)){ isLoading = true }
        
        defer {
            withAnimation(){ isLoading = false }
        }
        
        guard let imageURL = AIProxy.encodeImageAsURL(image: image!, compressionQuality: 0.8) else {
            print("Could not convert image to a local data URI")
            return
        }

        do {
            let input = ReplicateFluxPulidInputSchema(
                mainFaceImage: imageURL,
                prompt: prompt,
                numOutputs: 1,
                startStep: 4
            )
            let output = try await replicateService.createFluxPulidImage(
                input: input
            )
            print("Done creating Flux-PuLID image: ", output)
            
            if let url = output.first?.absoluteString {
                image = await loadImage(from: url)
            }
            
            counter += 1
            
        }  catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {
            print("Received non-200 status code: \(statusCode) with response body: \(responseBody)")
        } catch {
            print("Could not create Flux-Pulid images: \(error.localizedDescription)")
        }
       
    }
    
    // Function to load an image from a URL
    private func loadImage(from url: String) async -> UIImage? {
        guard let url = URL(string: url) else { return nil }
        do {
            let (data, _) = try await URLSession.shared.data(from: url)
            return UIImage(data: data)
        } catch {
            print("Failed to load image from URL: \(error.localizedDescription)")
            return nil
        }
    }
    
}

#Preview {
    ContentView()
}



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/Info.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict/>
</plist>



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/PuLIDDemoApp.swift
================================================
//
//  PuLIDDemoApp.swift
//  PuLIDDemo
//
//  Created by Todd Hamilton on 9/26/24.
//

import SwiftUI

@main
struct PuLIDDemoApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/Ripple.metal
================================================
//
//  Ripple.metal
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/21/24.
//

// Insert #include <metal_stdlib>
#include <SwiftUI/SwiftUI.h>
using namespace metal;

[[ stitchable ]]
half4 Ripple(
    float2 position,
    SwiftUI::Layer layer,
    float2 origin,
    float time,
    float amplitude,
    float frequency,
    float decay,
    float speed
) {
    // The distance of the current pixel position from `origin`.
    float distance = length(position - origin);
    // The amount of time it takes for the ripple to arrive at the current pixel position.
    float delay = distance / speed;

    // Adjust for delay, clamp to 0.
    time -= delay;
    time = max(0.0, time);

    // The ripple is a sine wave that Metal scales by an exponential decay
    // function.
    float rippleAmount = amplitude * sin(frequency * time) * exp(-decay * time);

    // A vector of length `amplitude` that points away from position.
    float2 n = normalize(position - origin);

    // Scale `n` by the ripple amount at the current pixel position and add it
    // to the current pixel position.
    //
    // This new position moves toward or away from `origin` based on the
    // sign and magnitude of `rippleAmount`.
    float2 newPosition = position + rippleAmount * n;

    // Sample the layer at the new position.
    half4 color = layer.sample(newPosition);

    // Lighten or darken the color based on the ripple amount and its alpha
    // component.
    color.rgb += 0.3 * (rippleAmount / amplitude) * color.a;

    return color;
}





================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/Ripple.swift
================================================
//
//  Ripple.swift
//  AIColorPalette
//
//  Created by Todd Hamilton on 6/21/24.
//

import SwiftUI

struct PushEffect<T: Equatable>: ViewModifier {
    var trigger: T

    func body(content: Content) -> some View {
        content.keyframeAnimator(
            initialValue: 1.0,
            trigger: trigger
        ) { view, value in
            view.visualEffect { view, _ in
                view.scaleEffect(value)
            }
        } keyframes: { _ in
            SpringKeyframe(0.95, duration: 0.2, spring: .snappy)
            SpringKeyframe(1.0, duration: 0.2, spring: .bouncy)
        }
    }
}

/// A modifer that performs a ripple effect to its content whenever its
/// trigger value changes.
struct RippleEffect<T: Equatable>: ViewModifier {
    var origin: CGPoint

    var trigger: T

    init(at origin: CGPoint, trigger: T) {
        self.origin = origin
        self.trigger = trigger
    }

    func body(content: Content) -> some View {
        let origin = origin
        let duration = duration

        content.keyframeAnimator(
            initialValue: 0,
            trigger: trigger
        ) { view, elapsedTime in
            view.modifier(RippleModifier(
                origin: origin,
                elapsedTime: elapsedTime,
                duration: duration
            ))
        } keyframes: { _ in
            MoveKeyframe(0)
            LinearKeyframe(duration, duration: duration)
        }
    }

    var duration: TimeInterval { 3 }
}

/// A modifier that applies a ripple effect to its content.
struct RippleModifier: ViewModifier {
    var origin: CGPoint

    var elapsedTime: TimeInterval

    var duration: TimeInterval

    var amplitude: Double = 12
    var frequency: Double = 15
    var decay: Double = 8
    var speed: Double = 1200

    func body(content: Content) -> some View {
        let shader = ShaderLibrary.Ripple(
            .float2(origin),
            .float(elapsedTime),

            // Parameters
            .float(amplitude),
            .float(frequency),
            .float(decay),
            .float(speed)
        )

        let maxSampleOffset = maxSampleOffset
        let elapsedTime = elapsedTime
        let duration = duration

        content.visualEffect { view, _ in
            view.layerEffect(
                shader,
                maxSampleOffset: maxSampleOffset,
                isEnabled: 0 < elapsedTime && elapsedTime < duration
            )
        }
    }

    var maxSampleOffset: CGSize {
        CGSize(width: amplitude, height: amplitude)
    }
}

extension View {
    func onPressingChanged(_ action: @escaping (CGPoint?) -> Void) -> some View {
        modifier(SpatialPressingGestureModifier(action: action))
    }
}

struct SpatialPressingGestureModifier: ViewModifier {
    var onPressingChanged: (CGPoint?) -> Void

    @State var currentLocation: CGPoint?

    init(action: @escaping (CGPoint?) -> Void) {
        self.onPressingChanged = action
    }

    func body(content: Content) -> some View {
        let gesture = SpatialPressingGesture(location: $currentLocation)

        content
            .gesture(gesture)
            .onChange(of: currentLocation, initial: false) { _, location in
                onPressingChanged(location)
            }
    }
}

struct SpatialPressingGesture: UIGestureRecognizerRepresentable {
    final class Coordinator: NSObject, UIGestureRecognizerDelegate {
        @objc
        func gestureRecognizer(
            _ gestureRecognizer: UIGestureRecognizer,
            shouldRecognizeSimultaneouslyWith other: UIGestureRecognizer
        ) -> Bool {
            true
        }
    }

    @Binding var location: CGPoint?

    func makeCoordinator(converter: CoordinateSpaceConverter) -> Coordinator {
        Coordinator()
    }

    func makeUIGestureRecognizer(context: Context) -> UILongPressGestureRecognizer {
        let recognizer = UILongPressGestureRecognizer()
        recognizer.minimumPressDuration = 0
        recognizer.delegate = context.coordinator

        return recognizer
    }

    func handleUIGestureRecognizerAction(
        _ recognizer: UIGestureRecognizerType, context: Context) {
            switch recognizer.state {
                case .began:
                    location = context.converter.localLocation
                case .ended, .cancelled, .failed:
                    location = nil
                default:
                    break
            }
        }
    }



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/Assets.xcassets/pulid.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "pulid.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/PuLIDDemo/PuLIDDemo/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Stickers/Stickers/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import AIProxy

enum AppConstants {
    #error(
        """
        Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
        Please see https://www.aiproxy.pro/docs/integration-guide.html")
        """
    )
    
    /* Uncomment for BYOK use cases */
    static let openAIService = AIProxy.openAIDirectService(
        unprotectedAPIKey: "your-openai-key"
    )

    /* Uncomment for all other production use cases */
//    let openAIService = AIProxy.openAIService(
//        partialKey: "partial-key-from-your-developer-dashboard",
//        serviceURL: "service-url-from-your-developer-dashboard"
//    )

}



================================================
FILE: Demos/Stickers/Stickers/AppLogger.swift
================================================
//
//  AppLogger.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import OSLog

/// Log levels available:
///
///     AppLogger.debug
///     AppLogger.info
///     AppLogger.warning
///     AppLogger.error
///     AppLogger.critical
///
/// Flip on metadata logging in Xcode's console to show which source line the log occurred from.
///
/// See my reddit post for a video instructions:
/// https://www.reddit.com/r/SwiftUI/comments/15lsdtk/how_to_use_the_oslog_logger/
let AppLogger = Logger(subsystem: Bundle.main.bundleIdentifier ?? "UnknownApp",
                       category: "AIProxyBootstrapStickers")



================================================
FILE: Demos/Stickers/Stickers/ButtonStyles.swift
================================================
//
//  ButtonStyles.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct ButtonStyles: View {
    var body: some View {
        
        ZStack{
            Color(.systemGroupedBackground)
                .ignoresSafeArea()
            
            VStack(spacing:48){
                
                Button{
                    /// do something
                }label:{
                    HStack(spacing:6){
                        Image(systemName: "play.circle.fill")
                            .font(.system(size: 15, weight:.semibold, design: .rounded))
                        Text("10s")
                            .font(.system(size: 11, weight: .regular, design: .monospaced))
                            .fontDesign(.monospaced)
                    }
                }
                .buttonStyle(TranscriptionButtonStyle())

                Button(){
                    /// do something
                } label:{
                    Label("Chunky Button", systemImage: "sparkles")
                }
                .buttonStyle(ChunkyButtonStyle(offsetSize: 10.0))

                Button{
                    /// do something
                } label: {
                    ZStack{
                        RoundedRectangle(cornerRadius: 45, style:.continuous)
                            .fill(.red.gradient)
                            .frame(width:85, height:85)
                        Image(systemName: "waveform")
                            .font(.title)
                            .foregroundColor(.white)
                    }
                }
                .buttonStyle(RecordButton())
                
                Button(){
                    /// do something
                }label:{
                    Image(systemName: "arrow.forward")
                        .font(.system(size: 17, weight: .bold))
                }
                .buttonStyle(TranslateButton())
                
            }
            .padding()
        }
        
    }
}

struct ChunkyButtonStyle: ButtonStyle {

    var offsetSize = 10.0

    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .font(.system(size: 24, weight: .bold, design: .rounded))
            .foregroundColor(.black)
            .padding()
            .offset(y: configuration.isPressed ? offsetSize-2 : 0)
            .background(
                GeometryReader{ geo in
                    RoundedRectangle(cornerRadius: 17)
                        .strokeBorder(Color.black, lineWidth: 4)
                        .background(
                            RoundedRectangle(cornerRadius: 17)
                                .fill(.white)
                        )
                        .offset(y:offsetSize)

                        .overlay(
                            RoundedRectangle(cornerRadius: 17)
                                .fill(.white)
                                .strokeBorder(Color.black, lineWidth: 4)
                                .background(RoundedRectangle(cornerRadius: 17).fill(Color.white))
                                .offset(y: configuration.isPressed ? offsetSize : 0)
                        )
                }
            )
    }
}

struct RecordButton: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .frame(maxWidth:80, maxHeight: 80)
            .shadow(color:.black.opacity(0.28), radius: 10, y:8)
            .scaleEffect(configuration.isPressed ? 0.9 : 1, anchor: .center)
            .animation(.easeOut(duration: 0.2), value: configuration.isPressed)
            .padding(.bottom, 40)
            
    }
}

struct TranslateButton: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .foregroundColor(.white)
            .frame(width:56, height:56)
            .background(
                Circle()
                    .fill(.teal.gradient)
            )
            .brightness(configuration.isPressed ? -0.1 : 0.0)
            .scaleEffect(configuration.isPressed ? 0.9 : 1, anchor: .center)
            .animation(.easeOut(duration: 0.2), value: configuration.isPressed)
    }
}

struct TranscriptionButtonStyle: ButtonStyle {

    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .foregroundColor(.secondary)
            .padding(.leading, 4)
            .padding(.trailing, 8)
            .padding(.vertical, 4)
            .background(
                Capsule().fill(.secondary.opacity(configuration.isPressed ? 0.28 : 0.14))
            )
            .scaleEffect(configuration.isPressed ? 0.9 : 1.0)
    }
}

#Preview {
    ButtonStyles()
}



================================================
FILE: Demos/Stickers/Stickers/StickerDataLoader.swift
================================================
//
//  StickerDataLoader.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import Vision
import UIKit
import AIProxy

final actor StickerDataLoader {
    /// Creates a sticker from a given `prompt` using OpenAI's APIs
    /// On simulator, the sticker has an opaque background because the Vision framework is not available.
    /// On device, the sticker has a transparent background
    ///
    /// - Parameter prompt: The user-entered prompt
    /// - Returns: A sticker as a UIImage if we were able to get one from OpenAI, or nil otherwise
    func create(fromPrompt prompt: String) async throws -> UIImage? {
        let requestBody = OpenAICreateImageRequestBody(
            prompt: "cute design of a " + prompt + " kawaii sticker. nothing in the bg. white bg.",
            model: "dall-e-3"
        )
        let response = try await AppConstants.openAIService.createImageRequest(body: requestBody)
        print(response.data.first?.url ?? "")
        
        guard let url = response.data.first?.url, let data = try? Data(contentsOf: url) else {
            AppLogger.error("OpenAI returned a sticker imageURL that we could not fetch")
            return nil
        }

        guard let img = UIImage(data: data) else {
            AppLogger.error("Could not create a UIImage from the imageURL provided by OpenAI")
            return nil
        }
        return img.extractForegroundWithVision() ?? img
    }
}


private extension UIImage {

    convenience init?(pixelBuffer: CVPixelBuffer) {
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let context = CIContext(options: nil)
        guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else {
            return nil
        }
        self.init(cgImage: cgImage)
    }

    func extractForegroundWithVision() -> UIImage? {
        guard let cgImage = self.cgImage else { return nil }
        let request = VNGenerateForegroundInstanceMaskRequest()
        let handler = VNImageRequestHandler(cgImage: cgImage)
        do {
            try handler.perform([request])
            guard let result = request.results?.first else { return nil }

            let foregroundPixelBuffer = try result.generateMaskedImage(
                ofInstances: result.allInstances,
                from: handler,
                croppedToInstancesExtent: false
            )

            if let foregroundImage = UIImage(pixelBuffer: foregroundPixelBuffer) {
                return foregroundImage
            }
        } catch {
            AppLogger.info("Could not use Vision to cut the sticker out. Perhaps you are running on simulator?")
        }
        return nil
    }
}



================================================
FILE: Demos/Stickers/Stickers/StickerImageView.swift
================================================
//
//  StickerImageView.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI

/// Holds a sticker image.
/// The sticker animates into view with a scale effect, and then floats in the Y-axis.
struct StickerImageView: View {

    /// The sticker as UIImage
    let uiImage: UIImage

    @State private var floating = false
    @State private var showSticker = false
    private let floatingAnimation = Animation.easeInOut(duration: 2.0).repeatForever(autoreverses: true)

    var body: some View {
        Image(uiImage: uiImage)
            .resizable()
            .scaledToFit()
            .cornerRadius(14)
            .shadow(color:.black.opacity(0.28), radius: 8, x:0, y:4)
            .padding()
            .offset(y:floating ? 8.0 : -8.0)
            .animation(floatingAnimation, value: floating)
            .scaleEffect(showSticker ? 1.0 : 0.5)
            .animation(.bouncy, value: showSticker)
            .onAppear{
                withAnimation(.bouncy){
                    floating = true
                    showSticker = true
                }
            }
    }
}




================================================
FILE: Demos/Stickers/Stickers/StickerInputView.swift
================================================
//
//  StickerInputView.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI

/// The user enters a sticker prompt using this view.
struct StickerInputView: View {

    enum FocusedField {
        case currentPrompt
    }

    /// Bind to a UI model's property for that property to change as the user enters text,
    /// and for programmatic changes to the UI model's property to be reflected in this view
    @Binding var currentPrompt: String
    @FocusState private var focusedField: FocusedField?

    var body: some View {
        VStack(spacing:8){
            Text("Describe your sticker below")
                .frame(maxWidth: .infinity, alignment: .topLeading)
                .font(.system(size: 20, weight: .bold, design: .rounded))
                .foregroundColor(.black.opacity(0.28))
            TextField("type here...", text: $currentPrompt, axis: .vertical)
                .focused($focusedField, equals: .currentPrompt)
                .frame(maxWidth: .infinity, maxHeight: .infinity, alignment: .topLeading)
                .font(.system(size: 36, weight: .bold, design: .rounded))
                .textFieldStyle(.plain)
                .foregroundColor(.black.opacity(0.75))
                .onAppear {
                    focusedField = .currentPrompt
                }
        }
        .padding()
    }
}




================================================
FILE: Demos/Stickers/Stickers/StickerLoadingView.swift
================================================
//
//  StickerLoadingView.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI

@MainActor
struct StickerLoadingView: View {

    /// Loading text to display while long requests to OpenAI are fulfilled
    @State private var currentLoadState = "Hold tight"

    var body: some View {
        VStack(spacing:16){
            ProgressView()
                .controlSize(.extraLarge)
                .tint(.white)
            Text(currentLoadState)
                .transition(.move(edge: .bottom))
                .font(.system(size: 20, weight: .semibold, design: .rounded))
                .foregroundColor(.black.opacity(0.28))
        }
        .frame(maxWidth:.infinity, maxHeight:.infinity)
        .onAppear {
            Task {
                try await Task.sleep(for: .seconds(4))
                currentLoadState = "Generating sticker"
                try await Task.sleep(for: .seconds(4))
                currentLoadState = "Finalizing"
            }
        }
    }
}



================================================
FILE: Demos/Stickers/Stickers/StickerManager.swift
================================================
//
//  StickerManager.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import UIKit
import SwiftUI

/// The default message to display in the result view of the sticker experience.
private let defaultUserMessage = "Tap on the image to copy to your clipboard."

@MainActor
@Observable
final class StickerManager {

    /// The user-entered prompt
    var prompt: String = ""

    /// The current background color to use for the view
    var currentColor: Color = .teal

    /// The generated sticker as a UIImage
    private(set) var image: UIImage?

    /// A flag to indicate that the sticker is being generated and we are waiting on I/O from OpenAI
    private(set) var isProcessing = false

    /// The user message to display along with the generated sticker
    private(set) var userMessage = defaultUserMessage

    /// The set of potential background colors for the view
    private var bgColors: Set<Color> = [.teal, .mint, .indigo, .red, .pink, .purple, .orange, .brown, .blue, .cyan, .green, .yellow, .gray]

    /// A few examples to get the user's wheels turning
    private let placeholderExamples = [
        "a cactus wearing a sombrero...",
        "a hedgehog riding a motorcycle...",
        "a kangaroo holding a basketball..."
    ]
    private var placeholderIndex = 0

    private let stickerDataLoader = StickerDataLoader()

    /// Changes the user message briefly away from the default text.
    /// After two seconds, the user message reverts to the default message
    func flashUserMessage(_ message: String) {
        withAnimation(.bouncy) {
            userMessage = message
        }
        Task { [weak self] in
            try await Task.sleep(for: .seconds(2))
            withAnimation(.bouncy) { [weak self] in
                self?.userMessage = defaultUserMessage
           }
        }
    }

    /// Creates a sticker from the current prompt stored in `self.prompt`
    func createSticker() {
        guard !self.isProcessing else {
            AppLogger.info("Already creating a sticker. Please wait")
            return
        }

        let prompt = self.prompt
        guard prompt.count > 0 else {
            AppLogger.error("Trying to submit a sticker without a prompt. This is a programmer error")
            return
        }

        self.isProcessing = true
        Task {
            self.image = try await stickerDataLoader.create(fromPrompt: prompt)
            self.isProcessing = false
        }
    }

    /// Change the placeholder prompt
    func nextPlaceholder() {
        self.placeholderIndex = (self.placeholderIndex + 1) % self.placeholderExamples.count
        self.prompt = self.placeholderExamples[self.placeholderIndex]
    }

    /// Returns to the starting point of the sticker experience, e.g. where no sticker is in the UI
    func startOver() {
        self.image = nil
        self.nextPlaceholder()
        self.currentColor = self.bgColors.randomElement()!
    }

    /// Regenerate a sticker using the same prompt
    func regenerate() {
        self.image = nil
        self.createSticker()
        self.currentColor = self.bgColors.randomElement()!
    }
}




================================================
FILE: Demos/Stickers/Stickers/StickersApp.swift
================================================
//
//  StickersApp.swift
//  Stickers
//
//  Created by Lou Zell
//

import SwiftUI

@main
@MainActor
struct StickersApp: App {

    @State var stickerManager = StickerManager()

    var body: some Scene {
        WindowGroup {
            StickerView(stickerManager: stickerManager)
        }
    }
}



================================================
FILE: Demos/Stickers/Stickers/StickerView.swift
================================================
//
//  StickerView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

@MainActor
struct StickerView: View {
    @Environment(\.dismiss) private var dismiss

    /// The manager that drives this view
    @Bindable var stickerManager: StickerManager

    var body: some View {
        VStack{
            if let image = stickerManager.image {
                buildResultView(withUIImage: image)
            } else {
                promptView
            }
        } 
        .toolbar {
            stickerToolbar
        }
        .onAppear() {
            stickerManager.startOver()
        }
    }

    private func buildResultView(withUIImage uiImage: UIImage) -> some View {
        VStack(spacing:16) {
            VStack{
                StickerImageView(uiImage: uiImage)
                    .onTapGesture {
                        self.copyImage(uiImage: uiImage)
                    }
                Spacer()
                userMessageView
                Spacer()
            }
            .background(stickerManager.currentColor.gradient)
            .cornerRadius(17)
            startOverButton
            regenerateButton
        }
        .padding()
    }

    private var promptView: some View {
        VStack(spacing:16) {
            if stickerManager.isProcessing {
                StickerLoadingView()
                    .background(stickerManager.currentColor.gradient)
                    .cornerRadius(17)
            } else {
                StickerInputView(currentPrompt: $stickerManager.prompt)
                    .background(stickerManager.currentColor.gradient)
                    .cornerRadius(17)
                generateButton
            }
        }
        .padding()
    }

    private var stickerToolbar: some View {
        Button("Clear") {
            stickerManager.prompt = ""
        }
        .disabled(stickerManager.prompt.count == 0)
    }

    private var userMessageView: some View {
        Text(stickerManager.userMessage)
            .font(.system(size: 24, weight: .semibold, design: .rounded))
            .multilineTextAlignment(.center)
            .transition(.scale)
    }

    // MARK: - Actions

    /// Copy the passed `uiImage` to system clipboard
    private func copyImage(uiImage: UIImage) {
        UIPasteboard.general.image = uiImage
        stickerManager.flashUserMessage("Copied!")
    }

    // MARK: - Buttons

    /// Button to start from the prompt input view and dispose of the existing sticker
    private var startOverButton: some View {
        Button {
            withAnimation {
                stickerManager.startOver()
            }
        } label: {
            Label("Start Over", systemImage: "sparkles")
                .frame(maxWidth:.infinity)
        }
        .buttonStyle(ChunkyButtonStyle())
    }
    
    /// Button to regenerate a sticker from the current prompt, disposing of the existing sticker.
    private var regenerateButton: some View {
        Button{
            withAnimation{
                stickerManager.regenerate()
            }
        }label:{
            Label("Regenerate", systemImage: "arrow.triangle.2.circlepath")
                .frame(maxWidth:.infinity)
        }
        .buttonStyle(ChunkyButtonStyle())
    }

    /// Button to generate a sticker from the current user prompt
    private var generateButton: some View {
        Button {
            withAnimation() {
                stickerManager.createSticker()
            }
        } label: {
            Label("Generate", systemImage: "sparkles")
                .frame(maxWidth: .infinity)
        }
        .buttonStyle(ChunkyButtonStyle())
    }
}

#Preview {
    StickerView(stickerManager: StickerManager())
}



================================================
FILE: Demos/Stickers/Stickers/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Stickers/Stickers/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Stickers/Stickers/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "sticker.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Stickers/Stickers/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Transcriber/Transcriber/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftData
import AIProxy

/// Use this actor for audio work
@globalActor actor AudioActor {
    static let shared = AudioActor()
}

enum AppConstants {

    static let swiftDataModels: [any PersistentModel.Type] = [AudioRecording.self, TranscribedAudioRecording.self]
    static let swiftDataContainer = try! ModelContainer(for: AudioRecording.self, TranscribedAudioRecording.self)

    static let audioSampleQueue = DispatchQueue(label: "com.AIProxyBootstrap.audioSampleQueue")

    #error(
        """
        Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
        Please see https://www.aiproxy.pro/docs/integration-guide.html")
        """
    )

    /* Uncomment for BYOK use cases */
    static let openAIService = AIProxy.openAIDirectService(
        unprotectedAPIKey: "your-openai-key"
    )

    /* Uncomment for all other production use cases */
//    let openAIService = AIProxy.openAIService(
//        partialKey: "partial-key-from-your-developer-dashboard",
//        serviceURL: "service-url-from-your-developer-dashboard"
//    )
}



================================================
FILE: Demos/Transcriber/Transcriber/AppLogger.swift
================================================
//
//  AppLogger.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import OSLog

/// Log levels available:
///
///     AppLogger.debug
///     AppLogger.info
///     AppLogger.warning
///     AppLogger.error
///     AppLogger.critical
///
/// Flip on metadata logging in Xcode's console to show which source line the log occurred from.
///
/// See my reddit post for a video instructions:
/// https://www.reddit.com/r/SwiftUI/comments/15lsdtk/how_to_use_the_oslog_logger/
let AppLogger = Logger(subsystem: Bundle.main.bundleIdentifier ?? "UnknownApp",
                       category: "AIProxyBootstrapTranscriber")



================================================
FILE: Demos/Transcriber/Transcriber/AudioFileWriter.swift
================================================
//
//  AudioFileWriter.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import AVFoundation

/// One of the following errors will be thrown at initialization if the microphone vendor can't vend samples.
enum AudioFileWriterError: Error {
    case couldNotWriteToDestinationURL
    case couldNotCreateAudioInput
}


/// Writes an m4a file out of audio sample buffers.
/// Samples passed to the `append` method will be written to the m4a file between calls to `init()` and `finishWriting()`.
/// Create one instance of AudioFileWriter for each audio file that you'd like to write.
@AudioActor
final class AudioFileWriter {
    /// The location to write the audio file to
    let fileURL: URL

    private let assetWriter: AVAssetWriter
    private let microphoneWriter: AVAssetWriterInput
    private let audioSettings: [String: Any] = [
        AVFormatIDKey: kAudioFormatMPEG4AAC,
        AVSampleRateKey: 48_000,
        AVNumberOfChannelsKey: 2,
        AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
    ]

    private var isWriting = false
    
    /// Throws one of `AudioFileWriterError` if we can't initialize the AVFoundation dependencies
    /// - Parameter fileURL: The location to write the audio file to
    init(fileURL: URL) throws {
        self.fileURL = fileURL
        do {
            self.assetWriter = try AVAssetWriter(outputURL: fileURL, fileType: .m4a)
        } catch {
            throw AudioFileWriterError.couldNotWriteToDestinationURL
        }

        self.microphoneWriter = AVAssetWriterInput(mediaType: .audio, outputSettings: self.audioSettings)
        self.microphoneWriter.expectsMediaDataInRealTime = true

        if self.assetWriter.canAdd(self.microphoneWriter) {
            self.assetWriter.add(self.microphoneWriter)
        } else {
            throw AudioFileWriterError.couldNotCreateAudioInput
        }
    }
    
    /// Append a sample buffer to the audio file
    /// - Parameter sample: A core media sample buffer. See the `MicrophoneSampleVendor` file for an example of how to source these.
    func append(sample: CMSampleBuffer) {
        if !self.isWriting {
            self.assetWriter.startWriting()
            self.assetWriter.startSession(atSourceTime: sample.presentationTimeStamp)
            self.isWriting = true
        }
        if self.microphoneWriter.isReadyForMoreMediaData {
            self.microphoneWriter.append(sample)
        } else {
            AppLogger.warning("The AudioFileWriter is not ready for more audio data")
        }
    }
    
    /// Finishes writing the file to disk
    /// - Returns: URL location of the m4a file on disk
    func finishWriting() async -> URL {
        self.microphoneWriter.markAsFinished()
        await self.assetWriter.finishWriting()
        self.isWriting = false
        return self.fileURL
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/AudioRecorder.swift
================================================
//
//  Manager.swift
//  OpenAIExperiment
//
//  Created by Lou Zell
//

import AVFoundation
import Foundation

@AudioActor
final class AudioRecorder {
    private var microphoneSampleVendor: MicrophoneSampleVendor?
    private var audioFileWriter: AudioFileWriter?

    nonisolated init() {}
    
    /// Start recording an audio file
    /// - Returns: true if the audio recorder was able to start recording, false otherwise
    func start() -> Bool {
        do {
            self.microphoneSampleVendor = try MicrophoneSampleVendor()
        } catch {
            AppLogger.error("Could not create a MicrophoneSampleVendor: \(error)")
            return false
        }

        do {
            self.audioFileWriter = try AudioFileWriter(fileURL: FileUtils.getFileURL())
        } catch {
            AppLogger.error("Could not create an audio file writer: \(error)")
            return false
        }

        self.microphoneSampleVendor?.start(onSample: { [weak self] sampleBuffer in
            self?.audioFileWriter?.append(sample: sampleBuffer)
        })
        return true
    }

    /// Returns the recording created between calls to `startRecording` and `stopRecording`
    func stopRecording(duration: String) async -> AudioRecording? {
        guard let fileWriter = self.audioFileWriter,
              let sampleVendor = self.microphoneSampleVendor else
        {
            AppLogger.warning("Expected audio dependencies to be set")
            return nil
        }
        sampleVendor.stop()
        let url = await fileWriter.finishWriting()
        return AudioRecording(localUrl: url, duration: duration)
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/AudioRecording.swift
================================================
//
//  AudioRecording.swift
//  Transcriber
//
//  Created by Lou Zell
//

import Foundation
import SwiftData

/// Encapsulates a recording. The `url` is the location on disk of the raw audio file (an m4a).
@Model
final class AudioRecording {
    @Attribute(.unique) let localUrl: URL
    let duration: String

    init(localUrl: URL, duration: String) {
        self.localUrl = localUrl
        self.duration = duration
    }

    var resolvedURL: URL? {
        // There is a little nuance here. Every time you build and run the app the apple sandbox changes.
        // We first try to find the associated file at the spot that we stored it, but if it's not there then
        // we construct a new URL based on the current apple sandbox
        if (FileManager.default.fileExists(atPath: self.localUrl.path)) {
            return self.localUrl
        } else {
            let resolvedURL = FileUtils.getDocumentsURL().appending(component: self.localUrl.lastPathComponent)
            if FileManager.default.fileExists(atPath: resolvedURL.path) {
                return resolvedURL
            }
        }
        return nil
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/ButtonStyles.swift
================================================
//
//  ButtonStyles.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct ButtonStyles: View {
    var body: some View {
        
        ZStack{
            Color(.systemGroupedBackground)
                .ignoresSafeArea()
            
            VStack(spacing:48){
                
                Button{
                    /// do something
                }label:{
                    HStack(spacing:6){
                        Image(systemName: "play.circle.fill")
                            .font(.system(size: 15, weight:.semibold, design: .rounded))
                        Text("10s")
                            .font(.system(size: 11, weight: .regular, design: .monospaced))
                            .fontDesign(.monospaced)
                    }
                }
                .buttonStyle(TranscriptionButtonStyle())

                Button(){
                    /// do something
                } label:{
                    Label("Chunky Button", systemImage: "sparkles")
                }
                .buttonStyle(ChunkyButtonStyle(offsetSize: 10.0))

                Button{
                    /// do something
                } label: {
                    ZStack{
                        RoundedRectangle(cornerRadius: 45, style:.continuous)
                            .fill(.red.gradient)
                            .frame(width:85, height:85)
                        Image(systemName: "waveform")
                            .font(.title)
                            .foregroundColor(.white)
                    }
                }
                .buttonStyle(RecordButton())
                
                Button(){
                    /// do something
                }label:{
                    Image(systemName: "arrow.forward")
                        .font(.system(size: 17, weight: .bold))
                }
                .buttonStyle(TranslateButton())
                
            }
            .padding()
        }
        
    }
}

struct ChunkyButtonStyle: ButtonStyle {

    var offsetSize = 10.0

    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .font(.system(size: 24, weight: .bold, design: .rounded))
            .foregroundColor(.black)
            .padding()
            .offset(y: configuration.isPressed ? offsetSize-2 : 0)
            .background(
                GeometryReader{ geo in
                    RoundedRectangle(cornerRadius: 17)
                        .strokeBorder(Color.black, lineWidth: 4)
                        .background(
                            RoundedRectangle(cornerRadius: 17)
                                .fill(.white)
                        )
                        .offset(y:offsetSize)

                        .overlay(
                            RoundedRectangle(cornerRadius: 17)
                                .fill(.white)
                                .strokeBorder(Color.black, lineWidth: 4)
                                .background(RoundedRectangle(cornerRadius: 17).fill(Color.white))
                                .offset(y: configuration.isPressed ? offsetSize : 0)
                        )
                }
            )
    }
}

struct RecordButton: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .frame(maxWidth:80, maxHeight: 80)
            .shadow(color:.black.opacity(0.28), radius: 10, y:8)
            .scaleEffect(configuration.isPressed ? 0.9 : 1, anchor: .center)
            .animation(.easeOut(duration: 0.2), value: configuration.isPressed)
            .padding(.bottom, 40)
            
    }
}

struct TranslateButton: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .foregroundColor(.white)
            .frame(width:56, height:56)
            .background(
                Circle()
                    .fill(.teal.gradient)
            )
            .brightness(configuration.isPressed ? -0.1 : 0.0)
            .scaleEffect(configuration.isPressed ? 0.9 : 1, anchor: .center)
            .animation(.easeOut(duration: 0.2), value: configuration.isPressed)
    }
}

struct TranscriptionButtonStyle: ButtonStyle {

    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .foregroundColor(.secondary)
            .padding(.leading, 4)
            .padding(.trailing, 8)
            .padding(.vertical, 4)
            .background(
                Capsule().fill(.secondary.opacity(configuration.isPressed ? 0.28 : 0.14))
            )
            .scaleEffect(configuration.isPressed ? 0.9 : 1.0)
    }
}

#Preview {
    ButtonStyles()
}



================================================
FILE: Demos/Transcriber/Transcriber/FileUtils.swift
================================================
//
//  FileUtils.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation


struct FileUtils {
    private init() {
        fatalError("FileUtils is a namespace only")
    }

    static func getDocumentsURL() -> URL {
        guard let documentsUrl = FileManager.default.urls(
            for: .documentDirectory, in: .userDomainMask).first
        else {
            fatalError("Could could not find the Documents directory")
        }
        return documentsUrl
    }

    static func getFileURL() -> URL {
        let documentsUrl = self.getDocumentsURL()
        let isoFormatter = ISO8601DateFormatter()
        isoFormatter.formatOptions = [.withFullDate, .withTime, .withColonSeparatorInTime]
        isoFormatter.timeZone = .current

        var dateString = isoFormatter.string(from: Date())
        dateString = dateString.replacingOccurrences(of: ":", with: ".")
        let filename = "AIProxyBootstrap-\(dateString).m4a"

        return documentsUrl.appendingPathComponent(filename)
    }

    static func deleteFile(at url: URL) {
        do {
            try FileManager.default.removeItem(at: url)
        } catch {
            AppLogger.info("Could not find file to delete at \(url)")
        }
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/MicrophoneSampleVendor.swift
================================================
//
//  MicrophoneSampleVendor.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import CoreMedia
import AVFoundation

/// One of the following errors will be thrown at initialization if the microphone vendor can't vend samples.
enum MicrophoneSampleVendorError: Error {
    case micNotFound
    case micNotUsableAsCaptureDevice
    case captureSessionRejectedMic
    case captureSessionRejectedOutput
}


/// Vends samples of the microphone audio
///
/// ## Requirements
///
/// - Assumes an `NSMicrophoneUsageDescription` description has been added to Target > Info
/// - Assumes that microphone permissions have already been granted
///
/// ## Usage
///
/// ```
///     self.microphoneVendor = try MicrophoneSampleVendor()
///     self.microphoneVendor.start { sample in
///        // Do something with `sample`
///        // Note: this callback is invoked on a background thread
///     }
/// ```
///
@AudioActor
final class MicrophoneSampleVendor {

    private let captureSession = AVCaptureSession()
    private let audioOutput = AVCaptureAudioDataOutput()
    private let sampleBufferDelegate = SampleBufferDelegate()

    init() throws {
        let mic = try findMic()
        let micInput = try mic.asInput()
        try self.captureSession.addMicInput(micInput)
        try self.captureSession.addAudioOutput(self.audioOutput)
        self.audioOutput.setSampleBufferDelegate(self.sampleBufferDelegate,
                                                 queue: AppConstants.audioSampleQueue)
    }

    func start(onSample: @escaping (CMSampleBuffer) -> Void) {
        self.sampleBufferDelegate.sampleCallback = onSample
        self.captureSession.startRunning()
    }

    func stop() {
        self.captureSession.stopRunning()
    }
}

private class SampleBufferDelegate: NSObject, AVCaptureAudioDataOutputSampleBufferDelegate {
    var sampleCallback: ((CMSampleBuffer) -> Void)?

    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        dispatchPrecondition(condition: .onQueue(AppConstants.audioSampleQueue))
        self.sampleCallback?(sampleBuffer)
    }
}

private func findMic() throws -> AVCaptureDevice {
    let microphones = AVCaptureDevice.DiscoverySession(deviceTypes: [.microphone], mediaType: .audio, position: .unspecified).devices
    if let mic = microphones.first {
        return mic
    }
    throw MicrophoneSampleVendorError.micNotFound
}

private extension AVCaptureDevice {
    func asInput() throws -> AVCaptureDeviceInput {
        do {
            return try AVCaptureDeviceInput(device: self)
        } catch {
            throw MicrophoneSampleVendorError.micNotUsableAsCaptureDevice
        }
    }
}

private extension AVCaptureSession {
    func addMicInput(_ micInput: AVCaptureDeviceInput) throws {
        if self.canAddInput(micInput) {
            self.addInput(micInput)
        } else {
            throw MicrophoneSampleVendorError.captureSessionRejectedMic
        }
    }

    func addAudioOutput(_ audioOutput: AVCaptureAudioDataOutput) throws {
        if self.canAddOutput(audioOutput) {
            self.addOutput(audioOutput)
        } else {
            throw MicrophoneSampleVendorError.captureSessionRejectedMic
        }
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/ModelContext+Extensions.swift
================================================
//
//  ModelContext+Extensions.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftData

extension ModelContext {

    /// Deletes all models in `AppConstants.swiftDataModels` from SwiftData.
    /// Use this during development to return to a clean slate.
    func reset() {
        do {
            for model in AppConstants.swiftDataModels {
                try self.delete(model: model)
            }
        } catch {
            AppLogger.error("Failed to reset swift data for all models. Error: \(error.localizedDescription)")
        }
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/NoRecordingsView.swift
================================================
//
//  NoRecordingsView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct NoRecordingsView: View {
    var body: some View {
        VStack{
            Image(systemName: "waveform")
                .font(.largeTitle)
                .foregroundColor(.secondary)
                .padding(.bottom, 8)

            Text("No recordings")
                .font(.headline)
            Text("Tap the record button below to start transcribing.")
                .multilineTextAlignment(.center)
                .frame(maxWidth:240)
                .foregroundColor(.secondary)
                .font(.subheadline)
        }
        .frame(maxHeight:.infinity)
        .foregroundColor(.primary)
        .padding(.bottom, 48)
    }
}

#Preview {
    NoRecordingsView()
}



================================================
FILE: Demos/Transcriber/Transcriber/RecordingRowView.swift
================================================
//
//  RecordingRowView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI


struct RecordingRowView: View {

    let recording: TranscribedAudioRecording
    @State private var startAnimation = false
    
    var body: some View {
        HStack(spacing:0){
            Text(recording.transcript)
                .font(.body)

            Spacer()

            Button{
                recording.play()
            }label:{
                HStack(spacing:6){
                    Image(systemName: "play.circle.fill")
                        .font(.system(size: 15, weight:.semibold, design: .rounded))
                    Text("\(recording.audioRecording.duration)s")
                        .font(.system(size: 11, weight: .regular, design: .monospaced))
                }
            }
            .buttonStyle(TranscriptionButtonStyle())
        }
        .padding(.vertical, 8)
        .opacity(startAnimation ? 1 : 0)
        .offset(y:startAnimation ? 0 : -10)
        .onAppear{
            withAnimation(.smooth.delay(0.2)){
                startAnimation = true
            }
        }   
    }
}

#Preview {
    RecordingRowView(recording: previewRecording())
        .padding()
}

private func previewRecording() -> TranscribedAudioRecording {
    let audioRecording = AudioRecording(localUrl: URL(fileURLWithPath: "/dev/null"),
                                        duration: "1.2s")
    return TranscribedAudioRecording(
        audioRecording: audioRecording,
        transcript: "hello world",
        createdAt: Date()
    )
}




================================================
FILE: Demos/Transcriber/Transcriber/TranscribedAudioRecording.swift
================================================
//
//  TranscribedAudioRecording.swift
//  Transcriber
//
//  Created by Lou Zell
//

import AVFoundation
import Foundation
import SwiftData

/// Encapsulates a transcribed audio recording
@Model
final class TranscribedAudioRecording {
    @Relationship(deleteRule: .cascade) var audioRecording: AudioRecording
    let transcript: String
    let createdAt: Date
    @Transient var player: AVAudioPlayer?

    init(audioRecording: AudioRecording, transcript: String, createdAt: Date) {
        self.audioRecording = audioRecording
        self.transcript = transcript
        self.createdAt = createdAt
    }

    func play() {
        guard let resolvedURL = self.audioRecording.resolvedURL else {
            AppLogger.error("The audio recording model does not have an associated audio file")
            return
        }
        AppLogger.info("Playing file at \(resolvedURL), which exists? \(FileManager.default.fileExists(atPath: resolvedURL.path))")

        Task.detached {
            do {
                try AVAudioSession.sharedInstance().setCategory(.playback)
                self.player = try AVAudioPlayer(contentsOf: resolvedURL)
                self.player?.play()
            } catch {
                AppLogger.error("Could not play audio file. Error: \(error.localizedDescription)")
            }
        }
     }
}



================================================
FILE: Demos/Transcriber/Transcriber/TranscriberApp.swift
================================================
//
//  TranscriberApp.swift
//  Transcriber
//
//  Created by Lou Zell
//

import SwiftUI

@main
@MainActor
struct TranscriberApp: App {

    @State var transcriberManager = TranscriberManager()

    var body: some Scene {
        WindowGroup {
            TranscriberView(transcriberManager: transcriberManager)
        }
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/TranscriberDataLoader.swift
================================================
//
//  TranscriberDataLoader.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import AIProxy

/// Interfaces with OpenAI to convert a recording into a transcript
final actor TranscriberDataLoader {
    
    /// Run the OpenAI transcriber on an audio recording
    /// - Parameter recording: the audio recording to transcribe
    /// - Returns: a transcript of the recording created by OpenAI's Whisper model
    func run(onRecording recording: AudioRecording) async -> String {
        do {
            let requestBody = OpenAICreateTranscriptionRequestBody(
                file: try Data(contentsOf: recording.localUrl),
                model: "whisper-1"
            )
            let response = try await AppConstants.openAIService.createTranscriptionRequest(body: requestBody)
            return response.text
        } catch {
            AppLogger.error("Could not get transcript from OpenAI: \(error.localizedDescription)")
            return "Transcription Error"
        }
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/TranscriberManager.swift
================================================
//
//  TranscriberManager.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI
import SwiftData

@MainActor
@Observable
final class TranscriberManager {

    private(set) var isRecording = false
    private let audioRecorder = AudioRecorder()
    private let transcriber = TranscriberDataLoader()
    private let modelContext: ModelContext
    var recordings = [TranscribedAudioRecording]()

    init() {
        let context = AppConstants.swiftDataContainer.mainContext
        self.recordings = fetchPersistedRecordings(context)
        self.modelContext = context
    }

    /// This pollutes the manager a bit.
    /// I wrote of a better way to do this, here: https://stackoverflow.com/a/77772091/143447
    /// - Parameter newValue: The value to set `isRecording` to
    private func setIsRecording(_ newValue: Bool) {
        withAnimation(.smooth(duration: 0.75)) {
            self.isRecording = newValue
        }
    }

    /// Start recording an audio file
    func startRecording() async {
        self.setIsRecording(await self.audioRecorder.start())
        if !self.isRecording {
            AppLogger.error("Could not start the audio recorder")
        }
    }

    /// Stop recording the audio file and transcribe it to text with Whisper
    /// - Parameter duration: Annotate the audio file with this duration.
    func stopRecording(duration: String) async {
        if let recording = await self.audioRecorder.stopRecording(duration: duration) {
            let transcript = await self.transcriber.run(onRecording: recording)
            let transcribed = TranscribedAudioRecording(audioRecording: recording, transcript: transcript, createdAt: Date())
            self.modelContext.insert(transcribed)
            self.recordings = fetchPersistedRecordings(self.modelContext)
        }
        self.setIsRecording(false)
    }

    /// Removes a recording from persistent storage and deletes the associated audio file from disk
    /// - Parameter index: the index in `recordings` to delete
    func deleteRecording(at index: Int) {
        FileUtils.deleteFile(at: self.recordings[index].audioRecording.localUrl)
        self.modelContext.delete(self.recordings[index])
        self.recordings = fetchPersistedRecordings(self.modelContext)
    }
}

private func fetchPersistedRecordings(_ modelContext: ModelContext) -> [TranscribedAudioRecording] {
    do {
        let descriptor = FetchDescriptor<TranscribedAudioRecording>(
            sortBy: [SortDescriptor(\TranscribedAudioRecording.createdAt, order: .reverse)]
        )
        return try modelContext.fetch(descriptor)
    } catch {
        AppLogger.error("Could not fetch audio recordings with SwiftData")
        return []
    }
}



================================================
FILE: Demos/Transcriber/Transcriber/TranscriberView.swift
================================================
//
//  TranscribeView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftData
import SwiftUI
import AVFoundation

@MainActor
struct TranscriberView: View {
    let transcriberManager: TranscriberManager

    var isRecording: Bool {
        transcriberManager.isRecording
    }

    @State private var showDot = false
    @State private var isPulsing = false
    @State private var isProcessing = false
    
    @State var isTimerRunning = false
    @State private var startTime =  Date()
    @State private var timerString = "0:00"
    @State private var timer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()
    
    // With vibration
    private let startSFX: SystemSoundID = 1113
    private let stopSFX: SystemSoundID = 1114

    private let deviceWidth = UIScreen.main.bounds.width
    private let deviceHeight = UIScreen.main.bounds.height
    
    private var initialX: Double {
        deviceWidth / 2.0
    }
    private var midY: Double {
        -deviceHeight / 2.0 + 80
    }
    
    var body: some View {
        ZStack(alignment:.bottom){
            
            if transcriberManager.recordings.count > 0 {
                List{
                    ForEach(transcriberManager.recordings) { recording in
                        RecordingRowView(recording: recording)
                    }
                    .onDelete { indexSet in
                        if let index = indexSet.first {
                            self.transcriberManager.deleteRecording(at: index)
                        }
                    }
                }
                .listStyle(.plain)
                
            }  else {
                NoRecordingsView()
            }
            
            /// Overlay when recording
            if isRecording {
                ZStack{
                    VStack(spacing:4){
                        Text(isProcessing ? "Processing" : "Recording")
                            .font(.title2)
                            .fontWeight(.bold)
                            .foregroundColor(.primary)
                        Text(isProcessing ? "This may take a second" : "\(self.timerString)s")
                            .font(.system(size: 13, weight: .medium, design: .monospaced))
                            .foregroundColor(.secondary)
                            .onReceive(timer) { _ in
                                if self.isTimerRunning {
                                    timerString = String(format: "%.2f", (Date().timeIntervalSince( self.startTime)))
                                }
                            }
                    }
                    .offset(y:-140)
                }
                .frame(maxWidth: .infinity, maxHeight:.infinity)
                .ignoresSafeArea()
                .background(.ultraThinMaterial)
                .transition(.opacity)
            }
            
            GeometryReader { geometry in
                /// Gooey button effect
                Canvas { context, size in
                    let circle0 = context.resolveSymbol(id: 0)!
                    let circle1 = context.resolveSymbol(id: 1)!
                    context.addFilter(.alphaThreshold(min: 0.25, color: .primary))
                    context.addFilter(.blur(radius: 15))
                    context.drawLayer {context in
                        context.draw(circle0, at: CGPoint(x:initialX, y:geometry.size.height - 80))
                        context.draw(circle1, at: CGPoint(x:initialX, y:geometry.size.height - 80))
                    }
                } symbols: {
                    Circle()
                        .frame(width: 80, height: 80)
                        .scaleEffect(isProcessing ? 0.75 : 1, anchor: .center)
                        .tag(0)
                    Circle()
                        .frame(width: 80, height: 80)
                        .tag(1)
                        .scaleEffect(showDot ? 1 : 0.5, anchor: .center)
                        .scaleEffect(isPulsing ? 1.15 : 1, anchor: .center)
                        .offset(y: showDot ? midY : 0)
                }
            }
            
            Button{
                UIImpactFeedbackGenerator(style: .medium).impactOccurred()
                /// Start  recording
                if !isRecording{
                    Task {
                        await transcriberManager.startRecording()
                    }
                    AudioServicesPlaySystemSound(startSFX)
                    timerString = "0.00"
                    startTime = Date()
                    // start UI updates
                    self.startTimer()
                    withAnimation(.smooth(duration:0.75)){
                        showDot = true
                    }
                    withAnimation(.easeInOut(duration: 0.75).repeatForever(autoreverses: true)){
                        isPulsing = true
                    }
                    isTimerRunning = true
                } else { /// Stop recording
                    Task {
                        await transcriberManager.stopRecording(duration: self.timerString)
                        withAnimation(.bouncy){
                            isProcessing = false
                        }
                    }

                    AudioServicesPlaySystemSound(stopSFX)
                    self.stopTimer()
                    isTimerRunning = false
                    withAnimation(.smooth(duration: 0.75)){
                        showDot = false
                    }
                    withAnimation(.default){
                        isProcessing = true
                        isPulsing = false
                    }
                }
            } label: {
                ZStack{
                    RoundedRectangle(cornerRadius: isRecording ? 8 : 45, style:.continuous)
                        .fill(.red.gradient)
                        .frame(width:isRecording ? 40 : 85, height:isRecording ? 40 : 85)
                        .opacity(isProcessing ? 0 : 1)
                    if isRecording {
                        ProgressView()
                            .opacity(isProcessing ? 1.0 : 0)
                            .tint(.primary)
                            .colorInvert()
                    } else {
                        Image(systemName: "waveform")
                            .font(.title)
                            .foregroundColor(.white)
                            .transition(.scale)
                    }
                    
                }
            }
            .buttonStyle(RecordButton())
            .disabled(isProcessing ? true : false)
        }
    }

    func stopTimer() {
        self.timer.upstream.connect().cancel()
    }
    
    func startTimer() {
        self.timer = Timer.publish(every: 00.01, on: .main, in: .common).autoconnect()
    }
}


#Preview {
    TranscriberView(transcriberManager: TranscriberManager())
}



================================================
FILE: Demos/Transcriber/Transcriber/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Transcriber/Transcriber/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Transcriber/Transcriber/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "transcribe.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Transcriber/Transcriber/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Translator/Translator/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import AIProxy

enum AppConstants {
    #error(
        """
        Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
        Please see https://www.aiproxy.pro/docs/integration-guide.html")
        """
    )
    
    /* Uncomment for BYOK use cases */
    static let openAIService = AIProxy.openAIDirectService(
        unprotectedAPIKey: "your-openai-key"
    )

    /* Uncomment for all other production use cases */
//    let openAIService = AIProxy.openAIService(
//        partialKey: "partial-key-from-your-developer-dashboard",
//        serviceURL: "service-url-from-your-developer-dashboard"
//    )
}



================================================
FILE: Demos/Translator/Translator/AppLogger.swift
================================================
//
//  AppLogger.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import OSLog

/// Log levels available:
///
///     AppLogger.debug
///     AppLogger.info
///     AppLogger.warning
///     AppLogger.error
///     AppLogger.critical
///
/// Flip on metadata logging in Xcode's console to show which source line the log occurred from.
///
/// See my reddit post for a video instructions:
/// https://www.reddit.com/r/SwiftUI/comments/15lsdtk/how_to_use_the_oslog_logger/
let AppLogger = Logger(subsystem: Bundle.main.bundleIdentifier ?? "UnknownApp",
                       category: "AIProxyBootstrapTranslator")



================================================
FILE: Demos/Translator/Translator/BottomTranslateView.swift
================================================
//
//  BottomTranslateView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct BottomTranslateView: View {

    @Binding var processing:Bool
    @Binding var translatedText:String

    var body: some View {
        VStack{

            VStack(alignment:.leading, spacing:8){
                Text("Spanish")
                    .font(.callout)
                    .foregroundColor(.secondary)
                if processing {
                    ProgressView()
                        .frame(maxWidth: .infinity, maxHeight:.infinity)
                } else{
                    Text(translatedText)
                        .font(.title2)
                }
            }
            .frame(maxWidth: .infinity, maxHeight:.infinity, alignment:.topLeading)


            HStack(spacing:0){
                Button(){
                    /// copy result
                } label:{
                    Image(systemName: "square.on.square")
                        .font(.title2)
                }
                .frame(width:44, height:44)
            }
            .frame(maxWidth: .infinity, alignment:.leading)
        }
        .frame(maxWidth: .infinity, maxHeight:.infinity)
        .padding(16)
        .background(
            RoundedRectangle(cornerRadius: 14, style: .continuous)
                .fill(Color(.tertiarySystemBackground))
                .shadow(color:.black.opacity(0.14), radius: 1)
        )
    }
}


#Preview {
    BottomTranslateView(processing: .constant(false), translatedText: .constant(""))
}



================================================
FILE: Demos/Translator/Translator/ButtonStyles.swift
================================================
//
//  ButtonStyles.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct ButtonStyles: View {
    var body: some View {
        
        ZStack{
            Color(.systemGroupedBackground)
                .ignoresSafeArea()
            
            VStack(spacing:48){
                
                Button{
                    /// do something
                }label:{
                    HStack(spacing:6){
                        Image(systemName: "play.circle.fill")
                            .font(.system(size: 15, weight:.semibold, design: .rounded))
                        Text("10s")
                            .font(.system(size: 11, weight: .regular, design: .monospaced))
                            .fontDesign(.monospaced)
                    }
                }
                .buttonStyle(TranscriptionButtonStyle())

                Button(){
                    /// do something
                } label:{
                    Label("Chunky Button", systemImage: "sparkles")
                }
                .buttonStyle(ChunkyButtonStyle(offsetSize: 10.0))

                Button{
                    /// do something
                } label: {
                    ZStack{
                        RoundedRectangle(cornerRadius: 45, style:.continuous)
                            .fill(.red.gradient)
                            .frame(width:85, height:85)
                        Image(systemName: "waveform")
                            .font(.title)
                            .foregroundColor(.white)
                    }
                }
                .buttonStyle(RecordButton())
                
                Button(){
                    /// do something
                }label:{
                    HStack(spacing:4){
                        Text("Translate")
                        Image(systemName: "arrow.forward")
                    }
                }
                .buttonStyle(TranslateButton())
                
            }
            .padding()
        }
        
    }
}

struct ChunkyButtonStyle: ButtonStyle {

    var offsetSize = 10.0

    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .font(.system(size: 24, weight: .bold, design: .rounded))
            .foregroundColor(.black)
            .padding()
            .offset(y: configuration.isPressed ? offsetSize-2 : 0)
            .background(
                GeometryReader{ geo in
                    RoundedRectangle(cornerRadius: 17)
                        .strokeBorder(Color.black, lineWidth: 4)
                        .background(
                            RoundedRectangle(cornerRadius: 17)
                                .fill(.white)
                        )
                        .offset(y:offsetSize)

                        .overlay(
                            RoundedRectangle(cornerRadius: 17)
                                .fill(.white)
                                .strokeBorder(Color.black, lineWidth: 4)
                                .background(RoundedRectangle(cornerRadius: 17).fill(Color.white))
                                .offset(y: configuration.isPressed ? offsetSize : 0)
                        )
                }
            )
    }
}

struct RecordButton: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .frame(maxWidth:80, maxHeight: 80)
            .shadow(color:.black.opacity(0.28), radius: 10, y:8)
            .scaleEffect(configuration.isPressed ? 0.9 : 1, anchor: .center)
            .animation(.easeOut(duration: 0.2), value: configuration.isPressed)
            .padding(.bottom, 40)
            
    }
}

struct TranslateButton: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .foregroundColor(.white)
            .padding(.vertical, 12)
            .padding(.horizontal, 16)
            .font(.system(size: 13, weight: .bold, design: .rounded))
            .background(.teal.gradient)
            .clipShape(Capsule())
            .brightness(configuration.isPressed ? -0.1 : 0.0)
            .scaleEffect(configuration.isPressed ? 0.9 : 1, anchor: .center)
    }
}

struct TranscriptionButtonStyle: ButtonStyle {

    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .foregroundColor(.secondary)
            .padding(.leading, 4)
            .padding(.trailing, 8)
            .padding(.vertical, 4)
            .background(
                Capsule().fill(.secondary.opacity(configuration.isPressed ? 0.28 : 0.14))
            )
            .scaleEffect(configuration.isPressed ? 0.9 : 1.0)
    }
}

#Preview {
    ButtonStyles()
}



================================================
FILE: Demos/Translator/Translator/TopTranslateView.swift
================================================
//
//  TopTranslateView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct TopTranslateView: View {

    @Binding var newText:String
    @Binding var translatedText:String
    @State private var showButton: Bool = false
    var translate: () -> Void

    var body: some View {
        VStack(alignment:.leading){
            Text("English")
                .font(.callout)
                .foregroundColor(.secondary)
            TextField("Type something...", text: $newText, axis: .vertical)
                .font(.title2)
                .lineLimit(...2)
                .textFieldStyle(.plain)
                .frame(maxHeight: .infinity, alignment:.topLeading)
                .onChange(of: newText) { _, newValue in
                    withAnimation(.bouncy){
                        if !newValue.isEmpty {
                            showButton = true
                        } else {
                            showButton = false
                        }
                    }
                }

            if showButton {
                HStack(alignment:.bottom){
                    Button{
                        newText = ""
                        translatedText = ""
                        showButton = false
                    } label:{
                        Text("Clear")
                    }

                    Spacer()

                    Button{
                        self.translate()
                    }label:{
                        HStack(spacing:4){
                            Text("Translate")
                            Image(systemName: "arrow.forward")
                        }
                    }
                    .buttonStyle(TranslateButton())
                }
                .transition(.opacity)
            }
        }
        .frame(maxWidth: .infinity)
        .padding(16)
        .background(
            RoundedRectangle(cornerRadius: 14, style: .continuous)
                .fill(Color(.tertiarySystemBackground))
                .shadow(color:.black.opacity(0.14), radius: 1)
        )
    }
}

#Preview {
    TopTranslateView(newText: .constant(""), translatedText: .constant(""), translate: {})
}



================================================
FILE: Demos/Translator/Translator/TranslateView.swift
================================================
//
//  TranslateView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

@MainActor
struct TranslateView: View {

    @State private var newText:String = ""
    @State private var translatedText:String = ""
    @State private var processing:Bool = false
    
    private let prompt = "The response is an exact translation from english to spanish. You don't respond with any english."

    var body: some View {
        ZStack{
            Color(.systemGroupedBackground)
                .ignoresSafeArea()
            
            VStack{
                TopTranslateView(
                    newText: $newText,
                    translatedText: $translatedText,
                    translate: { self.translate() }
                )
                BottomTranslateView(
                    processing: $processing,
                    translatedText: $translatedText
                )
            }
            .padding()
        }
    }

    func translate(){
        withAnimation(.smooth){
            processing = true
        }
        Task {
            translatedText = await TranslationDataLoader.run(on: self.newText)
            withAnimation(.smooth){
                processing = false
            }
        }
    }
}

#Preview {
    TranslateView()
}



================================================
FILE: Demos/Translator/Translator/TranslationDataLoader.swift
================================================
//
//  Translator.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation

private let prompt = "The response is an exact translation from english to spanish. You don't respond with any english."

/// Interfaces with OpenAI to translate input text from english to spanish
struct TranslationDataLoader {
    private init() {
        fatalError("Translator is a namespace only")
    }
    
    /// Translate `input` from english to spanish
    /// - Parameter input: the english input
    /// - Returns: the spanish translation
    static func run(on input: String) async -> String {
        do {
            let response = try await AppConstants.openAIService.chatCompletionRequest(body: .init(
                model: "gpt-4o",
                messages: [
                    .system(content: .text(prompt)),
                    .user(content: .text(input))
                ]
            ))
            if let text = response.choices.first?.message.content {
                return text
            }
        } catch {
            AppLogger.error("Could not translate using gpt4o: \(error)")
        }
        return "Translation failed!"
    }
}



================================================
FILE: Demos/Translator/Translator/TranslatorApp.swift
================================================
//
//  TranslatorApp.swift
//  Translator
//
//  Created by Lou Zell
//

import SwiftUI

@main
struct TranslatorApp: App {
    var body: some Scene {
        WindowGroup {
            TranslateView()
        }
    }
}



================================================
FILE: Demos/Translator/Translator/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Translator/Translator/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Translator/Translator/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "translate.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Translator/Translator/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Trivia/Trivia/AppConstants.swift
================================================
//
//  AppConstants.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import AIProxy

enum AppConstants {
    #error(
        """
        Uncomment one of the methods below. To build and run on device you must follow the AIProxy integration guide.
        Please see https://www.aiproxy.pro/docs/integration-guide.html")
        """
    )

    /* Uncomment for BYOK use cases */
    static let openAIService = AIProxy.openAIDirectService(
        unprotectedAPIKey: "your-openai-key"
    )

    /* Uncomment for all other production use cases */
//    let openAIService = AIProxy.openAIService(
//        partialKey: "partial-key-from-your-developer-dashboard",
//        serviceURL: "service-url-from-your-developer-dashboard"
//    )
}



================================================
FILE: Demos/Trivia/Trivia/AppLogger.swift
================================================
//
//  AppLogger.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import OSLog

/// Log levels available:
///
///     AppLogger.debug
///     AppLogger.info
///     AppLogger.warning
///     AppLogger.error
///     AppLogger.critical
///
/// Flip on metadata logging in Xcode's console to show which source line the log occurred from.
///
/// See my reddit post for a video instructions:
/// https://www.reddit.com/r/SwiftUI/comments/15lsdtk/how_to_use_the_oslog_logger/
let AppLogger = Logger(subsystem: Bundle.main.bundleIdentifier ?? "UnknownApp",
                       category: "AIProxyBootstrapTrivia")



================================================
FILE: Demos/Trivia/Trivia/TriviaAnswerPicker.swift
================================================
//
//  TriviaAnswerPicker.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI

struct TriviaAnswerPicker: View {
    /// Data model that holds the trivia question, potential answers, and correct answer index
    let questionModel: TriviaQuestionModel

    /// This question position in the stack of trivia cards
    let questionNumber: Int

    /// Number of questions in the stack of trivia cards
    let questionOf: Int

    /// The argument passed to this closure is the guessed answer index for comparison with `questionModel.correctAnswerIndex`
    let didTapAnswer: (Int) -> Void


    var body: some View {
        VStack(alignment:.leading, spacing:36) {

            VStack(alignment:.leading, spacing:16){
                Text("Question \(questionNumber) of \(questionOf)")
                    .font(.system(size: 15, weight:.medium, design: .rounded))
                    .foregroundColor(.secondary)

                Text(questionModel.question)
                    .font(.system(size: 20, weight:.medium, design: .rounded))
                    .fixedSize(horizontal: false, vertical: true)
            }
            .frame(maxWidth: .infinity, alignment:.leading)

            VStack(alignment:.leading, spacing:8){
                ForEach(questionModel.labeledAnswers) { labeledAnswer in
                    Text(labeledAnswer.text)
                        .fixedSize(horizontal: false, vertical: true)
                }
            }
            .font(.system(size: 17, weight:.medium, design: .rounded))
            .frame(maxWidth: .infinity, alignment:.leading)

            VStack{
                HStack(spacing:8) {
                    CardButton(systemImageName: "a.circle.fill", tint: .blue) {
                        didTapAnswer(0)
                    }

                    CardButton(systemImageName: "b.circle.fill", tint: .mint) {
                        didTapAnswer(1)
                    }
                }
                HStack {
                    CardButton(systemImageName: "c.circle.fill", tint: .green) {
                        didTapAnswer(2)
                    }

                    CardButton(systemImageName: "d.circle.fill", tint: .indigo) {
                        didTapAnswer(3)
                    }
                }
            }
            .buttonStyle(.bordered)
            .font(.title)
            .frame(maxWidth:.infinity, alignment:.leading)
        }
        .padding(16)
    }
}

private struct CardButton: View {
    let systemImageName: String
    let tint: Color
    let action: () -> Void

    var body: some View {
        Button(action: action) {
            Image(systemName: systemImageName)
                .frame(maxWidth: .infinity)
        }
        .tint(tint)
        .controlSize(.large)
    }
}



================================================
FILE: Demos/Trivia/Trivia/TriviaApp.swift
================================================
//
//  TriviaApp.swift
//  Trivia
//
//  Created by Lou Zell
//

import SwiftUI

@main
struct TriviaApp: App {
    var body: some Scene {
        WindowGroup {
            TriviaView()
        }
    }
}



================================================
FILE: Demos/Trivia/Trivia/TriviaCardData.swift
================================================
//
//  TriviaQuestion.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI

@MainActor
@Observable
/// UI model for TriviaCardView
final class TriviaCardData: Identifiable {

    /// Position of the card, with 0 meaning that the card is on top and 1 meaning directly below the top card, etc.
    let position: Int

    /// Data model for the card contents
    var triviaQuestionModel: TriviaQuestionModel?

    /// Networker to load card contents from OpenAI
    private let triviaFetcher: TriviaDataLoader

    /// Creates a UI model for TriviaCardView
    /// - Parameters:
    ///   - triviaFetcher: Loads card contents from OpenAI
    ///   - position: Position of the card, with 0 being the top of the stack
    init(triviaFetcher: TriviaDataLoader, position: Int) {
        self.triviaFetcher = triviaFetcher
        self.position = position
    }

    /// Loads the trivia question's data model asynchronously
    func load() async {
        self.triviaQuestionModel = try! await self.triviaFetcher.getNextQuestion()
    }
}



================================================
FILE: Demos/Trivia/Trivia/TriviaCardView.swift
================================================
//
//  QuizView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import Foundation
import SwiftUI

@MainActor
struct TriviaCardView: View {

    let triviaCardData: TriviaCardData
    @Binding var triviaManager: TriviaManager?
    @State var attempts: Int = 0
    @State var isCorrect = false

    private var questionNumber: Int {
        triviaCardData.position + 1
    }

    private var totalQuestions: Int {
        triviaManager?.triviaCards.count ?? 0
    }

    var body: some View{
        ZStack {
            if let model = triviaCardData.triviaQuestionModel {
                ZStack {
                    TriviaAnswerPicker(
                        questionModel: model,
                        questionNumber: questionNumber,
                        questionOf: totalQuestions
                    ) { guessIndex in
                        checkAnswer(forQuestion: model, withGuessedIndex: guessIndex)
                    }

                    if self.isCorrect {
                        Rectangle()
                            .fill(.black.opacity(0.4))
                            .frame(width: .infinity, height: .infinity)
                            .transition(.opacity)
                        Image(systemName: "checkmark.circle.fill")
                            .font(.system(size: 64))
                            .foregroundColor(.green)
                            .background(.white)
                            .clipShape(Circle())
                            .transition(.scale(0.5).combined(with: .opacity))
                    }
                }
            } else {
                VStack(spacing:16) {
                    ProgressView()
                    Text("Generating questions")
                        .font(.system(size: 15, weight:.regular, design:.rounded))
                        .foregroundColor(.secondary)
                }
                .frame(maxHeight:.infinity)
            }
        }
        .frame(maxWidth: .infinity, maxHeight:480, alignment:.top)
        .background(Color(.systemBackground))
        .cornerRadius(14)
        .shadow(color: .black.opacity(0.14), radius: 1, x: 0, y: 1)
        .modifier(Shake(animatableData: CGFloat(attempts)))
    }


    private func checkAnswer(forQuestion question: TriviaQuestionModel, withGuessedIndex guessedIndex: Int) {
        triviaManager?.trackGuess(ofQuestion: question)
        if (question.correctAnswerIndex == guessedIndex) {
            withAnimation(.bouncy){
                isCorrect = true
            }
            Task {
                try await Task.sleep(for: .seconds(1))
                withAnimation(.bouncy) {
                    triviaManager?.progress()
                }
            }
        } else {
            withAnimation(.default) {
                attempts += 1
            }
        }
    }
}


private struct Shake: GeometryEffect {
    var amount: CGFloat = 10
    var shakesPerUnit = 3
    var animatableData: CGFloat

    func effectValue(size: CGSize) -> ProjectionTransform {
        ProjectionTransform(CGAffineTransform(translationX:
            amount * sin(animatableData * .pi * CGFloat(shakesPerUnit)),
            y: 0))
    }
}



================================================
FILE: Demos/Trivia/Trivia/TriviaDataLoader.swift
================================================
//
//  TriviaFetcher.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import AIProxy

// It's important to add the 'produce JSON' instruction to the system prompt.
// See the note at https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format
private let prompt = """
You are a trivia bot that produces JSON. You ask hard questions with four possible answers. Specifying the index of the correct answer in the key `correct_answer_index`. Example response:
{ question: "xyz", answers: ["a", "b", "c", "d"], correct_answer_index: 2 }
"""

/// Loads trivia data from openai
final actor TriviaDataLoader {
    /// The topic of trivia
    let topic: String

    /// Create the TriviaDataLoader responsible for fetching trivia data from OpenAI
    /// - Parameter topic: The topic of trivia
    init(topic: String) {
        self.topic = topic
    }

    /// We store past questions, and send them back to openai on subsequent requests.
    /// This prevents chat from asking the same questions
    private var pastQuestions = [String]()

    deinit {
        AppLogger.debug("TriviaFetcher is being freed")
    }


    /// Fetches the next trivia question from OpenAI over the network
    /// - Returns: A TriviaQuestionModel containing one question and multiple choice answers
    func getNextQuestion() async throws -> TriviaQuestionModel {
        var messages = prompt
        if self.pastQuestions.count > 0 {
            let pastQuestionsList = self.pastQuestions.joined(separator: "\n\n")
            messages += "\nDo not repeat any of these questions: \(pastQuestionsList)"
        }

        let requestBody = OpenAIChatCompletionRequestBody(
            model: "gpt-4o",
            messages: [
                .system(content: .text("Ask me a question about: \(topic)")),
                .user(content: .text(messages))
            ],
            responseFormat: .jsonObject
        )
        let response = try await AppConstants.openAIService.chatCompletionRequest(body: requestBody)

        guard let text = response.choices.first?.message.content else {
            throw TriviaFetcherError.couldNotFetchQuestion
        }

        let decoder = JSONDecoder()
        decoder.keyDecodingStrategy = .convertFromSnakeCase

        AppLogger.info("Received from openai: \(text)")
        let model = try decoder.decode(TriviaQuestionModel.self, from: text.data(using: .utf8)!)
        self.pastQuestions.append(model.question)
        return model
    }
}

enum TriviaFetcherError: Error {
    case couldNotFetchQuestion
}

struct TriviaQuestionModel: Decodable, Hashable {

    struct LabeledAnswer: Identifiable {
        let id = UUID()
        let text: String
    }

    let question: String
    let answers: [String]
    let correctAnswerIndex: Int

    var labeledAnswers: [LabeledAnswer] {
        return zip(["A", "B", "C", "D"], self.answers).map {
            LabeledAnswer(text: "\($0). \($1)")
        }
    }
}



================================================
FILE: Demos/Trivia/Trivia/TriviaFormView.swift
================================================
//
//  TriviaFormView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct TriviaFormView:View{

    enum FocusedField {
        case topic
    }

    /// Topic entered by the user in a SwiftUI text field
    @State private var topic = ""
    @Binding var triviaManager: TriviaManager?
    @FocusState private var focusedField: FocusedField?

    var body: some View{

        VStack(spacing:24){
            VStack{
                ZStack{
                    Image(systemName: "doc.questionmark.fill")
                        .foregroundColor(.blue)
                        .rotationEffect(.degrees(-15))
                    Image(systemName: "doc.questionmark.fill")
                        .foregroundColor(.teal)
                        .rotationEffect(.degrees(10))
                    Image(systemName: "doc.questionmark")
                        .foregroundColor(.white)
                    Image(systemName: "doc.questionmark.fill")
                        .overlay {
                            LinearGradient(
                                colors: [.orange, .red, .purple],
                                startPoint: .topLeading,
                                endPoint: .bottomTrailing
                            )
                            .mask(
                                Image(systemName: "doc.questionmark.fill")
                                    .font(.system(size: 72))
                            )
                        }
                }
                .font(.system(size: 72))
                .padding(.vertical, 8)

                Text("Trivia Generator")
                    .font(.system(size: 36, weight:.bold, design: .rounded))
                    .multilineTextAlignment(.center)
                Text("Type a trivia theme below")
                    .font(.system(size: 17, weight:.medium, design: .rounded))
                    .foregroundColor(.secondary)
            }

            VStack{
                TextField("Ex. 80's movies...", text: $topic, axis: .vertical)
                    .focused($focusedField, equals: .topic)
                    .font(.system(size: 17, weight:.medium, design: .rounded))
                    .lineLimit(...3)
                    .textFieldStyle(.plain)
                    .padding()
                    .background(.white)
                    .cornerRadius(8)
                    .overlay(
                        RoundedRectangle(cornerRadius: 8)
                            .fill(.clear)
                            .stroke(.separator)
                    )
                    .onAppear {
                        focusedField = .topic
                    }
                Button{
                    withAnimation(){
                        triviaManager = TriviaManager(topic: topic, numCards: 5)
                    }
                }label:{
                    Label("Generate", systemImage: "sparkles")
                        .frame(maxWidth:.infinity)
                        .font(.system(size: 17, weight:.bold, design: .rounded))
                        .fontWeight(.bold)
                }
                .buttonStyle(.borderedProminent)
                .controlSize(.large)
            }
        }
        .padding()
    }
}



================================================
FILE: Demos/Trivia/Trivia/TriviaManager.swift
================================================
//
//  TriviaManager.swift
//  AIProxyBootstrap
//
//  Created by Lou Zell
//

import Foundation
import SwiftUI

@MainActor
@Observable
final class TriviaManager {

    /// The topic of trivia
    let topic: String

    /// The number of cards for the user to solve
    let numCards: Int

    /// All trivia cards
    let triviaCards: [TriviaCardData]

    /// Observable of remaining cards that the user hasn't yet solved
    var remainingCards: [TriviaCardData] {
        return Array(self.triviaCards.suffix(from: self.currentCardIndex))
    }

    /// Number of questions that were answered correctly on the first guess
    var numCorrectOnFirstGuess: Int {
        return self.guessTracker.filter { $0.value == 1 }.count
    }

    private var currentCardIndex: Int
    private let triviaDataLoader: TriviaDataLoader

    /// Tracks number of guesses before the right answer was reached.
    /// The key is the the question, the value is the number of guesses
    private var guessTracker = [TriviaQuestionModel: Int]()

    /// Creates a UI model for the TriviaView view
    /// - Parameters:
    ///   - topic: The topic of trivia
    ///   - numCards: The number of cards to display in the UI
    init(topic: String, numCards: Int) {
        let triviaFetcher = TriviaDataLoader(topic: topic)
        self.topic = topic
        self.numCards = numCards
        self.currentCardIndex = 0
        self.triviaDataLoader = triviaFetcher
        self.triviaCards = (0..<numCards).map { i in TriviaCardData(triviaFetcher: triviaFetcher, position: i) }
        Task {
            await self.triviaCards[0].load()
        }
        self.loadNext()
    }

    /// Progress to the next trivia card
    func progress() {
        self.currentCardIndex += 1
        self.loadNext()
    }

    /// Increments the number of guesses for the question passed as argument
    func trackGuess(ofQuestion question: TriviaQuestionModel) {
        self.guessTracker[question, default: 0] += 1
    }

    private func loadNext() {
        if (self.currentCardIndex < self.numCards - 1) {
            Task {
                await self.triviaCards[self.currentCardIndex + 1].load()
            }
        }
    }

    deinit {
        AppLogger.debug("TriviaData is being freed")
    }
}



================================================
FILE: Demos/Trivia/Trivia/TriviaView.swift
================================================
//
//  QuizView.swift
//  AIProxyBootstrap
//
//  Created by Todd Hamilton
//

import SwiftUI

struct TriviaView: View {

    @State private var triviaManager: TriviaManager?

    var body: some View {
        ZStack{

            Rectangle()
                .fill(Color(.secondarySystemBackground))
                .ignoresSafeArea()

            if let triviaManager = self.triviaManager {
                ZStack{

                    if triviaManager.remainingCards.count == 0 {
                        VStack{
                            Text("You answered")
                                .font(.system(size: 15, weight:.bold, design: .rounded))
                                .foregroundColor(.secondary)
                            Text("\(triviaManager.numCorrectOnFirstGuess) out \(triviaManager.numCards) correctly.")
                                .font(.system(size: 24, weight:.semibold, design: .rounded))
                                .multilineTextAlignment(.center)
                            Button("Play again") {
                                self.triviaManager = nil
                            }
                            .fontDesign(.rounded)
                            .fontWeight(.bold)
                            .controlSize(.large)
                            .buttonStyle(.borderedProminent)
                            .padding(.top, 8)
                        }
                    }


                    VStack(spacing:24){
                        Text(triviaManager.topic)
                            .font(.system(size: 24, weight:.bold, design: .rounded))
                            .fontWeight(.bold)
                        ZStack{
                            ForEach(triviaManager.remainingCards) { triviaCard in
                                let cardPosition = triviaCard.position - (triviaManager.remainingCards.first?.position ?? 0)
                                TriviaCardView(triviaCardData: triviaCard, triviaManager: $triviaManager)
                                    .zIndex(1 - Double(cardPosition))
                                    .offset(y: CGFloat(cardPosition) * 25)
                                    .scaleEffect(1.0 - (CGFloat(cardPosition) * 0.05))
                            }
                        }
                        Spacer()
                    }
                    .padding()
                }

            } else {
                TriviaFormView(triviaManager: $triviaManager)
            }
        }
    }
}

#Preview {
    TriviaView()
}






================================================
FILE: Demos/Trivia/Trivia/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Trivia/Trivia/Assets.xcassets/AccentColor.colorset/Contents.json
================================================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Trivia/Trivia/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "trivia.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: Demos/Trivia/Trivia/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}


